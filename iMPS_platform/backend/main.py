"use client"
from zoneinfo import ZoneInfo
from fastapi import FastAPI,HTTPException,Depends, status,Request,Query,APIRouter, WebSocket, WebSocketDisconnect
from fastapi.encoders import jsonable_encoder 
from fastapi.security import OAuth2PasswordRequestForm,OAuth2PasswordBearer
from jose import JWTError,jwt
from jose.exceptions import ExpiredSignatureError
from datetime import datetime, timedelta, UTC, timezone, time, date
from pymongo.errors import OperationFailure, PyMongoError,DuplicateKeyError
from pymongo import MongoClient,ReturnDocument
from pydantic import BaseModel,EmailStr,constr, Field
from bson.objectid import ObjectId
from bson.errors import InvalidId  
from fastapi.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
import json, os, asyncio

from fastapi.responses import StreamingResponse,Response
from typing import List, Any,Dict, Optional, Union, Literal,Mapping
import bcrypt
from dateutil import parser as dtparser
from bson.decimal128 import Decimal128
from fastapi import Path,UploadFile, File, Form
# from fastapi import UploadFile, File, Form
# from pathlib import Path 
from starlette.staticfiles import StaticFiles
import uuid
from zoneinfo import ZoneInfo
import re
from fastapi import HTTPException, Depends
from fastapi.responses import JSONResponse
from dateutil.relativedelta import relativedelta
import pathlib, secrets
from email.message import EmailMessage
import aiosmtplib
import paho.mqtt.client as mqtt
from contextlib import asynccontextmanager

SECRET_KEY = "supersecret"  # à¹ƒà¸Šà¹‰à¸ˆà¸£à¸´à¸‡à¸„à¸§à¸£à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ env
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES_TECHNICIAN = 1440  # 24 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡
ACCESS_TOKEN_EXPIRE_MINUTES_DEFAULT = 1440  # 24 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ (user à¸­à¸·à¹ˆà¸™)
SESSION_IDLE_MINUTES_TECHNICIAN = None  # technician à¹„à¸¡à¹ˆà¸¡à¸µ idle timeout
SESSION_IDLE_MINUTES_DEFAULT = 15  # user à¸­à¸·à¹ˆà¸™ idle 15 à¸™à¸²à¸—à¸µ à¹à¸¥à¹‰à¸§à¹€à¸”à¹‰à¸‡
REFRESH_TOKEN_EXPIRE_DAYS = 7
th_tz = ZoneInfo("Asia/Bangkok")

# .env à¸«à¸£à¸·à¸­à¸œà¹ˆà¸²à¸™à¸•à¸±à¸§à¹à¸›à¸£à¹à¸§à¸”à¸¥à¹‰à¸­à¸¡
SMTP_HOST = os.getenv("SMTP_HOST", "smtp.gmail.com")
SMTP_PORT = int(os.getenv("SMTP_PORT", "587"))
SMTP_USER = os.getenv("SMTP_USER", "eds194655@gmail.com")
SMTP_PASS = os.getenv("SMTP_PASS", "depllvpufjwtpysc")
SENDER_EMAIL = os.getenv("SENDER_EMAIL", "eds194655@gmail.com")

# BASE = Path(__file__).parent
app = FastAPI()

client1 = MongoClient("mongodb://imps_platform:eds_imps@203.154.130.132:27017/")
client = AsyncIOMotorClient("mongodb://imps_platform:eds_imps@203.154.130.132:27017/")

deviceDB = client["utilizationFactor"]
settingDB = client["settingParameter"]
errorDB = client["errorCode"]

db = client1["iMPS"]
users_collection = db["users"]
station_collection = db["stations"]

MDB_DB = client["MDB"]

CBM_DB = client["monitorCBM"]

PMReportDB = client["PMReport"]
PMUrlDB = client["PMReportURL"]

MDBPMReportDB = client["MDBPMReport"]
MDBPMUrlDB = client["MDBPMReportURL"]

CCBPMReportDB = client["CCBPMReport"]
CCBPMUrlDB = client["CCBPMReportURL"]

CBBOXPMReportDB = client["CBBOXPMReport"]
CBBOXPMUrlDB = client["CBBOXPMReportURL"]

DCTestReportDB = client["DCTestReport"]
DCUrlDB = client["DCUrl"]

ACTestReportDB = client["ACTestReport"]
ACUrlDB = client["ACUrl"]

stationPMReportDB = client["stationPMReport"]
stationPMUrlDB = client["stationPMReportURL"]

CMReportDB = client["CMReport"]
CMUrlDB = client["CMReportURL"]

outputModule1 = client["OutputModule1"]
outputModule2 = client["OutputModule2"]
outputModule3 = client["OutputModule3"]
outputModule4 = client["OutputModule4"]
outputModule5 = client["OutputModule5"]
outputModule6 = client["OutputModule6"]
outputModule7 = client["OutputModule7"]

inputModule1 = client["module1MdbDustPrediction"]
inputModule2 = client["module2ChargerDustPrediction"]
inputModule3 = client["module3ChargerOfflineAnalysis"]
inputModule4 = client["module4AbnormalPowerPrediction"]
inputModule5 = client["module5NetworkProblemPrediction"]
inputModule6 = client["module6DcChargerRulPrediction"]
inputModule7 = client["module7ChargerPowerIssue"]

# à¸›à¸£à¸±à¸šà¹ƒà¸«à¹‰à¸£à¸­à¸‡à¸£à¸±à¸šà¸«à¸¥à¸²à¸¢à¹‚à¸¡à¸”à¸¹à¸¥
MODULES = ["module1", "module2", "module3", "module4", "module5", "module6", "module7"]

OUTPUT_DBS = {
    "module1": outputModule1,
    "module2": outputModule2,
    "module3": outputModule3,
    "module4": outputModule4,
    "module5": outputModule5,
    "module6": outputModule6,
    "module7": outputModule7,
}

INPUT_DBS = {
    "module1": inputModule1,
    "module2": inputModule2,
    "module3": inputModule3,
    "module4": inputModule4,
    "module5": inputModule5,
    "module6": inputModule6,
    "module7": inputModule7,
}


imps_db_async = client["iMPS"]
stations_coll_async = imps_db_async["stations"]
users_coll_async = imps_db_async["users"]
email_log_coll = imps_db_async["errorEmailLog"]

MDB_collection = MDB_DB["Klongluang3"]

BROKER_HOST = "212.80.215.42"
BROKER_PORT = 1883
MQTT_TOPIC  = "iMPS/Test/settingPLC"
MQTT_CLIENT_ID = "imps-backend-setting-plc"
mqtt_client = mqtt.Client(client_id=MQTT_CLIENT_ID, clean_session=True)

def _on_connect(client, userdata, flags, rc):
    pass

def _on_disconnect(client, userdata, rc):
    pass

mqtt_client.on_connect = _on_connect
mqtt_client.on_disconnect = _on_disconnect

async def lifespan(app: FastAPI):
    # à¹ƒà¸Šà¹‰ connect_async + loop_start à¹€à¸žà¸·à¹ˆà¸­à¹„à¸¡à¹ˆ block event loop
    mqtt_client.connect_async(BROKER_HOST, BROKER_PORT, keepalive=60)
    mqtt_client.loop_start()
    try:
        yield
    finally:
        try:
            mqtt_client.loop_stop()
            mqtt_client.disconnect()
        except Exception as e:
            print(f"[MQTT] disconnect error: {e}")

app = FastAPI(lifespan=lifespan)

# CORS (à¸£à¸°à¸šà¸¸ origin à¸ˆà¸£à¸´à¸‡à¹ƒà¸™à¹‚à¸›à¸£à¸”à¸±à¸à¸Šà¸±à¸™)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["ETag"],              # à¹ƒà¸«à¹‰ FE à¸­à¹ˆà¸²à¸™ ETag à¹„à¸”à¹‰ (à¹ƒà¸Šà¹‰à¹ƒà¸™ /outputModule6)
    max_age=86400  
)

def _validate_station_id(station_id: str):
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")

def get_mdb_collection_for(station_id: str):
    # à¸à¸±à¸™à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™à¹à¸›à¸¥à¸ à¹† / injection: à¸­à¸™à¸¸à¸à¸²à¸• a-z A-Z 0-9 _ -
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return MDB_DB.get_collection(str(station_id))

def to_json(obj) -> str:
    # à¸šà¸±à¸‡à¸„à¸±à¸šà¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ single-line à¹à¸¥à¸°à¸£à¸­à¸‡à¸£à¸±à¸š UTF-8
    return json.dumps(obj, ensure_ascii=False, separators=(",", ":"))

def get_errorCode_collection_for(station_id: str):
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return errorDB.get_collection(str(station_id))

def _ensure_utc_iso(v):
    """
    à¸„à¸·à¸™à¸„à¹ˆà¸²à¹€à¸›à¹‡à¸™à¸ªà¸•à¸£à¸´à¸‡ ISO-8601 (UTC 'Z') à¹€à¸ªà¸¡à¸­
    - à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™ datetime â†’ à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ UTC + à¹€à¸•à¸´à¸¡ 'Z'
    - à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™à¸ªà¸•à¸£à¸´à¸‡ ISO à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸² â†’ à¹€à¸•à¸´à¸¡ 'Z'
    - à¸­à¸¢à¹ˆà¸²à¸‡à¸­à¸·à¹ˆà¸™ â†’ à¸„à¸·à¸™à¹€à¸”à¸´à¸¡
    """
    if isinstance(v, datetime):
        return v.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

    if isinstance(v, str) and re.match(r'^\d{4}-\d{2}-\d{2}T', v) and not re.search(r'(Z|[+\-]\d{2}:\d{2})$', v):
        return v + 'Z'
    return v

def create_access_token(data: dict, expires_delta: int | timedelta = 15):
    to_encode = dict(data)
    expire = (datetime.now(timezone.utc) + (timedelta(minutes=expires_delta) if isinstance(expires_delta, int) else expires_delta))
    to_encode["exp"] = int(expire.timestamp())
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

class LoginRequest(BaseModel):
    email: str
    password: str

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:3001",
        "http://127.0.0.1:3000",
        "http://203.154.130.132:3000",
        "http://203.154.130.132:3001",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["ETag"],              # à¹ƒà¸«à¹‰ FE à¸­à¹ˆà¸²à¸™ ETag à¹„à¸”à¹‰ (à¹ƒà¸Šà¹‰à¹ƒà¸™ /outputModule6)
    max_age=86400  
)



oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/login/")

class UserClaims(BaseModel):
    sub: str
    user_id: Optional[str] = None
    username: str
    role: str = "user"
    company: Optional[str] = None
    station_ids: List[str] = []
    

def get_current_user(request: Request) -> UserClaims:
    # 1) à¸¥à¸­à¸‡à¸­à¹ˆà¸²à¸™à¸ˆà¸²à¸à¸„à¸¸à¸à¸à¸µà¹‰ (à¹ƒà¸Šà¹‰à¸à¸±à¸š SSE)
    token = request.cookies.get(ACCESS_COOKIE_NAME)

    # 2) à¸ªà¸³à¸£à¸­à¸‡: Authorization: Bearer ...
    if not token:
        auth = request.headers.get("Authorization", "")
        if auth.startswith("Bearer "):
            token = auth.removeprefix("Bearer ").strip()

    if not token:
        raise HTTPException(status_code=401, detail="Not authenticated")

    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        sub = payload.get("sub")
        if not sub:
            raise HTTPException(status_code=401, detail="invalid_token")

        station_ids = payload.get("station_ids") or []
        if not isinstance(station_ids, list):
            station_ids = [station_ids]

        return UserClaims(
            sub=sub,
            user_id=payload.get("user_id"),
            username=payload.get("username"),
            role=payload.get("role", "user"),
            company=payload.get("company"),
            station_ids=station_ids,
        )
    except ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="token_expired")
    except JWTError:
        raise HTTPException(status_code=401, detail="invalid_token")

ACCESS_COOKIE_NAME = "access_token"

#####################loginnn
# @app.post("/login/")
# def login(form_data: OAuth2PasswordRequestForm = Depends()):
#     user = users_collection.find_one(
#         {"email": form_data.username},
#         {"_id": 1, "email": 1, "username": 1, "password": 1, "role": 1, "company": 1, "station_id": 1},
#     )
#     invalid_cred = HTTPException(status_code=401, detail="Invalid email or password")
#     if not user or not bcrypt.checkpw(form_data.password.encode("utf-8"), user["password"].encode("utf-8")):
#         raise invalid_cred

#     # à¸—à¸³à¹ƒà¸«à¹‰ station_ids à¹€à¸›à¹‡à¸™ list à¹€à¸ªà¸¡à¸­
#     station_ids = user.get("station_id", [])
#     if not isinstance(station_ids, list):
#         station_ids = [station_ids]

#     # â–¶ Access Token à¹ƒà¸ªà¹ˆà¸ªà¸´à¸—à¸˜à¸´à¹Œà¹„à¸§à¹‰à¹€à¸¥à¸¢
#     access_token = create_access_token({
#         "sub": user["email"],
#         "user_id": str(user["_id"]),
#         "username": user.get("username"),
#         "role": user.get("role", "user"),
#         "company": user.get("company"),
#         "station_ids": station_ids,
#     })

#     # â–¶ Refresh Token (à¸¡à¸µà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸¡à¸µà¸à¹‡à¹„à¸”à¹‰à¸•à¸²à¸¡à¸—à¸µà¹ˆà¸„à¸¸à¸“à¹ƒà¸Šà¹‰à¸­à¸¢à¸¹à¹ˆ)
#     refresh_token = create_access_token({"sub": user["email"]}, expires_delta=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS))

#     # à¸­à¸±à¸›à¹€à¸”à¸• refresh token à¹ƒà¸™ DB (à¸ˆà¸°à¹€à¸à¹‡à¸š hash à¸à¹‡à¹„à¸”à¹‰ à¸•à¸²à¸¡à¹à¸™à¸§à¸—à¸²à¸‡à¸—à¸µà¹ˆà¸„à¸¸à¸¢à¸à¸±à¸™à¸à¹ˆà¸­à¸™à¸«à¸™à¹‰à¸²)
#     users_collection.update_one({"_id": user["_id"]}, {"$set": {
#         "refreshTokens": [{
#             "token": refresh_token,
#             "createdAt": datetime.utcnow(),
#             "expiresAt": datetime.utcnow() + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS),
#         }]
#     }})

#     return {
#         "message": "Login success âœ…",
#         "access_token": access_token,
#         "refresh_token": refresh_token,
#         "user": {
#             "user_id": str(user["_id"]),
#             "username": user.get("username"),
#             "email": user["email"],
#             "role": user.get("role", "user"),
#             "company": user.get("company"),
#             "station_id": station_ids,
#         }
#     }

# @app.post("/login/")
# def login(body: LoginRequest, response: Response):
#     # à¸«à¸² user
#     user = users_collection.find_one(
#         {"email": body.email},
#         {"_id": 1, "email": 1, "username": 1, "password": 1, "role": 1, "company": 1, "station_id": 1},
#     )
#     if not user or not bcrypt.checkpw(body.password.encode("utf-8"), user["password"].encode("utf-8")):
#         raise HTTPException(status_code=401, detail="Invalid email or password")

#     # à¹ƒà¸«à¹‰ station_id à¹€à¸›à¹‡à¸™ list à¹€à¸ªà¸¡à¸­
#     station_ids = user.get("station_id", [])
#     if not isinstance(station_ids, list):
#         station_ids = [station_ids]

#     # à¸­à¸­à¸ access token
#     jwt_token = create_access_token({
#         "sub": user["email"],
#         "user_id": str(user["_id"]),
#         "username": user.get("username"),
#         "role": user.get("role", "user"),
#         "company": user.get("company"),
#         "station_ids": station_ids,
#     }, expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES))

#     # à¸­à¸­à¸ refresh token (à¸–à¹‰à¸²à¹ƒà¸Šà¹‰)
#     refresh_token = create_access_token({"sub": user["email"]}, expires_delta=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS))
#     users_collection.update_one({"_id": user["_id"]}, {"$set": {
#         "refreshTokens": [{
#             "token": refresh_token,
#             "createdAt": datetime.now(timezone.utc),
#             "expiresAt": datetime.now(timezone.utc) + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS),
#         }]
#     }})

#     # à¸„à¸¸à¸à¸à¸µà¹‰à¸ªà¸³à¸«à¸£à¸±à¸š SSE (à¸ªà¸³à¸„à¸±à¸)
#     response.set_cookie(
#         key=ACCESS_COOKIE_NAME,
#         value=jwt_token,
#         httponly=True,
#         secure=False,          # ðŸ‘ˆ dev à¸šà¸™ http://localhost à¹ƒà¸«à¹‰ False
#         samesite="lax",        # ðŸ‘ˆ dev à¸‚à¹‰à¸²à¸¡à¸žà¸­à¸£à¹Œà¸•à¸šà¹ˆà¸­à¸¢ à¹ƒà¸Šà¹‰ "lax" (à¸–à¹‰à¸² cross-domain à¸ˆà¸£à¸´à¸‡à¸„à¹ˆà¸­à¸¢à¹ƒà¸Šà¹‰ "none"+secure=True)
#         max_age=int(timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES).total_seconds()),
#         path="/",
#     )

#     # à¸„à¸·à¸™à¹ƒà¸«à¹‰ frontend à¹€à¸à¹‡à¸šà¸”à¹‰à¸§à¸¢ (à¹ƒà¸Šà¹‰à¸à¸±à¸š fetch à¸­à¸·à¹ˆà¸™à¹†)
#     return {
#         "message": "ok",
#         "access_token": jwt_token,
#         "refresh_token": refresh_token,
#         "user": {
#             "user_id": str(user["_id"]),
#             "username": user.get("username"),
#             "email": user["email"],
#             "role": user.get("role", "user"),
#             "company": user.get("company"),
#             "station_id": station_ids,
#         }
#     }

@app.post("/login/")
def login(body: LoginRequest, response: Response):
    user = users_collection.find_one(
        {"email": body.email},
        {"_id": 1, "email": 1, "username": 1, "password": 1, "role": 1, "company": 1, "station_id": 1},
    )
    if not user or not bcrypt.checkpw(body.password.encode("utf-8"), user["password"].encode("utf-8")):
        raise HTTPException(status_code=401, detail="Invalid email or password")

    station_ids = user.get("station_id", [])
    if not isinstance(station_ids, list):
        station_ids = [station_ids]

    # ðŸ‘‡ à¸ªà¸£à¹‰à¸²à¸‡ session id + à¸•à¸µà¸•à¸£à¸²à¹€à¸§à¸¥à¸²
    now = datetime.now(timezone.utc)
    sid = str(uuid.uuid4())
    
    # à¸à¸³à¸«à¸™à¸” token expire time à¸•à¸²à¸¡à¸šà¸—à¸šà¸²à¸—
    user_role = user.get("role", "user")
    if user_role == "technician":
        token_expire_minutes = ACCESS_TOKEN_EXPIRE_MINUTES_TECHNICIAN  # 24 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡
    else:
        token_expire_minutes = ACCESS_TOKEN_EXPIRE_MINUTES_DEFAULT  # 15 à¸™à¸²à¸—à¸µ

    jwt_token = create_access_token({
        "sub": user["email"],
        "user_id": str(user["_id"]),
        "username": user.get("username"),
        "role": user_role,
        "company": user.get("company"),
        "station_ids": station_ids,
        "sid": sid,  # â¬…ï¸ à¹à¸™à¸š session id à¹„à¸§à¹‰à¹ƒà¸™ access token
    }, expires_delta=timedelta(minutes=token_expire_minutes))

    refresh_token = create_access_token({"sub": user["email"]}, expires_delta=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS))

    # âœ… à¸œà¸¹à¸ session à¹ƒà¸™ DB (à¹€à¸à¹‡à¸š lastActiveAt à¹„à¸§à¹‰à¹€à¸Šà¹‡à¸„ idle)
    users_collection.update_one(
        {"_id": user["_id"]},
        {"$set": {
            "refreshTokens": [{
                "sid": sid,
                "token": refresh_token,
                "createdAt": now,
                "lastActiveAt": now,
                "expiresAt": now + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS),
            }]
        }}
    )

    response.set_cookie(
        key=ACCESS_COOKIE_NAME,
        value=jwt_token,
        httponly=True,
        secure=False,
        samesite="lax",
        max_age=int(timedelta(minutes=token_expire_minutes).total_seconds()),
        path="/",
    )

    return {
        "message": "ok",
        "access_token": jwt_token,
        "refresh_token": refresh_token,
        "user": {
            "user_id": str(user["_id"]),
            "username": user.get("username"),
            "email": user["email"],
            "role": user.get("role", "user"),
            "company": user.get("company"),
            "station_id": station_ids,
        }
    }

@app.get("/me")
def me(current: UserClaims = Depends(get_current_user)):
    if not current.user_id:
        raise HTTPException(status_code=401, detail="Missing uid in token")

    u = users_collection.find_one(
        {"_id": ObjectId(current.user_id)},
        {"_id": 1, "username": 1, "email": 1, "role": 1, "company": 1, "tel": 1}
    )
    if not u:
        raise HTTPException(status_code=404, detail="User not found")

    return {
        "id": str(u["_id"]),
        "username": u.get("username") or "",
        "email": u.get("email") or "",
        "role": u.get("role") or "",
        "company": u.get("company") or "",
        "tel": u.get("tel") or "",
    }

@app.get("/my-stations/detail")
def my_stations_detail(current: UserClaims = Depends(get_current_user)):
    proj = {"_id": 0, "station_id": 1, "station_name": 1}

    if current.role == "admin":
        docs = list(station_collection.find({}, proj))
        return {"stations": docs}

    # non-admin â†’ à¸«à¸² station à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸‚à¸­à¸‡ user à¸™à¸µà¹‰ (à¸£à¸­à¸‡à¸£à¸±à¸šà¸—à¸±à¹‰à¸‡ str à¹à¸¥à¸° ObjectId)
    conds = [{"user_id": current.user_id}]
    try:
        conds.append({"user_id": ObjectId(current.user_id)})
    except Exception:
        pass

    docs = list(station_collection.find({"$or": conds}, proj))
    return {"stations": docs}

@app.get("/station/info")
def station_info(
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),   # à¸”à¸¶à¸‡ claims à¸ˆà¸²à¸ JWT
):
    # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™ stations
    doc = station_collection.find_one(
        {"station_id": station_id},
        # à¹€à¸¥à¸·à¸­à¸ field à¸—à¸µà¹ˆà¸­à¸¢à¸²à¸à¸„à¸·à¸™ (à¸•à¸±à¸” _id à¸­à¸­à¸à¹€à¸žà¸·à¹ˆà¸­à¸¥à¸” serialize à¸›à¸±à¸à¸«à¸² ObjectId)
        {
            "_id": 0, 
            "station_id": 1, 
            "station_name": 1, 
            "SN": 1, 
            "WO": 1,
            "brand":1, 
            "PLCFirmware": 1, 
            "PIFirmware": 1, 
            "RTFirmware": 1, 
            "chargeBoxID": 1, 
            "commit_date" :1,
            "warranty_year": 1,
            "model": 1, 
            "status": 1, 
            "module1_isActive":1, 
            "module2_isActive":1, 
            "module3_isActive":1, 
            "module4_isActive":1, 
            "module5_isActive":1, 
            "module6_isActive":1, 
            "module7_isActive":1
        }
    )
    if not doc:
        raise HTTPException(status_code=404, detail="Station not found")

    return {"station": doc}

@app.get("/station/info/public")
def station_info_public(
    station_id: str = Query(...)
):
    doc = station_collection.find_one(
        {"station_id": station_id},
        {"_id": 0, "station_id": 1, "station_name": 1, "SN": 1, "WO": 1,"chargeBoxID": 1,
         "model": 1, "status": 1,"brand":1,"chargerNo":1}
    )
    if not doc:
        raise HTTPException(status_code=404, detail="Station not found")
    return {"station": doc}

@app.get("/get_history")
def get_history(
    station_id: str = Query(...),
    start: str = Query(...),
    end: str = Query(...),
    current: UserClaims = Depends(get_current_user),  # â† à¸­à¹ˆà¸²à¸™à¸ªà¸´à¸—à¸˜à¸´à¹Œà¸ˆà¸²à¸ JWT
):
    # âœ… à¹€à¸Šà¹‡à¸„à¸ªà¸´à¸—à¸˜à¸´à¹Œà¸à¹ˆà¸­à¸™à¸„à¸´à¸§à¸£à¸µà¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡
    if station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

class RefreshIn(BaseModel):
    refresh_token: str

@app.post("/refresh")
def refresh(body: RefreshIn, response: Response):
    try:
        payload = jwt.decode(body.refresh_token, SECRET_KEY, algorithms=[ALGORITHM])
        email = payload.get("sub")
        if not email:
            raise HTTPException(status_code=401, detail="Invalid token")

        user = users_collection.find_one({"email": email})
        if not user:
            raise HTTPException(status_code=401, detail="User not found")

        entry = next((t for t in user.get("refreshTokens", []) if t.get("token") == body.refresh_token), None)
        if not entry:
            raise HTTPException(status_code=401, detail="Invalid refresh token")

        now = datetime.now(timezone.utc)
        if entry.get("expiresAt") and now > entry["expiresAt"]:
            raise HTTPException(status_code=401, detail="refresh_token_expired")

        # à¸à¸³à¸«à¸™à¸” idle timeout à¸•à¸²à¸¡à¸šà¸—à¸šà¸²à¸—
        user_role = user.get("role", "user")
        if user_role == "technician":
            idle_timeout = SESSION_IDLE_MINUTES_TECHNICIAN  # None = à¹„à¸¡à¹ˆà¸¡à¸µ idle timeout
        else:
            idle_timeout = SESSION_IDLE_MINUTES_DEFAULT  # 15 à¸™à¸²à¸—à¸µ
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š idle timeout
        idle_at = entry.get("lastActiveAt")
        if idle_timeout is not None and idle_at and (now - idle_at) > timedelta(minutes=idle_timeout):
            raise HTTPException(status_code=401, detail="session_idle_timeout")

        # à¸à¸³à¸«à¸™à¸” token expire time à¸•à¸²à¸¡à¸šà¸—à¸šà¸²à¸—
        if user_role == "technician":
            token_expire_minutes = ACCESS_TOKEN_EXPIRE_MINUTES_TECHNICIAN  # 24 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡
        else:
            token_expire_minutes = ACCESS_TOKEN_EXPIRE_MINUTES_DEFAULT  # 15 à¸™à¸²à¸—à¸µ

        # à¸ªà¸£à¹‰à¸²à¸‡ access à¹ƒà¸«à¸¡à¹ˆ (à¸„à¸‡ sid à¹€à¸”à¸´à¸¡)
        station_ids = user.get("station_id", [])
        if not isinstance(station_ids, list):
            station_ids = [station_ids]

        new_access = create_access_token({
            "sub": user["email"],
            "user_id": str(user["_id"]),
            "username": user.get("username"),
            "role": user_role,
            "company": user.get("company"),
            "station_ids": station_ids,
            "sid": entry.get("sid"),
        }, expires_delta=timedelta(minutes=token_expire_minutes))

        # à¸­à¸±à¸›à¹€à¸”à¸• lastActiveAt
        users_collection.update_one(
            {"_id": user["_id"], "refreshTokens.token": body.refresh_token},
            {"$set": {"refreshTokens.$.lastActiveAt": now}}
        )

        # âš ï¸ à¸•à¸±à¹‰à¸‡à¸„à¸¸à¸à¸à¸µà¹‰ access à¹ƒà¸«à¸¡à¹ˆà¹ƒà¸«à¹‰ SSE à¸—à¸³à¸‡à¸²à¸™à¸•à¹ˆà¸­à¹„à¸”à¹‰
        response.set_cookie(
            key=ACCESS_COOKIE_NAME,
            value=new_access,
            httponly=True,
            secure=False,          # à¹‚à¸›à¸£à¸”à¸”à¸¹à¸‚à¹‰à¸­ 2 à¸”à¹‰à¸²à¸™à¸¥à¹ˆà¸²à¸‡
            samesite="lax",        # à¹‚à¸›à¸£à¸”à¸”à¸¹à¸‚à¹‰à¸­ 2 à¸”à¹‰à¸²à¸™à¸¥à¹ˆà¸²à¸‡
            max_age=int(timedelta(minutes=token_expire_minutes).total_seconds()),
            path="/",
        )
        return {"access_token": new_access}
    except ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="refresh_token_expired")
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
    
@app.post("/logout")
async def logout(email: str, refresh_token: str):
    result = users_collection.update_one(
        {"email": email, "refreshTokens.token": refresh_token},
        {"$pull": {"refreshTokens": {"token": refresh_token}}}
    )
    if result.modified_count == 0:
        raise HTTPException(status_code=400, detail="Token not found or already logged out")
    return {"msg": "Logged out successfully"}





@app.get("/username")
async def users():
    # âœ… à¸”à¸¶à¸‡à¸¡à¸²à¹€à¸‰à¸žà¸²à¸° role = "owner"
    cursor = users_collection.find({"role": "owner"})
    usernames = [u["username"] for u in cursor]

    if not usernames:
        raise HTTPException(status_code=404, detail="owners not found")

    return {"username": usernames}
    
class register(BaseModel):
    username: str
    email: str
    password: str
    tel: str
    company: str
#create
@app.post("/insert_users/")
async def create_users(users: register):
    # hash password
    hashed_pw = bcrypt.hashpw(users.password.encode("utf-8"), bcrypt.gensalt()).decode("utf-8")
    
    now = datetime.now(timezone.utc)

    users_collection.insert_one(
    {
        "username" : users.username,
        "email":users.email,
        "password":hashed_pw,
        "tel":users.tel,
        "refreshTokens": [],
        "role":"Technician",
        "company":users.company,
        "createdAt": now,   # âœ… à¹€à¸žà¸´à¹ˆà¸¡
        "updatedAt": now,   # âœ… à¹€à¸žà¸´à¹ˆà¸¡
    })

@app.get("/stations/")
async def get_stations(q:str = ""):
    """à¸„à¹‰à¸™à¸«à¸²à¸ªà¸–à¸²à¸™à¸™à¸µ"""
    query = {"station_name":{"$regex":  q, "$options": "i"}} if q else {}
    stations = station_collection.find(query,{"_id":0,"station_name":1})
    return [station["station_name"] for station in stations]

def to_json(doc: dict | None) -> str:
    if not doc:
        return "{}"
    d = dict(doc)
    d.pop("password", None)
    if isinstance(d.get("_id"), ObjectId):
        d["_id"] = str(d["_id"])
    return json.dumps(d, ensure_ascii=False, default=str)

# oauth2_scheme = OAuth2PasswordBearer(tokenUrl="login")  # à¸Šà¸µà¹‰à¹„à¸› endpoint login
# decode JWT 
    
@app.get("/owner/stations/")
async def get_stations(q: str = "", current: UserClaims = Depends(get_current_user)):
    # current_user à¸„à¸·à¸­ str(_id)
    user_obj_id = ObjectId(current.user_id)

    # à¸”à¸¶à¸‡ station_id à¸‚à¸­à¸‡ user
    user = users_collection.find_one({"_id": user_obj_id}, {"station_id": 1})
    if not user or "station_id" not in user:
        return []

    station_ids = user["station_id"]

    # filter stations à¸•à¸²à¸¡ station_id à¸‚à¸­à¸‡ user + query
    query_filter = {"station_id": {"$in": station_ids}}
    if q:
        query_filter["station_name"] = {"$regex": q, "$options": "i"}

    stations = station_collection.find(query_filter, {"_id": 0, "station_name": 1, "station_id": 1})
    return [{"station_name": s["station_name"], "station_id": s["station_id"]} for s in stations]


# @app.get("/selected/station/{station_id}")
# async def get_station_detail(station_id: str, current: UserClaims = Depends(get_current_user)):
#     station = station_collection.find_one({"station_id": station_id})
#     if not station:
#         raise HTTPException(status_code=404, detail="Station not found")

#     # âœ… à¹à¸›à¸¥à¸‡ _id à¹€à¸›à¹‡à¸™ string
#     station["_id"] = str(station["_id"])

#     return station

@app.get("/selected/station/{station_id}")
async def get_station_detail(station_id: str, current: UserClaims = Depends(get_current_user)):
    station = station_collection.find_one({"station_id": station_id})
    if not station:
        raise HTTPException(status_code=404, detail="Station not found")

    # à¹à¸›à¸¥à¸‡à¸—à¸¸à¸à¸­à¸¢à¹ˆà¸²à¸‡à¹ƒà¸«à¹‰ JSON à¹„à¸”à¹‰
    return jsonable_encoder(
        station,
        custom_encoder={
            ObjectId: str,
            datetime: lambda v: v.isoformat()
        }
    )

async def mdb_query(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    """
    SSE à¹à¸šà¸š query param:
    - à¸ªà¹ˆà¸‡ snapshot à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸—à¸±à¸™à¸—à¸µ (event: init)
    - à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™ polling à¸‚à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¹€à¸›à¹‡à¸™à¸Šà¹ˆà¸§à¸‡ à¹†
    """
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }
    coll = get_mdb_collection_for(station_id)

    async def event_generator():
        last_id = None
        latest = await coll.find_one({}, sort=[("_id", -1)])  # â¬…ï¸ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ filter station_id à¸ à¸²à¸¢à¹ƒà¸™à¹à¸¥à¹‰à¸§
        if latest:
            latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
            last_id = latest.get("_id")
            yield "retry: 3000\n"
            yield "event: init\n"
            yield f"data: {to_json(latest)}\n\n"
        else:
            yield "retry: 3000\n\n"

        while True:
            if await request.is_disconnected():
                break

            doc = await coll.find_one({}, sort=[("_id", -1)])
            if doc and doc.get("_id") != last_id:
                doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                last_id = doc.get("_id")
                yield f"data: {to_json(doc)}\n\n"
            else:
                yield ": keep-alive\n\n"

            await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

def _coerce_date_range(start: str, end: str) -> tuple[str, str]:
    def _norm(s: str, is_end: bool=False) -> str:
        if "T" not in s:  # à¸§à¸±à¸™à¸¥à¹‰à¸§à¸™
            hhmmss = "23:59:59.999" if is_end else "00:00:00.000"
            dt = datetime.fromisoformat(f"{s}T{hhmmss}+07:00")
            # print("521",dt)
            # print("522",dt.astimezone(timezone.utc).isoformat())
            iso_th = dt.astimezone(th_tz).isoformat()
            # print("iso_th", iso_th)

            test = dt.astimezone(timezone.utc).isoformat()
            # print("526",dt)
            # print("527",type(dt.astimezone(timezone.utc).isoformat().replace("+00:00", "T")))
            # print("528",type(dt))
            # return dt.astimezone(timezone.utc).isoformat().replace("+07:00", "T")
            return iso_th
            
            # return dt
        # à¸¡à¸µ T à¹à¸¥à¹‰à¸§ à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ timezone â†’ à¸–à¸·à¸­à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢
        has_tz = bool(re.search(r'(Z|[+\-]\d{2}:\d{2})$', s))
        if not has_tz:
            dt = datetime.fromisoformat(s + "+07:00")
            # print("528",dt)
        else:
            dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
            # print("531",datetime.fromisoformat(s))
            # print("532",dt)
        # print("533",dt.astimezone(timezone.utc).isoformat())    
        return dt.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")
    try:
        return _norm(start, False), _norm(end, True)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad date range")
    
@app.get("/MDB/history")
async def stream_history(
    request: Request,
    station_id: str = Query(...),
    start: str = Query(...),  
    end: str = Query(...),    
    every: str = Query("5m"), 
    current: UserClaims = Depends(get_current_user),
):
    if not re.fullmatch(r"\d{4}-\d{2}-\d{2}", start) or not re.fullmatch(r"\d{4}-\d{2}-\d{2}", end):
        raise HTTPException(status_code=400, detail="start/end must be YYYY-MM-DD")
    if start > end:
        start, end = end, start

    tz_th = ZoneInfo("Asia/Bangkok")
    now_th = datetime.now(tz_th)

    def coerce_day_bound_th(datestr: str, bound: Literal["start", "end"], now_th: datetime) -> datetime:
        tz_th = ZoneInfo("Asia/Bangkok")
        # à¸§à¸±à¸™à¸—à¸µà¹ˆà¸¥à¹‰à¸§à¸™ â†’ à¸šà¸±à¸‡à¸„à¸±à¸šà¸‚à¸­à¸šà¸§à¸±à¸™à¹„à¸—à¸¢
        if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", datestr):
            if bound == "start":
                dt_th = datetime.fromisoformat(f"{datestr}T00:00:00").replace(tzinfo=tz_th)
                return dt_th.astimezone(timezone.utc)
            else:
                # à¸–à¹‰à¸² end à¹€à¸›à¹‡à¸™ "à¸§à¸±à¸™à¸™à¸µà¹‰" â†’ à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™à¸‚à¸­à¸‡à¹„à¸—à¸¢ (à¸à¸±à¸™à¸à¸£à¸²à¸Ÿà¸¥à¸²à¸à¹„à¸›à¸­à¸™à¸²à¸„à¸•)
                if datestr == now_th.strftime("%Y-%m-%d"):
                    return now_th.astimezone(timezone.utc)
                # à¸¡à¸´à¸‰à¸°à¸™à¸±à¹‰à¸™ à¸›à¸¥à¸²à¸¢à¸§à¸±à¸™à¹„à¸—à¸¢
                dt_th = datetime.fromisoformat(f"{datestr}T23:59:59.999").replace(tzinfo=tz_th)
                return dt_th.astimezone(timezone.utc)

        # à¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸²à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ (Z à¸«à¸£à¸·à¸­ Â±HH:MM) â†’ à¹ƒà¸Šà¹‰à¸•à¸²à¸¡à¸™à¸±à¹‰à¸™
        if re.search(r"(Z|[+\-]\d{2}:\d{2})$", datestr):
            return datetime.fromisoformat(datestr.replace("Z", "+00:00")).astimezone(timezone.utc)

        # à¹€à¸›à¹‡à¸™ datetime à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™ â†’ à¸•à¸µà¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¹„à¸—à¸¢ à¹à¸¥à¹‰à¸§à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ UTC
        return datetime.fromisoformat(datestr).replace(tzinfo=tz_th).astimezone(timezone.utc)

    def ensure_dt_with_current_time_th(datestr: str) -> datetime:
        """
        - 'YYYY-MM-DD'                  -> à¹€à¸•à¸´à¸¡à¹€à¸§à¸¥à¸²à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™à¸‚à¸­à¸‡à¹„à¸—à¸¢ à¹à¸¥à¹‰à¸§à¸•à¸µà¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢ (+07:00)
        - 'YYYY-MM-DDTHH[:MM[:SS]]'    -> à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸² à¹ƒà¸«à¹‰à¸•à¸µà¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢ (+07:00)
        - à¸¥à¸‡à¸—à¹‰à¸²à¸¢à¸”à¹‰à¸§à¸¢ 'Z' à¸«à¸£à¸·à¸­à¸¡à¸µ '+/-HH:MM' -> à¹ƒà¸Šà¹‰à¹‚à¸‹à¸™à¸—à¸µà¹ˆà¸¡à¸²à¸à¸±à¸šà¸ªà¸•à¸£à¸´à¸‡
        à¹à¸¥à¹‰à¸§à¸„à¸·à¸™à¸„à¹ˆà¸²à¹€à¸›à¹‡à¸™ UTC datetime
        """
        # à¹€à¸•à¸´à¸¡à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™ à¸«à¸²à¸à¹€à¸›à¹‡à¸™à¸§à¸±à¸™à¸—à¸µà¹ˆà¸¥à¹‰à¸§à¸™
        if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", datestr):
            datestr = f"{datestr}T{now_th.strftime('%H:%M:%S')}"

        # à¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸²à¹à¸¥à¹‰à¸§ (Z à¸«à¸£à¸·à¸­ +/-HH:MM)
        if re.search(r"(Z|[+\-]\d{2}:\d{2})$", datestr):
            # à¸£à¸­à¸‡à¸£à¸±à¸š 'Z' à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ +00:00
            dt = datetime.fromisoformat(datestr.replace("Z", "+00:00"))
            return dt.astimezone(timezone.utc)

        # à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸² -> à¸•à¸µà¸„à¸§à¸²à¸¡à¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ "à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢" (+07:00) à¹à¸¥à¹‰à¸§à¸„à¹ˆà¸­à¸¢à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ UTC
        naive = datetime.fromisoformat(datestr)        # à¹„à¸¡à¹ˆà¸œà¸¹à¸à¹‚à¸‹à¸™à¸à¹ˆà¸­à¸™
        dt_th = naive.replace(tzinfo=tz_th)            # â† à¸•à¸£à¸‡à¸™à¸µà¹‰à¸„à¸·à¸­à¸à¸²à¸£ â€œà¸œà¸¹à¸ +07:00â€
        return dt_th.astimezone(timezone.utc)          # â† à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ UTC (à¸œà¸¥ = à¸¥à¸š 7 à¸Šà¸¡.)

    def _ensure_iso_with_tz(val: Any, tz: ZoneInfo) -> str | None:
        """
        à¸£à¸±à¸šà¸„à¹ˆà¸² datetime (à¸«à¸£à¸·à¸­à¸ªà¸•à¸£à¸´à¸‡) à¹à¸¥à¹‰à¸§à¸„à¸·à¸™à¸„à¹ˆà¸² ISO8601 à¸—à¸µà¹ˆà¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸²à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢ (à¹€à¸Šà¹ˆà¸™ +07:00)
        """
        if val is None:
            return None
        if isinstance(val, str):
            try:
                # parse à¸—à¸±à¹‰à¸‡à¸à¸£à¸“à¸µà¸¡à¸µ Z/offset à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™
                dt = datetime.fromisoformat(val.replace("Z", "+00:00")) if re.search(r"(Z|[+\-]\d{2}:\d{2})$", val) \
                    else datetime.fromisoformat(val).replace(tzinfo=timezone.utc)
            except Exception:
                return val  # à¸–à¹‰à¸² parse à¹„à¸¡à¹ˆà¹„à¸”à¹‰ à¸à¹‡à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸šà¹€à¸”à¸´à¸¡ (à¸à¸±à¸™à¸žà¸±à¸‡)
        elif isinstance(val, datetime):
            dt = val if val.tzinfo else val.replace(tzinfo=timezone.utc)
        else:
            return None
        return dt.astimezone(tz).isoformat(timespec="milliseconds")
        
    UNIT_MAP = {"s": "second", "m": "minute", "h": "hour"}

    def parse_every(s: str) -> tuple[str, int]:
        """
        à¹à¸›à¸¥à¸‡à¸ªà¸•à¸£à¸´à¸‡à¹€à¸Šà¹ˆà¸™ '5m', '15m', '1h' â†’ (unit, binSize)
        à¸”à¸µà¸Ÿà¸­à¸¥à¸•à¹Œ = ('minute', 5)
        """
        m = re.fullmatch(r"(\d+)([smh])$", s.strip())
        if not m:
            return ("minute", 5)
        n, u = int(m.group(1)), m.group(2)
        return (UNIT_MAP[u], max(1, n))
    # start_utc = ensure_dt_with_current_time_th(start)
    # print("565",start_utc)
    # end_utc   = ensure_dt_with_current_time_th(end)
    # print("567",end_utc)

    start_utc = coerce_day_bound_th(start, "start", now_th)
    end_utc   = coerce_day_bound_th(end,   "end",   now_th)

    coll = get_mdb_collection_for(station_id)

    # à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ $regexReplace/$replaceOne â€” à¹à¸¢à¸ case à¸”à¹‰à¸§à¸¢ $regexMatch + $toDate/$dateFromString
    def _parse_string(varname: str):
        # à¸–à¹‰à¸²à¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸² (Z à¸«à¸£à¸·à¸­ Â±HH:MM) â†’ à¹ƒà¸«à¹‰ Mongo à¹à¸›à¸¥à¸‡à¹€à¸­à¸‡à¸”à¹‰à¸§à¸¢ $toDate
        # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸² â†’ à¸•à¸µà¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢à¸”à¹‰à¸§à¸¢ $dateFromString timezone "+07:00"
        return {
            "$cond": [
                { "$regexMatch": { "input": f"$${varname}", "regex": r"(Z|[+\-]\d{2}:\d{2})$" } },
                { "$toDate": f"$${varname}" },
                { "$dateFromString": {
                    "dateString": f"$${varname}",
                    "timezone": "+07:00",
                    "onError": None,
                    "onNull": None
                } }
            ]
        }

    # à¹à¸›à¸¥à¸‡à¸„à¹ˆà¸² sampling interval
    unit, bin_size = parse_every(every)

    # ---------- prefix à¹€à¸”à¸´à¸¡: ts/dayTH + match 2 à¸Šà¸±à¹‰à¸™ ----------
    prefix = [
        {   # âœ… ts: à¹à¸›à¸¥à¸‡ timestamp/Datetime à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ Date
            "$addFields": {
                "ts": {
                    "$let": { "vars": { "t": "$timestamp", "d": "$Datetime" }, "in":
                        { "$cond": [
                            { "$ne": ["$$t", None] },
                            { "$switch": {
                                "branches": [
                                    { "case": { "$eq": [ { "$type": "$$t" }, "date"   ] }, "then": "$$t" },
                                    { "case": { "$eq": [ { "$type": "$$t" }, "string" ] }, "then": {
                                        "$cond": [
                                            { "$regexMatch": { "input": "$$t", "regex": r"(Z|[+\-]\d{2}:\d{2})$" } },
                                            { "$toDate": "$$t" },
                                            { "$dateFromString": { "dateString": "$$t", "timezone": "+07:00", "onError": None, "onNull": None } }
                                        ]
                                    }},
                                ],
                                "default": None
                            }},
                            { "$switch": {
                                "branches": [
                                    { "case": { "$eq": [ { "$type": "$$d" }, "date"   ] }, "then": "$$d" },
                                    { "case": { "$eq": [ { "$type": "$$d" }, "string" ] }, "then": {
                                        "$cond": [
                                            { "$regexMatch": { "input": "$$d", "regex": r"(Z|[+\-]\d{2}:\d{2})$" } },
                                            { "$toDate": "$$d" },
                                            { "$dateFromString": { "dateString": "$$d", "timezone": "+07:00", "onError": None, "onNull": None } }
                                        ]
                                    }},
                                ],
                                "default": None
                            }}
                        ] }
                    }
                }
            }
        },
        { "$addFields": { "dayTH": { "$dateToString": { "date": "$ts", "format": "%Y-%m-%d", "timezone": "+07:00" }}}},
        { "$match": { "dayTH": { "$gte": start, "$lte": end } }},  # à¸Šà¸±à¹‰à¸™à¸—à¸µà¹ˆ 1: à¸§à¸±à¸™à¹„à¸—à¸¢
        { "$match": { "$expr": { "$and": [
            { "$gte": ["$ts", start_utc] },  # à¸Šà¸±à¹‰à¸™à¸—à¸µà¹ˆ 2: ts à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸Šà¹ˆà¸§à¸‡
            { "$lte": ["$ts", end_utc] }
        ]}}}
    ]

    # ---------- downsample à¸”à¹‰à¸§à¸¢ $dateTrunc ----------
    group_stage = {
        "$group": {
            "_id": {
                "bucket": {
                    "$dateTrunc": {
                        "date": "$ts",
                        "unit": unit,         # second / minute / hour
                        "binSize": bin_size,  # à¹€à¸Šà¹ˆà¸™ 5
                        "timezone": "+07:00"  # à¸­à¸´à¸‡à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢
                    }
                }
            },
            # à¸„à¹ˆà¸²à¹€à¸‰à¸¥à¸µà¹ˆà¸¢à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸à¸£à¸²à¸Ÿà¸ªà¸¡à¸¹à¸— (à¸à¸±à¸™à¸à¸£à¸“à¸µà¸„à¹ˆà¸²à¸¡à¸²à¹€à¸›à¹‡à¸™ string à¸”à¹‰à¸§à¸¢ $convert)
            "VL1N": {"$avg": {"$convert": {"input": "$VL1N", "to": "double", "onError": None, "onNull": None}}},
            "VL2N": {"$avg": {"$convert": {"input": "$VL2N", "to": "double", "onError": None, "onNull": None}}},
            "VL3N": {"$avg": {"$convert": {"input": "$VL3N", "to": "double", "onError": None, "onNull": None}}},
            "I1":   {"$avg": {"$convert": {"input": "$I1",   "to": "double", "onError": None, "onNull": None}}},
            "I2":   {"$avg": {"$convert": {"input": "$I2",   "to": "double", "onError": None, "onNull": None}}},
            "I3":   {"$avg": {"$convert": {"input": "$I3",   "to": "double", "onError": None, "onNull": None}}},
            "PL1N": {"$avg": {"$convert": {"input": "$PL1N", "to": "double", "onError": None, "onNull": None}}},
            "PL2N": {"$avg": {"$convert": {"input": "$PL2N", "to": "double", "onError": None, "onNull": None}}},
            "PL3N": {"$avg": {"$convert": {"input": "$PL3N", "to": "double", "onError": None, "onNull": None}}},
        }
    }
    sort_stage = { "$sort": { "_id.bucket": 1 } }
    project_stage = {
        "$project": {
            "_id": 0,
            "timestamp": "$_id.bucket",  # à¸«à¸£à¸·à¸­ "$_id.bucket" à¸–à¹‰à¸²à¹ƒà¸Šà¹‰ dateTrunc
            "VL1N": 1, "VL2N": 1, "VL3N": 1,
            "I1": 1, "I2": 1, "I3": 1,
            "PL1N": 1, "PL2N": 1, "PL3N": 1,
        }
    }

    pipeline = prefix + [group_stage, sort_stage, project_stage]
    cursor = coll.aggregate(pipeline, allowDiskUse=True)


    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    async def event_generator():
        try:
            # base = pipeline[:-2]
            # cnt = await coll.aggregate(base + [{"$count": "n"}]).to_list(length=1)
            # n = cnt[0]["n"] if cnt else 0
            cnt = await coll.aggregate(prefix + [group_stage, {"$count":"n"}]).to_list(length=1)
            n = cnt[0]["n"] if cnt else 0

            yield "retry: 3000\n"
            yield f"event: stats\ndata: {json.dumps({'matched': n})}\n\n"

            sent = 0
            async for doc in cursor:
                if await request.is_disconnected():
                    break

                # âœ… à¸­à¸¢à¹ˆà¸²à¹à¸•à¸° _id à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ (à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ KeyError)
                # à¸–à¹‰à¸²à¸ˆà¸³à¹€à¸›à¹‡à¸™à¸ˆà¸£à¸´à¸‡ à¹† à¸„à¹ˆà¸­à¸¢à¸—à¸³à¹à¸šà¸šà¸›à¸¥à¸­à¸”à¸ à¸±à¸¢:
                # _id_val = doc.get("_id", None)
                # if _id_val is not None:
                #     try:
                #         doc["_id"] = str(_id_val)
                #     except Exception:
                #         doc["_id"] = json.dumps(_id_val, default=str, ensure_ascii=False)

                # âœ… à¹ƒà¸«à¹‰à¹à¸™à¹ˆà¹ƒà¸ˆà¸§à¹ˆà¸² timestamp à¹€à¸›à¹‡à¸™ ISO string (à¸£à¸­à¸‡à¸£à¸±à¸šà¸—à¸±à¹‰à¸‡ Date/str)
                ts_val = doc.get("timestamp")
                if ts_val is not None:
                    doc["timestamp"] = _ensure_iso_with_tz(ts_val, ZoneInfo("Asia/Bangkok"))

                yield f"data: {json.dumps(doc, ensure_ascii=False)}\n\n"
                sent += 1
                await asyncio.sleep(0.001)

            if sent == 0:
                yield "event: empty\ndata: no documents in range\n\n"
            else:
                yield ": keep-alive\n\n"
        except Exception as e:
            yield f"event: server-error\ndata: {json.dumps({'message': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream", headers=headers)

@app.get("/MDB/history/debug")
async def mdb_history_debug(station_id: str, start: str, end: str):
    start_iso, end_iso = _coerce_date_range(start, end)
    start_key, end_key = start_iso.rstrip("Z"), end_iso.rstrip("Z")
    coll = get_mdb_collection_for(station_id)
    q = {"timestamp": {"$gte": start_key, "$lte": end_key}}
    docs = await coll.find(q, {"_id":0,"timestamp":1}).sort("timestamp", 1).limit(5).to_list(length=5)
    n = await coll.count_documents(q)
    return {"matched": n, "start_key": start_key, "end_key": end_key, "sample": docs}

def extract_token(authorization: str | None, access_token: str | None):
    if authorization and authorization.startswith("Bearer "):
        return authorization.split(" ", 1)[1]
    if access_token:
        return access_token
    raise HTTPException(status_code=401, detail="Not authenticated")
    
# @app.get("/MDB/{station_id}")
# async def mdb(request: Request, station_id: str, current: UserClaims = Depends(get_current_user)):
#     headers = {
#         "Content-Type": "text/event-stream",
#         "Cache-Control": "no-cache",
#         "Connection": "keep-alive",
#         "X-Accel-Buffering": "no",
#     }

#     coll = get_mdb_collection_for(station_id)  # â¬…ï¸ à¹ƒà¸Šà¹‰ coll à¸•à¸²à¸¡à¸ªà¸–à¸²à¸™à¸µ

#     async def event_generator():
#         last_id = None

#         latest = await coll.find_one({}, sort=[("_id", -1)])
#         if latest:
#             latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
#             last_id = latest.get("_id")
#             yield f"event: init\ndata: {to_json(latest)}\n\n"
#         else:
#             yield ": keep-alive\n\n"

#         while True:
#             if await request.is_disconnected():
#                 break

#             doc = await coll.find_one({}, sort=[("_id", -1)])
#             if doc and doc.get("_id") != last_id:
#                 doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
#                 last_id = doc.get("_id")
#                 yield f"data: {to_json(doc)}\n\n"
#             else:
#                 yield ": keep-alive\n\n"

#             await asyncio.sleep(1)

#     return StreamingResponse(event_generator(), headers=headers)
@app.get("/MDB/{station_id}")
async def mdb(request: Request, station_id: str, current: UserClaims = Depends(get_current_user)):
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    coll = get_mdb_collection_for(station_id)

    async def event_generator():
        # à¸ªà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸à¹ˆà¸­à¸™
        latest = await coll.find_one({}, sort=[("_id", -1)])
        if latest:
            latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
            yield f"event: init\ndata: {to_json(latest)}\n\n"
        else:
            yield ": keep-alive\n\n"

        # à¹€à¸›à¸´à¸” Change Stream à¹€à¸žà¸·à¹ˆà¸­à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹à¸šà¸š real-time
        pipeline = [
            {"$match": {"operationType": "insert"}}  # à¸Ÿà¸±à¸‡à¹€à¸‰à¸žà¸²à¸° insert
        ]
        
        try:
            async with coll.watch(pipeline) as stream:
                async for change in stream:
                    if await request.is_disconnected():
                        break
                    
                    doc = change["fullDocument"]
                    doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                    yield f"data: {to_json(doc)}\n\n"
                    
        except Exception as e:
            print(f"Change stream error: {e}")
            # Fallback à¹€à¸›à¹‡à¸™ polling à¸–à¹‰à¸² Change Stream à¹„à¸¡à¹ˆà¸—à¸³à¸‡à¸²à¸™
            last_id = latest.get("_id") if latest else None
            while True:
                if await request.is_disconnected():
                    break
                    
                doc = await coll.find_one({}, sort=[("_id", -1)])
                if doc and doc.get("_id") != last_id:
                    doc["timestamp"] = doc.get("timestamp")
                    last_id = doc.get("_id")
                    yield f"data: {to_json(doc)}\n\n"
                else:
                    yield ": keep-alive\n\n"
                    
                await asyncio.sleep(1)

    return StreamingResponse(event_generator(), headers=headers)

async def _resolve_user_id_by_chargebox(chargebox_id: Optional[str]) -> Optional[str]:
    if not chargebox_id:
        return None
    doc = await stations_coll_async.find_one(
        {"chargeBoxID": chargebox_id},
        projection={"user_id": 1}
    )
    if not doc:
        return None
    return str(doc.get("user_id")) if doc.get("user_id") is not None else None

async def _resolve_user_email_by_user_id(user_id: Optional[str]) -> Optional[str]:
    if not user_id:
        return None

    queries = []
    if ObjectId.is_valid(user_id):
        queries.append({"_id": ObjectId(user_id)})
    queries.append({"_id": user_id})  # à¹€à¸œà¸·à¹ˆà¸­à¸à¸£à¸“à¸µà¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ string

    for q in queries:
        doc = await users_coll_async.find_one(q, projection={"email": 1})
        if doc:
            return doc.get("email")
    return None

async def _send_email_async(to_email: str, subject: str, body: str) -> None:
    msg = EmailMessage()
    msg["From"] = SENDER_EMAIL
    msg["To"] = to_email
    msg["Subject"] = subject
    msg.set_content(body)

    # STARTTLS (port 587). à¸–à¹‰à¸²à¹ƒà¸Šà¹‰ SMTPS (465) à¹ƒà¸«à¹‰à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™ use_tls=True à¹à¸¥à¸°à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ starttls
    await aiosmtplib.send(
        msg,
        hostname=SMTP_HOST,
        port=SMTP_PORT,
        start_tls=True,
        username=SMTP_USER,
        password=SMTP_PASS,
        timeout=30,
    )

async def send_error_email_once(to_email: str | None, chargebox_id: str | None, error_text: str | None, doc_id) -> bool:
    """
    à¸ªà¹ˆà¸‡à¹€à¸¡à¸¥à¹à¸ˆà¹‰à¸‡ error à¹à¸„à¹ˆà¸„à¸£à¸±à¹‰à¸‡à¹€à¸”à¸µà¸¢à¸§à¸•à¹ˆà¸­à¹€à¸­à¸à¸ªà¸²à¸£ error (_id)
    - à¹ƒà¸Šà¹‰ collection iMPS.errorEmailLog à¹€à¸à¹‡à¸š _id à¹€à¸›à¹‡à¸™ unique key
    - à¸–à¹‰à¸²à¸ªà¹ˆà¸‡à¸ªà¸³à¹€à¸£à¹‡à¸ˆà¸­à¸±à¸›à¹€à¸”à¸•à¸ªà¸–à¸²à¸™à¸°à¹€à¸›à¹‡à¸™ sent
    - à¸–à¹‰à¸²à¸ªà¹ˆà¸‡à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§ à¸¥à¸šà¸¥à¹‡à¸­à¸à¸­à¸­à¸à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸žà¸¢à¸²à¸¢à¸²à¸¡à¹ƒà¸«à¸¡à¹ˆà¸„à¸£à¸±à¹‰à¸‡à¸«à¸™à¹‰à¸²
    """
    if not to_email or not error_text:
        return False

    key = str(doc_id)  # à¸£à¸­à¸‡à¸£à¸±à¸š ObjectId/str
    now_th = datetime.now(th_tz)

    # à¸‚à¸±à¹‰à¸™à¸—à¸µà¹ˆ 1: à¸à¸±à¸™à¸‹à¹‰à¸³à¸”à¹‰à¸§à¸¢à¸à¸²à¸£ insert à¸¥à¹‡à¸­à¸ (pending) à¸–à¹‰à¸²à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ -> à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸ªà¹ˆà¸‡
    try:
        await email_log_coll.insert_one({
            "_id": key,                    # à¸—à¸³à¹ƒà¸«à¹‰ unique key à¹€à¸›à¹‡à¸™ doc_id à¸‚à¸­à¸‡ error
            "status": "pending",
            "to": to_email,
            "chargeBoxID": chargebox_id,
            "createdAt": now_th,
        })
    except DuplicateKeyError:
        return False  # à¹€à¸„à¸¢à¸ªà¹ˆà¸‡à¸«à¸£à¸·à¸­à¸à¸³à¸¥à¸±à¸‡à¸ªà¹ˆà¸‡à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§

    # à¸‚à¸±à¹‰à¸™à¸—à¸µà¹ˆ 2: à¸ªà¸£à¹‰à¸²à¸‡ subject/body à¹à¸¥à¹‰à¸§à¸ªà¹ˆà¸‡à¸­à¸µà¹€à¸¡à¸¥
    subject = f"[IMPS Error] {chargebox_id or '-'}"
    body = (
        f"à¹€à¸£à¸µà¸¢à¸™à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰,\n\n"
        f"à¸¡à¸µ Error à¸ˆà¸²à¸à¸ªà¸–à¸²à¸™à¸µ/à¸­à¸¸à¸›à¸à¸£à¸“à¹Œ: {chargebox_id or '-'}\n"
        f"à¹€à¸§à¸¥à¸² (TH): {now_th:%Y-%m-%d %H:%M:%S}\n\n"
        f"à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”:\n{error_text}\n\n"
        f"-- à¸£à¸°à¸šà¸š iMPS"
    )
    try:
        await _send_email_async(to_email, subject, body)
        await email_log_coll.update_one({"_id": key}, {"$set": {"status": "sent", "sentAt": datetime.now(th_tz)}})
        return True
    except Exception:
        # à¸–à¹‰à¸²à¸ªà¹ˆà¸‡à¸¥à¹‰à¸¡à¹€à¸«à¸¥à¸§ à¸¥à¸šà¸¥à¹‡à¸­à¸ pending à¸­à¸­à¸ à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸¥à¸­à¸‡à¸ªà¹ˆà¸‡à¹ƒà¸«à¸¡à¹ˆà¹„à¸”à¹‰à¹ƒà¸™à¸£à¸­à¸šà¸–à¸±à¸”à¹„à¸›
        await email_log_coll.delete_one({"_id": key})
        raise



@app.get("/error/{station_id}")
async def error_stream(request: Request, station_id: str, current: UserClaims = Depends(get_current_user)):
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    coll = get_errorCode_collection_for(station_id)

    async def event_generator():
        last_id = None

        # ----- init -----
        latest = await coll.find_one({}, sort=[("_id", -1)])
        if latest and ("error" in latest):
            last_id = latest.get("_id")

            chargebox_id = latest.get("Chargebox_ID")
            user_id = await _resolve_user_id_by_chargebox(chargebox_id)
            email = await _resolve_user_email_by_user_id(user_id)

            try:
                await send_error_email_once(email, chargebox_id, latest.get("error"), last_id)
            except Exception as e:
                # à¹„à¸¡à¹ˆà¹ƒà¸«à¹‰à¸•à¸à¸ªà¸•à¸£à¸µà¸¡: log à¹à¸¥à¹‰à¸§à¹„à¸›à¸•à¹ˆà¸­
                print(f"[email] init send failed for {last_id}: {e}")

            payload = {
                "Chargebox_ID": chargebox_id,
                "user_id": user_id,
                "email": email,
                "error": latest.get("error"),
            }
            # yield f"event: init\ndata: {to_json(payload)}\n\n"
        else:
            yield ": keep-alive\n\n"

        # ----- updates -----
        while True:
            if await request.is_disconnected():
                break

            doc = await coll.find_one({}, sort=[("_id", -1)])
            if doc and doc.get("_id") != last_id and ("error" in doc):
                last_id = doc.get("_id")

                chargebox_id = doc.get("Chargebox_ID")
                user_id = await _resolve_user_id_by_chargebox(chargebox_id)
                email = await _resolve_user_email_by_user_id(user_id)
                
                try:
                    await send_error_email_once(email, chargebox_id, doc.get("error"), last_id)
                except Exception as e:
                    print(f"[email] update send failed for {last_id}: {e}")

                payload = {
                    "Chargebox_ID": chargebox_id,
                    "user_id": user_id,
                    "email": email,
                    "error": doc.get("error"),
                }
                # yield f"data: {to_json(payload)}\n\n"
            else:
                yield ": keep-alive\n\n"

            await asyncio.sleep(60)

    return StreamingResponse(event_generator(), headers=headers)

def parse_iso_dt(s: str) -> datetime:
    try:
        return datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        raise HTTPException(status_code=400, detail=f"Bad datetime: {s}")


def _to_utc_dt(iso_str: str) -> datetime:
    # à¸£à¸±à¸š ISO à¸—à¸µà¹ˆà¸­à¸²à¸ˆà¸¥à¸‡à¸—à¹‰à¸²à¸¢ Z à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸à¹‡à¹„à¸”à¹‰ à¹à¸¥à¹‰à¸§à¸šà¸±à¸‡à¸„à¸±à¸šà¹€à¸›à¹‡à¸™ aware UTC
    s = iso_str
    if s.endswith("Z"):
        s = s.replace("Z", "+00:00")
    dt = datetime.fromisoformat(s)  # à¹„à¸”à¹‰à¸—à¸±à¹‰à¸‡ aware/naive
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    else:
        dt = dt.astimezone(timezone.utc)
    return dt

def to_float(x, default=0.0):
    try:
        if x is None:
            return default
        if isinstance(x, (int, float)):
            return float(x)
        if isinstance(x, Decimal128):
            return float(x.to_decimal())
        s = str(x).strip().replace(",", ".")
        return float(s)
    except Exception:
        return default

async def change_stream_generator(station_id: str):
    coll = get_mdb_collection_for(station_id)
    async with coll.watch() as cs:
        async for change in cs:
            doc = change.get("fullDocument")
            if not doc:
                continue
            payload = {
                "t": _ensure_utc_iso(doc.get("timestamp")),
                "L1": doc.get("VL1N"),
                "L2": doc.get("VL2N"),
                "L3": doc.get("VL3N"),
                "I1": doc.get("I1"),
                "I2": doc.get("I2"),
                "I3": doc.get("I3"),
                "W1": doc.get("PL1N"),
                "W2": doc.get("PL2N"),
                "W3": doc.get("PL3N"),
            }
            yield f"data: {json.dumps(payload)}\n\n"



def floor_bin(dt: datetime, step_sec: int) -> datetime:
    epoch_ms = int(dt.timestamp() * 1000)
    bin_ms = epoch_ms - (epoch_ms % (step_sec * 1000))
    return datetime.fromtimestamp(bin_ms / 1000, tz=timezone.utc)

# def to_json(doc):
#     doc = dict(doc)
#     doc["_id"] = str(doc["_id"])
#     return json.dumps(doc, default=str)

################ Users
# @app.get("/all-users/")
# def all_users():
#     # à¹€à¸­à¸²à¸—à¸¸à¸à¸Ÿà¸´à¸¥à¸”à¹Œ à¸¢à¸à¹€à¸§à¹‰à¸™ password à¹à¸¥à¸° refreshTokens
#     cursor = users_collection.find({}, {"password": 0, "refreshTokens": 0})
#     docs = list(cursor)

#     # à¸–à¹‰à¸²à¸ˆà¸°à¸ªà¹ˆà¸‡ _id à¹„à¸›à¸”à¹‰à¸§à¸¢ à¸•à¹‰à¸­à¸‡à¹à¸›à¸¥à¸‡ ObjectId -> str
#     for d in docs:
#         if "_id" in d:
#             d["_id"] = str(d["_id"])

#     return {"users": docs}

@app.get("/all-users/")
def all_users(current: UserClaims = Depends(get_current_user)):
    # à¸­à¸™à¸¸à¸à¸²à¸•à¹€à¸‰à¸žà¸²à¸° admin (à¸ˆà¸°à¹€à¸žà¸´à¹ˆà¸¡ owner à¸à¹‡à¹„à¸”à¹‰à¸•à¸²à¸¡à¸™à¹‚à¸¢à¸šà¸²à¸¢)
    if current.role != "admin":
        raise HTTPException(status_code=403, detail="forbidden")

    cursor = users_collection.find({}, {"password": 0, "refreshTokens": 0})
    docs = list(cursor)
    for d in docs:
        if "_id" in d:
            d["_id"] = str(d["_id"])
    return {"users": docs}

class addUsers(BaseModel):
    username:str
    email:str
    password:str
    tel:str
    company_name:str
    station_id:Optional[Union[str, int, List[Union[str, int]]]] = None
    role:str 

class UserOut(BaseModel):
    id: str
    username: str
    email: EmailStr
    role: str
    company: str
    station_id: List[str] = Field(default_factory=list)
    tel: str

@app.post("/add_users/", response_model=UserOut, status_code=201)
def insert_users(body: addUsers, current: UserClaims = Depends(get_current_user)):
    if current.role != "admin":
        raise HTTPException(status_code=403, detail="forbidden")

    email = body.email.lower()
    hashed = bcrypt.hashpw(body.password.encode("utf-8"), bcrypt.gensalt()).decode("utf-8")

    station_ids: List[str] = []
    if body.station_id is not None and body.station_id != "":
        if isinstance(body.station_id, list):
            station_ids = [str(x) for x in body.station_id if str(x).strip() != ""]
        else:
            station_ids = [str(body.station_id)]

    doc = {
        "username": body.username.strip(),
        "email": email,
        "password": hashed,
        "role": body.role,
        "company": (body.company_name or "").strip() or None,
        "tel": (body.tel or "").strip() or None,
        "station_id": station_ids,
        "refreshTokens": [],
        "createdAt": datetime.now(timezone.utc),
    }

    try:
        res = users_collection.insert_one(doc)
    except DuplicateKeyError:
        raise HTTPException(status_code=409, detail="Email already exists")

    return {
        "id": str(res.inserted_id),
        "username": doc["username"],
        "email": doc["email"],
        "role": doc["role"],
        "company": doc.get("company"),
        "station_id": doc["station_id"],
        "tel": doc.get("tel"),
        "createdAt": doc["createdAt"],
    }

@app.delete("/delete_users/{user_id}", status_code=204)
def delete_user(user_id: str, current: UserClaims = Depends(get_current_user)):
    # (à¸—à¸²à¸‡à¹€à¸¥à¸·à¸­à¸) à¸šà¸±à¸‡à¸„à¸±à¸šà¸ªà¸´à¸—à¸˜à¸´à¹Œà¹€à¸‰à¸žà¸²à¸° admin/owner
    if current.role not in ("admin", "owner"):
        raise HTTPException(status_code=403, detail="Forbidden")

    try:
        oid = ObjectId(user_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid user id")

    res = users_collection.delete_one({"_id": oid})
    if res.deleted_count == 0:
        raise HTTPException(status_code=404, detail="User not found")

    # 204 No Content
    return Response(status_code=204)

class UserUpdate(BaseModel):
    username: str | None = None
    email: EmailStr | None = None
    tel : str | None = None      # à¹ƒà¸Šà¹‰ "phone" à¹ƒà¸«à¹‰à¸ªà¸­à¸”à¸„à¸¥à¹‰à¸­à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸¡à¸µ
    company: str | None = None
    role: str | None = None       # admin à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™à¸—à¸µà¹ˆà¹à¸à¹‰à¹„à¸”à¹‰
    is_active: bool | None = None # admin à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™à¸—à¸µà¹ˆà¹à¸à¹‰à¹„à¸”à¹‰
    password: str | None = None   # à¸ˆà¸°à¸–à¸¹à¸à¹à¸®à¸Šà¹€à¸ªà¸¡à¸­à¸–à¹‰à¸²à¸¡à¸µà¸„à¹ˆà¸²

def hash_password(raw: str) -> str:
    return bcrypt.hashpw(raw.encode("utf-8"), bcrypt.gensalt()).decode("utf-8")

# à¸Ÿà¸´à¸¥à¸”à¹Œà¸—à¸µà¹ˆà¸­à¸™à¸¸à¸à¸²à¸•
ALLOW_FIELDS_ADMIN_USER = {"username", "email", "tel", "company", "role", "is_active", "password"}
ALLOW_FIELDS_SELF_USER  = {"username", "email", "tel", "company", "password"}


@app.patch("/user_update/{id}", response_model=UserOut)
def update_user(id: str, body: UserUpdate, current: UserClaims = Depends(get_current_user)):
    oid = to_object_id_or_400(id)

    doc = users_collection.find_one({"_id": oid})
    if not doc:
        raise HTTPException(status_code=404, detail="user not found")

    # â”€â”€ Permission: admin à¸—à¸³à¹„à¸”à¹‰à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”, owner à¹„à¸”à¹‰à¹€à¸‰à¸žà¸²à¸°à¸‚à¸­à¸‡à¸•à¸±à¸§à¹€à¸­à¸‡, à¸­à¸·à¹ˆà¸™ à¹† à¸«à¹‰à¸²à¸¡
    if current.role == "admin":
        pass  # à¸œà¹ˆà¸²à¸™
    elif current.role == "owner":
        if current.user_id != str(oid):
            raise HTTPException(status_code=403, detail="forbidden")
    else:
        # à¸à¸±à¸™à¸šà¸—à¸šà¸²à¸—à¸­à¸·à¹ˆà¸™ à¹† (à¹€à¸Šà¹ˆà¸™ user) à¹„à¸¡à¹ˆà¹ƒà¸«à¹‰à¹€à¸‚à¹‰à¸²à¸¡à¸²à¸­à¸±à¸›à¹€à¸”à¸•
        raise HTTPException(status_code=403, detail="forbidden")

    # â”€â”€ à¹€à¸•à¸£à¸µà¸¢à¸¡ incoming fields
    incoming = {
        k: (v.strip() if isinstance(v, str) else v)
        for k, v in body.model_dump(exclude_none=True).items()
    }
    if not incoming:
        raise HTTPException(status_code=400, detail="no fields to update")

    # â”€â”€ à¸ˆà¸³à¸à¸±à¸”à¸Ÿà¸´à¸¥à¸”à¹Œà¸•à¸²à¸¡à¸šà¸—à¸šà¸²à¸—
    # à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¸›à¸£à¸°à¸à¸²à¸¨à¸ªà¸­à¸‡à¸Šà¸¸à¸”à¸™à¸µà¹‰à¹„à¸§à¹‰à¸”à¹‰à¸²à¸™à¸šà¸™à¹„à¸Ÿà¸¥à¹Œà¸«à¸£à¸·à¸­à¹„à¸Ÿà¸¥à¹Œ settings:
    ALLOW_FIELDS_ADMIN_USER = {"username","email","password","role","company","tel","is_active"}
    ALLOW_FIELDS_SELF_OWNER = {"username","email","password","tel"}  # à¸›à¸£à¸±à¸šà¸•à¸²à¸¡à¸—à¸µà¹ˆà¸­à¸¢à¸²à¸à¹ƒà¸«à¹‰à¹à¸à¹‰à¹€à¸­à¸‡à¹„à¸”à¹‰
    if current.role == "admin":
        allowed = ALLOW_FIELDS_ADMIN_USER
    else:  # owner
        allowed = ALLOW_FIELDS_SELF_OWNER

    payload = {k: v for k, v in incoming.items() if k in allowed}
    if not payload:
        raise HTTPException(status_code=400, detail="no permitted fields to update")

    # â”€â”€ à¹à¸®à¸Šà¸£à¸«à¸±à¸ªà¸œà¹ˆà¸²à¸™à¸–à¹‰à¸²à¸¡à¸µ
    if "password" in payload:
        payload["password"] = hash_password(payload["password"])

    # â”€â”€ validate is_active (admin à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™à¸—à¸µà¹ˆà¹€à¸‚à¹‰à¸²à¸¡à¸²à¸–à¸¶à¸‡à¸šà¸£à¸£à¸—à¸±à¸”à¸™à¸µà¹‰à¹„à¸”à¹‰à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§)
    if "is_active" in payload and not isinstance(payload["is_active"], bool):
        raise HTTPException(status_code=400, detail="is_active must be boolean")

    now = datetime.now(timezone.utc)
    payload["updatedAt"] = now

    try:
        users_collection.update_one({"_id": oid}, {"$set": payload})
    except DuplicateKeyError:
        raise HTTPException(status_code=409, detail="duplicate email or username")

    newdoc = users_collection.find_one({"_id": oid}) or {}
    created_at = newdoc.get("createdAt") or now
    if "createdAt" not in newdoc:
        users_collection.update_one({"_id": oid}, {"$set": {"createdAt": created_at}})

    return {
        "id": str(newdoc["_id"]),
        "username": newdoc.get("username", ""),
        "email": newdoc.get("email", ""),
        "role": newdoc.get("role", ""),
        "company": (newdoc.get("company") or ""),
        "station_id": list(newdoc.get("station_id") or []),
        "tel": (newdoc.get("tel") or ""),
        "createdAt": created_at,
        "updatedAt": newdoc.get("updatedAt", now),
    }


def parse_iso_utc(s: str) -> Optional[datetime]:
    try:
        # "2025-09-29T16:19:54.659Z"
        return datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        return None

def latest_onoff(station_id: str) -> Dict[str, Any]:
    """
    à¸­à¹ˆà¸²à¸™à¹€à¸­à¸à¸ªà¸²à¸£à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸ˆà¸²à¸ stationsOnOff/<station_id>
    à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡ doc:
      { payload: { value: 0/1, timestamp: "ISO-UTC" }, ... }
    """
    coll = stationOnOff.get_collection(station_id)
    doc = coll.find_one(
        sort=[("payload.timestamp", -1), ("_id", -1)]
    )
    if not doc:
        return {"status": None, "statusAt": None}

    payload = doc.get("payload", {})
    val = payload.get("value", None)
    ts = payload.get("timestamp", None)

    # à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ bool à¸Šà¸±à¸”à¹€à¸ˆà¸™: 1/true => True, 0/false => False, à¸­à¸·à¹ˆà¸™à¹† -> None
    if isinstance(val, (int, bool)):
        status = bool(val)
    else:
        try:
            status = bool(int(val))
        except Exception:
            status = None

    status_at = parse_iso_utc(ts) if isinstance(ts, str) else None
    return {"status": status, "statusAt": status_at}

@app.get("/all-stations/")
def all_stations(current: UserClaims = Depends(get_current_user)):
    # 1) à¸ªà¸£à¹‰à¸²à¸‡à¹€à¸‡à¸·à¹ˆà¸­à¸™à¹„à¸‚ match à¸•à¸²à¸¡ role
    if current.role == "admin":
        match_query = {}
    else:
        if not current.user_id:
            raise HTTPException(status_code=401, detail="Missing uid in token")
        # à¸£à¸­à¸‡à¸£à¸±à¸šà¸—à¸±à¹‰à¸‡à¸à¸£à¸“à¸µà¹€à¸à¹‡à¸š user_id à¹€à¸›à¹‡à¸™ string à¸«à¸£à¸·à¸­ ObjectId
        conds = [{"user_id": current.user_id}]
        try:
            conds.append({"user_id": ObjectId(current.user_id)})
        except Exception:
            pass
        match_query = {"$or": conds}

    pipeline = [
        {"$match": match_query},

        # 2) à¹à¸›à¸¥à¸‡ user_id -> ObjectId à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™ string (à¹€à¸žà¸·à¹ˆà¸­ lookup)
        {"$addFields": {
            "user_obj_id": {
                "$cond": [
                    {"$eq": [{"$type": "$user_id"}, "string"]},
                    {"$toObjectId": "$user_id"},
                    "$user_id"  # à¸–à¹‰à¸²à¹€à¸›à¹‡à¸™ ObjectId à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰à¹€à¸”à¸´à¸¡
                ]
            }
        }},

        # 3) à¸”à¸¶à¸‡ username (à¹à¸¥à¸°à¸Ÿà¸´à¸¥à¸”à¹Œà¸­à¸·à¹ˆà¸™à¹†à¸ˆà¸²à¸ users) à¸”à¹‰à¸§à¸¢ $lookup
        {"$lookup": {
            "from": "users",              # à¸Šà¸·à¹ˆà¸­ collection à¸‚à¸­à¸‡ user
            "localField": "user_obj_id",  # _id à¹ƒà¸™ users à¹€à¸›à¹‡à¸™ ObjectId
            "foreignField": "_id",
            "as": "owner"
        }},
        {"$addFields": {
            "username": {"$arrayElemAt": ["$owner.username", 0]},
            # à¹€à¸žà¸´à¹ˆà¸¡à¹„à¸”à¹‰à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ à¹€à¸Šà¹ˆà¸™ email/phone/company
            # "owner_email": {"$arrayElemAt": ["$owner.email", 0]},
        }},

        # 4) à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸ªà¹ˆà¸‡ array owner à¸à¸±à¸šà¸Ÿà¸´à¸¥à¸”à¹Œà¸Šà¹ˆà¸§à¸¢à¹à¸›à¸¥à¸‡à¸­à¸­à¸à¹„à¸›
        {"$project": {"owner": 0, "user_obj_id": 0}},
    ]

    docs = list(station_collection.aggregate(pipeline))

    # â˜… à¹€à¸•à¸´à¸¡à¸ªà¸–à¸²à¸™à¸°à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¹à¸šà¸šà¹€à¸£à¸µà¸¢à¸¥à¹„à¸—à¸¡à¹Œà¸•à¹ˆà¸­à¸ªà¸–à¸²à¸™à¸µ
    for d in docs:
        sid = d.get("station_id")
        try:
            last = latest_onoff(str(sid))
        except Exception:
            last = {"status": None, "statusAt": None}
        d["status"] = last["status"]          # true/false/None
        d["statusAt"] = last["statusAt"]      # datetime | None

    docs = jsonable_encoder(docs, custom_encoder={ObjectId: str})
    for d in docs:
        if "_id" in d:
            d["_id"] = str(d["_id"])
    return {"stations": docs}

class addStations(BaseModel):
    station_id:str
    station_name:str
    brand:str
    model:str
    SN:str
    WO:str 
    PLCFirmware:str 
    PIFirmware:str 
    RTFirmware:str
    chargeBoxID: str 
    user_id: Optional[str] = None  
    owner: Optional[str] = None
    is_active:Optional[bool] = None


class StationOut(BaseModel):
    id: str
    station_id:str
    station_name:str
    brand:str
    model:str
    SN:str
    WO:str 
    PLCFirmware:str 
    PIFirmware:str 
    RTFirmware:str 
    chargeBoxID:str
    user_id: str 
    username: Optional[str] = None
    is_active:  Optional[bool] = None
    images: Optional[dict] = None   # â¬…ï¸ à¹€à¸žà¸´à¹ˆà¸¡à¸šà¸£à¸£à¸—à¸±à¸”à¸™à¸µà¹‰
    createdAt: Optional[datetime] = None
    class Config:
        json_encoders = {
            datetime: lambda v: v.astimezone(ZoneInfo("Asia/Bangkok")).isoformat()
        }


@app.post("/add_stations/", response_model=StationOut, status_code=201)
def insert_stations(
    body: addStations,
    current: UserClaims = Depends(get_current_user)
):
    # 1) à¸•à¸±à¸”/à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸” string fields
    station_id   = body.station_id.strip()
    station_name = body.station_name.strip()
    brand        = body.brand.strip()
    model        = body.model.strip()
    SN           = body.SN.strip()
    WO           = body.WO.strip()
    PLCFirmware           = body.PLCFirmware.strip()
    PIFirmware           = body.PIFirmware.strip()
    RTFirmware           = body.RTFirmware.strip()
    chargeBoxID           = body.chargeBoxID.strip()

    # (à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸šà¸±à¸‡à¸„à¸±à¸šà¸£à¸¹à¸›à¹à¸šà¸š station_id)
    # if not re.fullmatch(r"[A-Za-z0-9_]+", station_id):
    #     raise HTTPException(status_code=422, detail="station_id must be [A-Za-z0-9_]")

    # 2) à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆ owner à¹€à¸«à¸¡à¸·à¸­à¸™à¹à¸™à¸§à¸„à¸´à¸”à¸‚à¸­à¸‡ update:
    #    - admin: à¸­à¸™à¸¸à¸à¸²à¸•à¸ªà¹ˆà¸‡ user_id (24hex) à¸«à¸£à¸·à¸­ owner(username) à¸¡à¸²à¸à¸³à¸«à¸™à¸”à¹€à¸ˆà¹‰à¸²à¸‚à¸­à¸‡
    #             à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡à¹€à¸¥à¸¢ à¸ˆà¸° fallback à¹€à¸›à¹‡à¸™ current.user_id
    #    - non-admin: à¸šà¸±à¸‡à¸„à¸±à¸šà¹€à¸›à¹‡à¸™ current.user_id (à¸«à¹‰à¸²à¸¡à¸ªà¸§à¸¡à¸ªà¸´à¸—à¸˜à¸´à¹Œ)
    if current.role == "admin":
        owner_oid = None
        if body.user_id:
            owner_oid = to_object_id_or_400(body.user_id)
        elif body.owner:
            u = users_collection.find_one({"username": body.owner.strip()}, {"_id": 1})
            if not u:
                raise HTTPException(status_code=400, detail="invalid owner username")
            owner_oid = u["_id"]
        else:
            if not current.user_id:
                raise HTTPException(status_code=401, detail="Missing uid in token")
            owner_oid = to_object_id_or_400(current.user_id)
    else:
        if not current.user_id:
            raise HTTPException(status_code=401, detail="Missing uid in token")
        owner_oid = to_object_id_or_400(current.user_id)

    # 3) is_active à¹€à¸›à¹‡à¸™ boolean à¸Šà¸±à¸”à¹€à¸ˆà¸™
    is_active = True if body.is_active is None else bool(body.is_active)

    # 4) à¸ªà¸£à¹‰à¸²à¸‡à¹€à¸­à¸à¸ªà¸²à¸£ (à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ UTC à¹à¸¥à¸°à¹€à¸à¹‡à¸š user_id à¹€à¸›à¹‡à¸™ ObjectId à¹€à¸«à¸¡à¸·à¸­à¸™à¹ƒà¸™ PATCH)
    # doc: Dict[str, Any] = {
    #     "station_id": station_id,
    #     "station_name": station_name,
    #     "brand": brand,
    #     "model": model,
    #     "SN": SN,
    #     "WO": WO,
    #     "PLCFirmware": PLCFirmware,
    #     "PIFirmware": PIFirmware,
    #     "RTFirmware": RTFirmware,
    #     "chargeBoxID": chargeBoxID,
    #     "user_id": owner_oid,                 # ObjectId à¹ƒà¸™ DB
    #     "is_active": is_active,
    #     "createdAt": datetime.now(timezone.utc),
    # }
    doc: Dict[str, Any] = {
        "station_id": station_id,
        "station_name": station_name,
        "brand": brand,
        "model": model,
        "SN": SN,
        "WO": WO,
        "PLCFirmware": PLCFirmware,
        "PIFirmware": PIFirmware,
        "RTFirmware": RTFirmware,
        "chargeBoxID": chargeBoxID,
        "user_id": owner_oid,
        "is_active": is_active,
        "images": {},      
        "createdAt": datetime.now(timezone.utc),
    }

    # 5) insert + à¸ˆà¸±à¸”à¸à¸²à¸£ duplicate key à¸‚à¸­à¸‡ station_id
    try:
        res = station_collection.insert_one(doc)
    except DuplicateKeyError:
        raise HTTPException(status_code=409, detail="station_id already exists")

    # 6) à¸«à¸² username à¹€à¸žà¸·à¹ˆà¸­à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸š (à¹€à¸«à¸¡à¸·à¸­à¸™à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸­à¸¢à¸²à¸à¹„à¸”à¹‰à¹ƒà¸™ table)
    owner_doc = users_collection.find_one({"_id": owner_oid}, {"username": 1})
    owner_username = owner_doc.get("username") if owner_doc else None

    # 7) à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸šà¸£à¸¹à¸›à¹à¸šà¸šà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š PATCH: user_id à¹€à¸›à¹‡à¸™ string, à¹à¸–à¸¡ username
    # return {
    #     "id": str(res.inserted_id),
    #     "station_id": doc["station_id"],
    #     "station_name": doc["station_name"],
    #     "brand": doc["brand"],
    #     "model": doc["model"],
    #     "SN": doc["SN"],
    #     "WO": doc["WO"],
    #     "PLCFirmware": doc["PLCFirmware"],
    #     "PIFirmware": doc["PIFirmware"],
    #     "RTFirmware": doc["RTFirmware"],
    #     "chargeBoxID": doc["chargeBoxID"],
    #     "user_id": str(doc["user_id"]),       # string à¸ªà¸³à¸«à¸£à¸±à¸š client
    #     "username": owner_username,           # à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸šà¹ƒà¸«à¹‰ table à¹‚à¸Šà¸§à¹Œà¹„à¸”à¹‰à¹€à¸¥à¸¢
    #     "is_active": doc["is_active"],
    #     "createdAt": doc["createdAt"],
    #     # "updatedAt": None,  # à¸ˆà¸°à¹ƒà¸ªà¹ˆà¸à¹‡à¹„à¸”à¹‰à¸–à¹‰à¸²à¸­à¸¢à¸²à¸à¹ƒà¸«à¹‰ schemaà¹€à¸«à¸¡à¸·à¸­à¸™ PATCH à¹€à¸›à¹Šà¸°
    # }
    return {
        "id": str(res.inserted_id),
        "station_id": doc["station_id"],
        "station_name": doc["station_name"],
        "brand": doc["brand"],
        "model": doc["model"],
        "SN": doc["SN"],
        "WO": doc["WO"],
        "PLCFirmware": doc["PLCFirmware"],
        "PIFirmware": doc["PIFirmware"],
        "RTFirmware": doc["RTFirmware"],
        "chargeBoxID": doc["chargeBoxID"],
        "user_id": str(doc["user_id"]),
        "username": owner_username,
        "is_active": doc["is_active"],
        "images": doc.get("images", {}),        # â¬…ï¸ à¹€à¸žà¸´à¹ˆà¸¡
        "createdAt": doc["createdAt"],
        
    }



@app.delete("/delete_stations/{id}", status_code=204)
def delete_station(id: str, current: UserClaims = Depends(get_current_user)):
    if current.role not in ("admin", "owner"):
        raise HTTPException(status_code=403, detail="Forbidden")
    try:
        oid = ObjectId(id)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid id")
    res = station_collection.delete_one({"_id":  oid})
    if res.deleted_count == 0:
        raise HTTPException(status_code=404, detail="Station not found")
    return Response(status_code=204)

class StationUpdate(BaseModel):
    station_name: Optional[str] = None
    brand: Optional[str] = None
    model: Optional[str] = None
    SN: Optional[str] = None
    WO: Optional[str] = None
    PLCFirmware: Optional[str] = None
    PIFirmware: Optional[str] = None
    RTFirmware: Optional[str] = None
    chargeBoxID: Optional[str] = None
    # status: Optional[bool] = None
    images: Optional[dict] = None
    is_active: Optional[bool] = None
    user_id: str | None = None 


ALLOW_FIELDS_ADMIN = {"station_id", "station_name", "brand", "model", "SN", "WO", "PLCFirmware", "PIFirmware", "RTFirmware", "chargeBoxID", "status","is_active", "user_id","images"}
# ALLOW_FIELDS_NONADMIN = {"status"}

def to_object_id_or_400(s: str) -> ObjectId:
    try:
        return ObjectId(s)
    except Exception:
        raise HTTPException(status_code=400, detail="invalid user_id")

# ===== Helpers à¸ªà¸³à¸«à¸£à¸±à¸šà¸£à¸¹à¸›à¸ªà¸–à¸²à¸™à¸µ =====
STATION_IMG_ALLOWED = {"image/jpeg", "image/png", "image/webp"}
STATION_IMG_MAX_BYTES = 3 * 1024 * 1024  # 3 MB

def _ensure_dir(p: pathlib.Path):
    p.mkdir(parents=True, exist_ok=True)

async def save_station_image(station_id: str, kind: str, up: UploadFile) -> str:
    """
    à¹€à¸‹à¸Ÿà¹„à¸Ÿà¸¥à¹Œà¸¥à¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ /uploads/stations/<station_id>/
    à¸„à¸·à¸™à¸„à¹ˆà¸² URL à¸—à¸µà¹ˆà¸à¸±à¹ˆà¸‡ Frontend à¹ƒà¸Šà¹‰à¹à¸ªà¸”à¸‡à¹„à¸”à¹‰à¹€à¸¥à¸¢ (/uploads/...)
    """
    if up.content_type not in STATION_IMG_ALLOWED:
        raise HTTPException(status_code=415, detail=f"Unsupported file type: {up.content_type}")

    data = await up.read()
    if len(data) > STATION_IMG_MAX_BYTES:
        raise HTTPException(status_code=413, detail="File too large (> 3MB)")

    # à¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡
    subdir = pathlib.Path(UPLOADS_ROOT) / "stations" / station_id
    _ensure_dir(subdir)

    # à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œ: kind-uuid.ext
    ext = {
        "image/jpeg": ".jpg",
        "image/png": ".png",
        "image/webp": ".webp",
    }.get(up.content_type, "")
    fname = f"{kind}-{uuid.uuid4().hex}{ext}"
    dest  = subdir / fname

    with open(dest, "wb") as f:
        f.write(data)

    # URL à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸­à¸²à¹„à¸›à¹à¸ªà¸”à¸‡
    url = f"/uploads/stations/{station_id}/{fname}"
    return url

@app.patch("/update_stations/{id}", response_model=StationOut)
def update_station(
    id: str,
    body: StationUpdate,
    current: UserClaims = Depends(get_current_user)
):
    # à¸•à¸£à¸§à¸ˆ id à¸ªà¸–à¸²à¸™à¸µ
    try:
        oid = ObjectId(id)
    except Exception:
        raise HTTPException(status_code=400, detail="invalid id")

    # à¸«à¸² station
    st = station_collection.find_one({"_id": oid})
    if not st:
        raise HTTPException(status_code=404, detail="station not found")

    # à¸ªà¸´à¸—à¸˜à¸´à¹Œ: non-admin à¸•à¹‰à¸­à¸‡à¹€à¸›à¹‡à¸™ owner à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™
    if current.role != "admin":
        st_owner = st.get("user_id")  # à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™ ObjectId
        st_owner_str = str(st_owner) if st_owner is not None else None
        if not current.user_id or current.user_id != st_owner_str:
            raise HTTPException(status_code=403, detail="forbidden")

    # à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸‚à¹‰à¸²
    incoming: Dict[str, Any] = {
        k: (v.strip() if isinstance(v, str) else v)
        for k, v in body.model_dump(exclude_none=True).items()
    }
    if not incoming:
        raise HTTPException(status_code=400, detail="no fields to update")

    # à¸—à¸³ allowlist + map owner (à¹€à¸‰à¸žà¸²à¸° admin)
    if current.role == "admin":
        payload = {k: v for k, v in incoming.items() if k in ALLOW_FIELDS_ADMIN}

        # à¸–à¹‰à¸² admin à¸ªà¹ˆà¸‡ user_id à¸¡à¸² â†’ à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ ObjectId à¹à¸¥à¸° validate
        if "user_id" in payload:
            user_id_raw = payload["user_id"]

            # à¸£à¸­à¸‡à¸£à¸±à¸šà¸ªà¸­à¸‡à¹à¸šà¸š: à¸ªà¹ˆà¸‡à¸¡à¸²à¹€à¸›à¹‡à¸™ id (24hex) à¸«à¸£à¸·à¸­à¸ªà¹ˆà¸‡à¸¡à¸²à¹€à¸›à¹‡à¸™ username
            udoc = None
            if isinstance(user_id_raw, str) and len(user_id_raw) == 24:
                # à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™ ObjectId string
                udoc = users_collection.find_one({"_id": to_object_id_or_400(user_id_raw)})
            else:
                # à¹€à¸œà¸·à¹ˆà¸­à¸à¸£à¸“à¸µà¸«à¸™à¹‰à¸²à¸šà¹‰à¸²à¸™à¸ªà¹ˆà¸‡ username à¸¡à¸² (à¹„à¸¡à¹ˆà¹à¸™à¸°à¸™à¸³ à¹à¸•à¹ˆà¸à¸±à¸™à¹„à¸§à¹‰)
                udoc = users_collection.find_one({"username": user_id_raw})

            if not udoc:
                raise HTTPException(status_code=400, detail="invalid user_id")

            # âœ… à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ ObjectId à¹ƒà¸™ DB
            payload["user_id"] = udoc["_id"]
    

    if "is_active" in payload and not isinstance(payload["is_active"], bool):
        raise HTTPException(status_code=400, detail="is_active must be boolean")

    # à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸ªà¸±à¹ˆà¸‡ update
    update_doc: Dict[str, Any] = {"$set": payload}

    # à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ â€œà¸¥à¸šâ€ à¸Ÿà¸´à¸¥à¸”à¹Œ username à¹€à¸”à¸´à¸¡à¸­à¸­à¸à¸ˆà¸²à¸ stations (à¹ƒà¸«à¹‰à¹€à¸«à¸¥à¸·à¸­à¹€à¸‰à¸žà¸²à¸° user_id)
    # à¹ƒà¸«à¹‰à¹€à¸žà¸´à¹ˆà¸¡à¸šà¸£à¸£à¸—à¸±à¸”à¸™à¸µà¹‰ (à¸›à¸¥à¸­à¸”à¸ à¸±à¸¢ à¹ƒà¸ªà¹ˆà¹„à¸”à¹‰à¸•à¸¥à¸­à¸”):
    update_doc["$unset"] = {"username": ""}

    # à¸­à¸±à¸›à¹€à¸”à¸•
    res = station_collection.update_one({"_id": oid}, update_doc)
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="station not found")

    # à¸­à¹ˆà¸²à¸™à¸„à¸·à¸™
    doc = station_collection.find_one({"_id": oid})
    created_at = doc.get("createdAt")
    if created_at is None:
        created_at = datetime.now(timezone.utc)   # ðŸ‘ˆ à¸à¸±à¸™à¸„à¹ˆà¸² None
    return {
        "id": str(doc["_id"]),
        "station_id": doc.get("station_id", ""),
        "station_name": doc.get("station_name", ""),
        "brand": doc.get("brand", ""),
        "model": doc.get("model", ""),
        "SN": doc.get("SN", ""),
        "WO": doc.get("WO", ""),
        "PLCFirmware": doc.get("PLCFirmware", ""),
        "PIFirmware": doc.get("PIFirmware", ""),
        "RTFirmware": doc.get("RTFirmware", ""),
        "chargeBoxID": doc.get("chargeBoxID", ""),
        "createdAt": created_at,  
        # à¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸šà¹€à¸›à¹‡à¸™ string à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸à¸±à¹ˆà¸‡ client à¹ƒà¸Šà¹‰à¸‡à¹ˆà¸²à¸¢
        "user_id": str(doc["user_id"]) if doc.get("user_id") else "",
        "username": doc.get("username"),
        "is_active": bool(doc.get("is_active", False)),
        "images": doc.get("images", {}),       # âœ… à¹ƒà¸ªà¹ˆà¸ à¸²à¸žà¸à¸¥à¸±à¸šà¹„à¸›à¸”à¹‰à¸§à¸¢
        "updatedAt": datetime.now(timezone.utc)
    }

@app.post("/stations/{station_id}/upload-images")
async def upload_station_images(
    station_id: str,
    station: Optional[UploadFile] = File(None),
    mdb: Optional[UploadFile]     = File(None),
    charger: Optional[UploadFile] = File(None),
    device: Optional[UploadFile]  = File(None),
    current: UserClaims = Depends(get_current_user),
):
    # à¸«à¸²à¹€à¸­à¸à¸ªà¸²à¸£à¸ªà¸–à¸²à¸™à¸µ
    doc = station_collection.find_one({"station_id": station_id})
    if not doc:
        raise HTTPException(status_code=404, detail="station not found")

    # à¹€à¸Šà¹‡à¸„à¸ªà¸´à¸—à¸˜à¸´à¹Œ: admin à¸œà¹ˆà¸²à¸™ / owner à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™
    owner_str = str(doc.get("user_id")) if doc.get("user_id") else None
    if current.role != "admin" and current.user_id != owner_str:
        raise HTTPException(status_code=403, detail="forbidden")

    updated: dict[str, str] = {}
    for kind, up in {"station": station, "mdb": mdb, "charger": charger, "device": device}.items():
        if up is None:
            continue
        url = await save_station_image(station_id, kind, up)
        updated[kind] = url

    if not updated:
        return {"updated": False, "images": doc.get("images", {})}

    images = doc.get("images", {})
    images.update(updated)

    station_collection.update_one(
        {"_id": doc["_id"]},
        {"$set": {"images": images, "updatedAt": datetime.now(timezone.utc)}}
    )

    return {"updated": True, "images": images}

@app.get("/owners")
async def get_owners():
    cursor = users_collection.find({"role": "owner"}, {"_id": 1, "username": 1})
    owners = [{"user_id": str(u["_id"]), "username": u["username"]} for u in cursor]

    if not owners:
        raise HTTPException(status_code=404, detail="owners not found")

    return {"owners": owners}

stationOnOff = client1["stationsOnOff"]
class StationIdsIn(BaseModel):
    station_ids: List[str]

def _latest_onoff_bool(sid: str) -> bool:
    coll = stationOnOff.get_collection(str(sid))
    doc = coll.find_one(sort=[("payload.timestamp", -1), ("_id", -1)])  # â† à¹€à¸­à¸²à¹€à¸­à¸à¸ªà¸²à¸£à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸ˆà¸£à¸´à¸‡ à¹†
    if not doc:
        return False
    payload = doc.get("payload", {})
    val = payload.get("value", 0)
    # map à¹€à¸›à¹‡à¸™ bool à¹ƒà¸«à¹‰à¸Šà¸±à¸”
    if isinstance(val, bool):
        return val
    try:
        return bool(int(val))
    except Exception:
        return False

@app.post("/station-onoff/bulk")
def get_station_onoff_bulk(body: StationIdsIn):
    out: Dict[str, bool] = {}
    for sid in body.station_ids:
        try:
            out[sid] = _latest_onoff_bool(sid)
        except Exception:
            out[sid] = False
    return {"status": out}

@app.get("/station-onoff/{station_id}")
def station_onoff_latest(station_id: str, current: UserClaims = Depends(get_current_user)):
    # if current.role != "admin" and station_id not in set(current.station_ids):
        # raise HTTPException(status_code=403, detail="Forbidden station_id")

    data = latest_onoff(str(station_id))
    status_at_iso = (
        data["statusAt"].astimezone(ZoneInfo("Asia/Bangkok")).isoformat()
        if data["statusAt"] else None
    )
    return {"station_id": station_id, "status": data["status"], "statusAt": status_at_iso}

def parse_iso_any_tz(s: str) -> datetime | None:
    if not isinstance(s, str):
        return None
    try:
        return datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        try:
            return datetime.fromisoformat(s + "+00:00")
        except Exception:
            return None

# -------------------------------------------------- PMReport (charger)       

def get_pmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    coll = PMReportDB.get_collection(str(station_id))
    return coll

def _compute_next_pm_date_str(pm_date_str: str | None) -> str | None:
    if not pm_date_str:
        return None
    # pm_date à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ "YYYY-MM-DD"
    try:
        d = datetime.fromisoformat(pm_date_str).date()  # date object
    except ValueError:
        return None
    next_d = d + relativedelta(months=+6)              # â† à¸•à¸£à¸‡ 6 à¹€à¸”à¸·à¸­à¸™
    return next_d.isoformat()     

# --- helper: à¹€à¸­à¸² pm_date à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸ˆà¸²à¸ PMReportDB/<station_id> ---
async def _latest_pm_date_from_pmreport(station_id: str) -> dict | None:
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    coll = PMReportDB.get_collection(str(station_id))

    pipeline = [
        {"$addFields": {
            "_ts": {
                "$ifNull": [
                    {
                        "$cond": [
                            {"$eq": [{"$type": "$timestamp"}, "string"]},
                            {"$dateFromString": {
                                "dateString": "$timestamp",
                                "timezone": "UTC",
                                "onError": None,
                                "onNull": None
                            }},
                            "$timestamp"
                        ]
                    },
                    {"$toDate": "$_id"}
                ]
            }
        }},
        {"$sort": {"_ts": -1, "_id": -1}},
        {"$limit": 1},
        {"$project": {"_id": 1, "pm_date": 1, "timestamp": 1}}
    ]

    cursor = coll.aggregate(pipeline)
    docs = await cursor.to_list(length=1)
    return docs[0] if docs else None

async def _pmreport_latest_core(station_id: str, current: UserClaims):
    # --- auth & validate ---
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")

    # 1) à¸”à¸¶à¸‡à¸ˆà¸²à¸ stations
    st = station_collection.find_one(
        {"station_id": station_id},
        {"_id": 1, "PIFirmware": 1, "PLCFirmware": 1, "RTFirmware": 1, "timestamp": 1, "updatedAt": 1}
    )
    if not st:
        raise HTTPException(status_code=404, detail="Station not found")

    pi_fw  = st.get("PIFirmware")
    plc_fw = st.get("PLCFirmware")
    rt_fw  = st.get("RTFirmware")

    # 2) à¸”à¸¶à¸‡ pm_date à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸ˆà¸²à¸ PMReportDB
    pm_latest = await _latest_pm_date_from_pmreport(station_id)
    pm_date = pm_latest.get("pm_date") if pm_latest else None

    # à¹€à¸§à¸¥à¸²: à¹ƒà¸Šà¹‰ timestamp à¸ˆà¸²à¸ pm report à¸–à¹‰à¸²à¸¡à¸µ à¹„à¸¡à¹ˆà¸‡à¸±à¹‰à¸™ fallback à¹„à¸›à¸‚à¸­à¸‡à¸ªà¸–à¸²à¸™à¸µ
    ts_raw = (pm_latest.get("timestamp") if pm_latest else None) or st.get("timestamp") or st.get("updatedAt")

    ts_dt = (parse_iso_any_tz(ts_raw) if isinstance(ts_raw, str)
             else (ts_raw if isinstance(ts_raw, datetime) else None))
    ts_utc = ts_dt.astimezone(ZoneInfo("UTC")).isoformat() if ts_dt else None
    ts_th  = ts_dt.astimezone(ZoneInfo("Asia/Bangkok")).isoformat() if ts_dt else None

    pm_next_date = _compute_next_pm_date_str(pm_date)

    return {
        "_id": str(st["_id"]),
        "pi_firmware": pi_fw,
        "plc_firmware": plc_fw,
        "rt_firmware": rt_fw,
        "pm_date": pm_date,              # â† à¸¡à¸²à¸ˆà¸²à¸ PMReportDB
        "pm_next_date": pm_next_date, 
        "timestamp": ts_raw,             # pmreport.timestamp à¸–à¹‰à¸²à¸¡à¸µ
        "timestamp_utc": ts_utc,
        "timestamp_th": ts_th,
        "source": "stations + PMReportDB",  # à¹€à¸œà¸·à¹ˆà¸­ debug
    }

@app.get("/pmreport/get")
async def pmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    coll = get_pmreport_collection_for(station_id)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    return doc

@app.get("/pmreport/list")
async def pmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_pmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side" : 1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)
    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # --- à¸”à¸¶à¸‡à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸²à¸ PMReportURL à¹‚à¸”à¸¢ map à¸”à¹‰à¸§à¸¢ pm_date (string) ---
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    urls_coll = get_pmurl_coll_upload(station_id)
    url_by_day: dict[str, str] = {}

    if pm_dates:
        ucur = urls_coll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "pm_date": it.get("pm_date"),
        "inspector": it.get("inspector"),
        "side" : it.get("side"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    pm_date_arr = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    return {"items": items, "pm_date": pm_date_arr, "page": page, "pageSize": pageSize, "total": total}

# à¹€à¸”à¸´à¸¡ (path param) â†’ à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹ƒà¸«à¹‰à¹€à¸£à¸µà¸¢à¸ helper
@app.get("/pmreport/latest/{station_id}")
async def pmreport_latest(station_id: str, current: UserClaims = Depends(get_current_user)):
    return await _pmreport_latest_core(station_id, current)

# à¹ƒà¸«à¸¡à¹ˆ (query param) â†’ à¹ƒà¸«à¹‰à¸£à¸­à¸‡à¸£à¸±à¸šà¸£à¸¹à¸›à¹à¸šà¸š /pmreport/latest/?station_id=...
@app.get("/pmreport/latest/")
async def pmreport_latest_q(
    station_id: str = Query(..., description="à¹€à¸Šà¹ˆà¸™ Klongluang3"),
    current: UserClaims = Depends(get_current_user),
):
    return await _pmreport_latest_core(station_id, current)

class PMMeasureRow(BaseModel):
    value: str = ""
    unit: str = "V"

class PMMeasures(BaseModel):
    m16: Dict[str, PMMeasureRow] = Field(default_factory=dict)  # L1-L2, L2-L3, ...
    cp: PMMeasureRow = PMMeasureRow()

class PMRowPF(BaseModel):
    pf: Optional[Literal["PASS","FAIL","NA",""]] = ""
    remark: Optional[str] = ""

class PMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    station_id: str
    job: dict
    # rows: dict
    measures_pre: dict
    # summary: str
    pm_date:str
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None 
    # summaryCheck: Optional[Literal["PASS","FAIL","NA"]] = None
    inspector: Optional[str] = None 
    # dust_filter: Optional[str] = None

async def _latest_issue_id_anywhere(
    station_id: str,
    pm_type: str,
    d: date,
    source: Literal["charger", "mdb", "ccb", "cbbox", "station"] = "charger",
) -> str | None:
    """
    source = "pm"    -> à¹ƒà¸Šà¹‰ get_pmreport_collection_for + get_pmurl_coll_upload
    source = "mdbpm" -> à¹ƒà¸Šà¹‰ get_mdbpmreport_collection_for + get_mdbpmurl_coll_upload
    """
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if source == "charger":
        rep_coll = get_pmreport_collection_for(station_id)
        url_coll = get_pmurl_coll_upload(station_id)
    elif source == "mdb": 
        rep_coll = get_mdbpmreport_collection_for(station_id)
        url_coll = get_mdbpmurl_coll_upload(station_id)
    elif source == "ccb": 
        rep_coll = get_ccbpmreport_collection_for(station_id)
        url_coll = get_ccbpmurl_coll_upload(station_id)
    elif source == "cbbox": 
        rep_coll = get_cbboxpmreport_collection_for(station_id)
        url_coll = get_cbboxpmurl_coll_upload(station_id)
    elif source == "station": 
        rep_coll = get_stationpmreport_collection_for(station_id)
        url_coll = get_stationpmurl_coll_upload(station_id)

    pipeline = [
        {"$match": {"issue_id": {"$regex": f"^{prefix}\\d+$"}}},
        {"$project": {"issue_id": 1}},
    ]

    rep_docs = await rep_coll.aggregate(pipeline).to_list(length=1000)
    url_docs = await url_coll.aggregate(pipeline).to_list(length=1000)

    best = None
    best_n = 0

    for ddoc in rep_docs + url_docs:
        s = ddoc.get("issue_id") or ""
        m = re.search(r"(\d+)$", s)
        if not m:
            continue
        n = int(m.group(1))
        if n > best_n:
            best_n = n
            best = s

    return best

async def _next_issue_id(db, station_id: str, pm_type: str, d, pad: int = 2) -> str:
    yymm = f"{d.year % 100:02d}{d.month:02d}"
    seq = await db.pm_sequences.find_one_and_update(
        {"station_id": station_id, "pm_type": pm_type, "yymm": yymm},
        {"$inc": {"n": 1}, "$setOnInsert": {"createdAt": datetime.now(timezone.utc)}},
        upsert=True,
        return_document=ReturnDocument.AFTER,
    )
    return f"PM-{pm_type}-{yymm}-{int(seq['n']):0{pad}d}"

@app.get("/pmreport/preview-issueid")
async def pmreport_preview_issueid(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¹ issue_id à¸–à¸±à¸”à¹„à¸› (PM-CG-YYMM-XX) à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸­à¸­à¸à¹€à¸¥à¸‚à¸ˆà¸£à¸´à¸‡
    à¹ƒà¸Šà¹‰à¸«à¸²à¹€à¸¥à¸‚à¹„à¸›à¹‚à¸Šà¸§à¹Œà¸šà¸™à¸Ÿà¸­à¸£à¹Œà¸¡à¹€à¸‰à¸¢ à¹†
    """
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "CG"

    latest = await _latest_issue_id_anywhere(station_id, pm_type, d)

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

async def _latest_doc_name_from_pmreport(
    station_id: str,
    pm_date: str,
    source: Literal["charger", "mdb", "ccb", "cbbox", "station"] = "charger",
) -> dict | None:
    """
    à¸”à¸¶à¸‡ doc_name à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸ˆà¸²à¸ PMReportDB/<station_id> à¸«à¸£à¸·à¸­ MDBPMReportDB/<station_id>
    à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸›à¸µà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š pm_date

    source = "pm"    -> à¹ƒà¸Šà¹‰ PMReportDB
    source = "mdbpm" -> à¹ƒà¸Šà¹‰ MDBPMReportDB
    """
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
        year = d.year
    except ValueError:
        return None
    
    # à¹€à¸¥à¸·à¸­à¸ DB à¸•à¸²à¸¡ source
    if source == "charger":
        coll = PMReportDB.get_collection(str(station_id))
    elif source == "mdb": 
        coll = MDBPMReportDB.get_collection(str(station_id))
    elif source == "ccb": 
        coll = CCBPMReportDB.get_collection(str(station_id))
    elif source == "cbbox": 
        coll = CBBOXPMReportDB.get_collection(str(station_id))
    elif source == "station": 
        coll = stationPMReportDB.get_collection(str(station_id))
    
    pipeline = [
        {
            "$match": {
                "doc_name": {"$regex": f"^{station_id}_\\d+/{year}$"}
            }
        },
        {"$sort": {"_id": -1}},
        {"$limit": 1},
        {"$project": {"_id": 1, "doc_name": 1}}
    ]
    
    cursor = coll.aggregate(pipeline)
    docs = await cursor.to_list(length=1)
    return docs[0] if docs else None

async def _latest_doc_name_anywhere(
    station_id: str,
    year: int,
    source: Literal["charger", "mdb", "ccb", "cbbox", "station"] = "charger",
) -> str | None:
    pattern = f"^{station_id}_\\d+/{year}$"

    # à¹€à¸¥à¸·à¸­à¸ collection à¸•à¸²à¸¡ source
    if source == "charger":
        rep_coll = get_pmreport_collection_for(station_id)
        url_coll = get_pmurl_coll_upload(station_id)
    elif source == "mdb": 
        rep_coll = get_mdbpmreport_collection_for(station_id)
        url_coll = get_mdbpmurl_coll_upload(station_id)
    elif source == "ccb": 
        rep_coll = get_ccbpmreport_collection_for(station_id)
        url_coll = get_ccbpmurl_coll_upload(station_id)
    elif source == "cbbox": 
        rep_coll = get_cbboxpmreport_collection_for(station_id)
        url_coll = get_cbboxpmurl_coll_upload(station_id)
    elif source == "station": 
        rep_coll = get_stationpmreport_collection_for(station_id)
        url_coll = get_stationpmurl_coll_upload(station_id)

    pipeline = [
        {"$match": {"doc_name": {"$regex": pattern}}},
        {"$project": {"doc_name": 1}},
    ]

    rep_docs = await rep_coll.aggregate(pipeline).to_list(length=1000)
    url_docs = await url_coll.aggregate(pipeline).to_list(length=1000)

    best_seq = 0
    best_name = None

    for d in rep_docs + url_docs:
        name = d.get("doc_name") or ""
        m = re.search(r"_(\d+)/\d{4}$", name)
        if not m:
            continue
        seq = int(m.group(1))
        if seq > best_seq:
            best_seq = seq
            best_name = name

    return best_name

@app.get("/pmreport/latest-docname")
async def pmreport_latest_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¶à¸‡ doc_name à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸ªà¸–à¸²à¸™à¸µ (à¸›à¸µà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š pm_date)
    à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸„à¸³à¸™à¸§à¸“à¹€à¸¥à¸‚à¸–à¸±à¸”à¹„à¸›à¸—à¸µà¹ˆ frontend
    """
    # auth à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    
    latest = await _latest_doc_name_from_pmreport(station_id, pm_date)
    
    return {
        "doc_name": latest.get("doc_name") if latest else None,
        "station_id": station_id,
        "pm_date": pm_date
    }

async def _next_year_seq(
    db,
    station_id: str,
    d: date,
    kind: Literal["pm", "cm"] = "pm",
    pm_type: str | None = None,
) -> int:
    """
    à¸­à¸­à¸à¹€à¸¥à¸‚à¸¥à¸³à¸”à¸±à¸šà¸£à¸²à¸¢à¸›à¸µ à¸•à¹ˆà¸­ station_id + (pm_type) + year

    PM: à¹à¸¢à¸à¸•à¸²à¸¡ station_id + pm_type + year
        à¹€à¸Šà¹ˆà¸™ Klongluang3 + CG + 2025 â†’ 1, 2, 3, ...

    CM: à¹à¸¢à¸à¸•à¸²à¸¡ station_id + year à¸­à¸¢à¹ˆà¸²à¸‡à¹€à¸”à¸µà¸¢à¸§
        à¹€à¸Šà¹ˆà¸™ Klongluang3 + 2025 â†’ 1, 2, 3, ...
    """
    year = d.year
    seq = await db.pm_year_sequences.find_one_and_update(
        {"station_id": station_id, "pm_type": pm_type, "year": year},
        {"$inc": {"n": 1}, "$setOnInsert": {"createdAt": datetime.now(timezone.utc)}},
        upsert=True,
        return_document=ReturnDocument.AFTER,
    )
    return int(seq["n"])

@app.get("/pmreport/preview-docname")
async def preview_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(station_id, year)

    if not latest:
        next_doc = f"{station_id}_1/{year}"
    else:
        import re
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{station_id}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

@app.post("/pmreport/pre/submit")
async def pmreport_submit(body: PMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_pmreport_collection_for(station_id)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "CG").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_pmurl_coll_upload(station_id)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    client_issue = body.issue_id
    issue_id: str | None = None    

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)
        rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            issue_id = client_issue
    
    if not issue_id:
        while True:
            candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
            rep_exists = await coll.find_one({"issue_id": candidate})
            url_exists = await url_coll.find_one({"issue_id": candidate})
            if not rep_exists and not url_exists:
                issue_id = candidate
                break

    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{station_id}_"
        valid_fmt = client_docName.startswith(prefix)

        url_coll = get_pmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, station_id, pm_type, d)
        year = d.year
        doc_name = f"{station_id}_{year_seq}/{year}"

    inspector = body.inspector
    doc = {
        "station_id": station_id,
        "doc_name": doc_name,
        "issue_id": issue_id,          # à¹€à¸”à¸´à¸¡
        "job": body.job,
        # "rows": body.rows,
        "measures_pre": body.measures_pre,
        # "summary": body.summary,
        # "summaryCheck": body.summaryCheck,
        "pm_date": body.pm_date,
        "inspector": inspector,
        # "dust_filter": body.dust_filter,
        # "year": year,                  # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
        # "year_seq": year_seq,          # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
        "photos_pre": {},
        "status": "draft",
        "side": body.side,
        "timestamp": datetime.now(timezone.utc),
    }
    res = await coll.insert_one(doc)
    return {
        "ok": True,
        "report_id": str(res.inserted_id),
        "issue_id": issue_id,
        # "year_seq": year_seq,
        "doc_name": doc_name,
    }

class PMPostIn(BaseModel):
    report_id: str | None = None      # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
    station_id: str
    # issue_id: str | None = None
    # job: dict
    rows: dict
    measures: dict
    summary: str
    # pm_date: str
    # doc_name: str | None = None
    summaryCheck: str | None = None
    dust_filter: str | None = None
    side: Literal["post", "after"]

# @app.post("/pmreport/submit")
# async def pmreport_submit(body: PMPostIn, current: UserClaims = Depends(get_current_user)):
#     station_id = body.station_id.strip()
#     coll = get_pmreport_collection_for(station_id)
#     db = coll.database

#     pm_type = str(body.job.get("pm_type") or "CG").upper()
#     body.job["pm_type"] = pm_type

#     url_coll = get_pmurl_coll_upload(station_id)

#     try:
#         d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     client_issue = body.issue_id
#     issue_id: str | None = None    

#     if client_issue:
#         yymm = f"{d.year % 100:02d}{d.month:02d}"
#         prefix = f"PM-{pm_type}-{yymm}-"
#         valid_fmt = client_issue.startswith(prefix)

#         # â­ à¹€à¸Šà¹‡à¸„à¸—à¸±à¹‰à¸‡ PMReportDB + PMUrlDB
#         # url_coll = get_pmurl_coll_upload(station_id)
#         rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
#         url_exists = await url_coll.find_one({"issue_id": client_issue})
#         unique = not (rep_exists or url_exists)

#         if valid_fmt and unique:
#             issue_id = client_issue
    
#     # if not issue_id:
#     #     issue_id = await _next_issue_id(db, station_id, pm_type, d, pad=2)
#     if not issue_id:
#         while True:
#             candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
#             rep_exists = await coll.find_one({"issue_id": candidate})
#             url_exists = await url_coll.find_one({"issue_id": candidate})
#             if not rep_exists and not url_exists:
#                 issue_id = candidate
#                 break
#     # doc_name = await _next_year_seq(db, station_id, pm_type, d)

#     client_docName = body.doc_name
#     doc_name = None
#     if client_docName:
#         year = f"{d.year}"
#         prefix = f"{station_id}_"
#         valid_fmt = client_docName.startswith(prefix)

#         url_coll = get_pmurl_coll_upload(station_id)
#         rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
#         url_exists = await url_coll.find_one({"doc_name": client_docName})
#         unique = not (rep_exists or url_exists)

#         if valid_fmt and unique:
#             doc_name = client_docName
 
#     if not doc_name:
#         year_seq = await _next_year_seq(db, station_id,d,"pm", pm_type)
#         year = d.year
#         doc_name = f"{station_id}_{year_seq}/{year}"

#     inspector = body.inspector
#     doc = {
#         "station_id": station_id,
#         "issue_id": issue_id,          # à¹€à¸”à¸´à¸¡
#         "job": body.job,
#         "rows": body.rows,
#         "measures": body.measures,
#         "summary": body.summary,
#         "summaryCheck": body.summaryCheck,
#         "pm_date": body.pm_date,
#         "inspector": inspector,
#         "dust_filter": body.dust_filter,
#         # "year": year,                  # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
#         # "year_seq": year_seq,          # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
#         "doc_name": doc_name,              # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡ à¹„à¸§à¹‰à¹‚à¸Šà¸§à¹Œà¸šà¸™à¸Ÿà¸­à¸£à¹Œà¸¡/à¸›à¸à¹€à¸­à¸à¸ªà¸²à¸£
#         "photos": {},
#         "status": "draft",
#         "timestamp": datetime.now(timezone.utc),
#     }
#     res = await coll.insert_one(doc)
#     return {
#         "ok": True,
#         "report_id": str(res.inserted_id),
#         "issue_id": issue_id,
#         # "year_seq": year_seq,
#         "doc_name": doc_name,
#     }

@app.post("/pmreport/submit")
async def pmreport_submit(
    body: PMPostIn,
    current: UserClaims = Depends(get_current_user)
):
    station_id = body.station_id.strip()
    coll = get_pmreport_collection_for(station_id)
    db = coll.database
    url_coll = get_pmurl_coll_upload(station_id)

    # pm_type = str(body.job.get("pm_type") or "CG").upper()
    # body.job["pm_type"] = pm_type

    # try:
    #     d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    # except ValueError:
    #     raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    # ---------- à¸à¸£à¸“à¸µ 1: à¸¡à¸µ report_id â†’ UPDATE doc à¹€à¸”à¸´à¸¡ (pre+post à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸•à¸±à¸§à¹€à¸”à¸µà¸¢à¸§) ----------
    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "station_id": station_id})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        # reuse à¸„à¹ˆà¸²à¹€à¸”à¸´à¸¡ à¹„à¸¡à¹ˆ gen à¹ƒà¸«à¸¡à¹ˆ
        # issue_id = existing.get("issue_id")
        # doc_name = existing.get("doc_name")
        # inspector = body.inspector or existing.get("inspector") or current.username

        update_fields = {
            # "job": body.job,
            "rows": body.rows,
            "measures": body.measures,          # à¹ƒà¸Šà¹‰à¹€à¸›à¹‡à¸™à¸„à¹ˆà¸²à¸«à¸¥à¸±à¸‡ PM
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            # "pm_date": body.pm_date,
            # "inspector": inspector,
            "dust_filter": body.dust_filter,
            # "doc_name": doc_name,
            "side": "post",                     # à¸•à¸­à¸™à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¸à¸±à¹ˆà¸‡ post à¹à¸¥à¹‰à¸§
            "timestamp_post": datetime.now(timezone.utc),
        }

        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
            # "issue_id": issue_id,
            # "doc_name": doc_name,
        }

    # ---------- à¸à¸£à¸“à¸µ 2: à¹„à¸¡à¹ˆà¸¡à¸µ report_id â†’ à¸ªà¸£à¹‰à¸²à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¹ƒà¸«à¸¡à¹ˆ (à¹€à¸«à¸¡à¸·à¸­à¸™à¹‚à¸„à¹‰à¸”à¹€à¸”à¸´à¸¡) ----------
    # client_issue = body.issue_id
    # issue_id: str | None = None

    # if client_issue:
    #     yymm = f"{d.year % 100:02d}{d.month:02d}"
    #     prefix = f"PM-{pm_type}-{yymm}-"
    #     valid_fmt = client_issue.startswith(prefix)

    #     # â­ à¹€à¸Šà¹‡à¸„à¸—à¸±à¹‰à¸‡ PMReportDB + PMUrlDB
    #     rep_exists = await coll.find_one(
    #         {"station_id": station_id, "issue_id": client_issue}
    #     )
    #     url_exists = await url_coll.find_one({"issue_id": client_issue})
    #     unique = not (rep_exists or url_exists)

    #     if valid_fmt and unique:
    #         issue_id = client_issue

    # if not issue_id:
    #     while True:
    #         candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
    #         rep_exists = await coll.find_one({"issue_id": candidate})
    #         url_exists = await url_coll.find_one({"issue_id": candidate})
    #         if not rep_exists and not url_exists:
    #             issue_id = candidate
    #             break

    # client_docName = body.doc_name
    # doc_name: str | None = None

    # if client_docName:
    #     prefix = f"{station_id}_"
    #     valid_fmt = client_docName.startswith(prefix)

    #     rep_exists = await coll.find_one(
    #         {"station_id": station_id, "doc_name": client_docName}
    #     )
    #     url_exists = await url_coll.find_one({"doc_name": client_docName})
    #     unique = not (rep_exists or url_exists)

    #     if valid_fmt and unique:
    #         doc_name = client_docName

    # if not doc_name:
    #     year_seq = await _next_year_seq(db, station_id, d, "pm", pm_type)
    #     year = d.year
    #     doc_name = f"{station_id}_{year_seq}/{year}"

    # inspector = body.inspector or current.username

    doc = {
        "station_id": station_id,
        # "issue_id": issue_id,
        # "job": body.job,
        "rows": body.rows,
        "measures": body.measures,
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        # "pm_date": body.pm_date,
        # "inspector": inspector,
        "dust_filter": body.dust_filter,
        # "doc_name": doc_name,
        "photos": {},
        "status": "draft",
        "side": "post",
        "timestamp": datetime.now(timezone.utc),
    }
    res = await coll.insert_one(doc)
    return {
        "ok": True,
        "report_id": str(res.inserted_id),
        # "issue_id": issue_id,
        # "doc_name": doc_name,
    }

# @app.post("/pmreport/submit")
# async def pmreport_submit(body: PMSubmitIn, current: UserClaims = Depends(get_current_user)):
#     station_id = body.station_id.strip()
#     coll = get_pmreport_collection_for(station_id)
#     db = coll.database

#     pm_type = str(body.job.get("pm_type") or "CG").upper()
#     body.job["pm_type"] = pm_type

#     url_coll = get_pmurl_coll_upload(station_id)

#     try:
#         d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     # ðŸ†• 1) à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸²à¸¡à¸µà¸£à¸²à¸¢à¸‡à¸²à¸™à¸‚à¸­à¸‡ station_id + pm_date à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§à¸«à¸£à¸·à¸­à¸¢à¸±à¸‡
#     existing = await coll.find_one({
#         "station_id": station_id,
#         "pm_date": body.pm_date,
#     })

#     # ---- 2) à¸«à¸² issue_id / doc_name ----
#     if existing:
#         # à¸–à¹‰à¸²à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ â†’ à¹ƒà¸Šà¹‰à¸‚à¸­à¸‡à¹€à¸”à¸´à¸¡ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ gen à¹ƒà¸«à¸¡à¹ˆ
#         issue_id: str = existing["issue_id"]
#         doc_name: str = existing.get("doc_name")
#     else:
#         # à¹€à¸”à¸´à¸¡à¸‚à¸­à¸‡à¸„à¸¸à¸“ (à¸”à¸±à¸”à¹à¸›à¸¥à¸‡à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢)
#         client_issue = body.issue_id
#         issue_id: str | None = None    

#         if client_issue:
#             yymm = f"{d.year % 100:02d}{d.month:02d}"
#             prefix = f"PM-{pm_type}-{yymm}-"
#             valid_fmt = client_issue.startswith(prefix)

#             rep_exists = await coll.find_one({
#                 "station_id": station_id,
#                 "issue_id": client_issue
#             })
#             url_exists = await url_coll.find_one({"issue_id": client_issue})
#             unique = not (rep_exists or url_exists)

#             if valid_fmt and unique:
#                 issue_id = client_issue
        
#         if not issue_id:
#             while True:
#                 candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
#                 rep_exists = await coll.find_one({"issue_id": candidate})
#                 url_exists = await url_coll.find_one({"issue_id": candidate})
#                 if not rep_exists and not url_exists:
#                     issue_id = candidate
#                     break

#         # ---------- doc_name ----------
#         client_docName = body.doc_name
#         doc_name: str | None = None

#         if client_docName:
#             year = f"{d.year}"
#             prefix = f"{station_id}_"
#             valid_fmt = client_docName.startswith(prefix)

#             rep_exists = await coll.find_one({
#                 "station_id": station_id,
#                 "doc_name": client_docName
#             })
#             url_exists = await url_coll.find_one({"doc_name": client_docName})
#             unique = not (rep_exists or url_exists)

#             if valid_fmt and unique:
#                 doc_name = client_docName
    
#         if not doc_name:
#             year_seq = await _next_year_seq(db, station_id, d, "pm", pm_type)
#             year = d.year
#             doc_name = f"{station_id}_{year_seq}/{year}"

#     # ---------- 3) à¹€à¸•à¸£à¸µà¸¢à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸±à¹ˆà¸‡ before/after ----------
#     inspector = body.inspector

#     side_data = {
#         "job": body.job,
#         "rows": body.rows,
#         "measures": body.measures,
#         "summary": body.summary,
#         "summaryCheck": body.summaryCheck,
#         "pm_date": body.pm_date,
#         "inspector": inspector,
#         "dust_filter": body.dust_filter,
#         "timestamp": datetime.now(timezone.utc),
#     }

#     # à¹€à¸­à¸à¸ªà¸²à¸£à¸«à¸¥à¸±à¸à¸•à¸­à¸™ insert à¸„à¸£à¸±à¹‰à¸‡à¹à¸£à¸à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™
#     base_doc = {
#         "station_id": station_id,
#         "issue_id": issue_id,
#         "pm_date": body.pm_date,
#         "doc_name": doc_name,
#         "photos": {},
#         "status": "draft",
#     }

#     # ---------- 4) upsert + set field à¸•à¸²à¸¡ side ----------
#     res = await coll.find_one_and_update(
#         {"station_id": station_id, "pm_date": body.pm_date},
#         {
#             # à¸–à¹‰à¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ doc â†’ à¹ƒà¸Šà¹‰ base_doc à¹€à¸›à¹‡à¸™à¸„à¹ˆà¸²à¸•à¸±à¹‰à¸‡à¸•à¹‰à¸™
#             "$setOnInsert": base_doc,
#             # à¹„à¸¡à¹ˆà¸§à¹ˆà¸²à¸ˆà¸° insert à¸«à¸£à¸·à¸­ update â†’ set à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸±à¹ˆà¸‡ before/after
#             "$set": {
#                 body.side: side_data,   # ðŸ‘ˆ à¹€à¸Šà¹ˆà¸™ "before": {...} à¸«à¸£à¸·à¸­ "after": {...}
#             },
#         },
#         upsert=True,
#         return_document=ReturnDocument.AFTER,
#     )

#     return {
#         "ok": True,
#         "report_id": str(res["_id"]),
#         "issue_id": res["issue_id"],
#         "doc_name": res.get("doc_name"),
#     }

# à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸šà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¹€à¸‹à¸´à¸£à¹Œà¸Ÿà¹€à¸§à¸­à¸£à¹Œ
UPLOADS_ROOT = os.getenv("UPLOADS_ROOT", "./uploads")
os.makedirs(UPLOADS_ROOT, exist_ok=True)

# à¹€à¸ªà¸´à¸£à¹Œà¸Ÿà¹„à¸Ÿà¸¥à¹Œà¸„à¸·à¸™à¹ƒà¸«à¹‰ Frontend à¸œà¹ˆà¸²à¸™ /uploads/...
app.mount("/uploads", StaticFiles(directory=UPLOADS_ROOT, html=False), name="uploads")

ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif"}
MAX_FILE_MB = 10

def _safe_name(name: str) -> str:
    # à¸à¸±à¸™ path traversal à¹à¸¥à¸°à¸­à¸±à¸à¸‚à¸£à¸°à¹à¸›à¸¥à¸ à¹†
    base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
    return base[:120] or secrets.token_hex(4)

def _ext(fname: str) -> str:
    return (fname.rsplit(".",1)[-1].lower() if "." in fname else "")

@app.post("/pmreport/{report_id}/pre/photos")
async def pmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # à¹€à¸Šà¹ˆà¸™ "g1" .. "g10"
    files: list[UploadFile] = File(...),
    # remark: str | None = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"g\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_pmreport_collection_for(station_id)
    from bson import ObjectId
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # à¸¢à¸·à¸™à¸¢à¸±à¸™à¸§à¹ˆà¸²à¸£à¸²à¸¢à¸‡à¸²à¸™à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ station à¸™à¸µà¹‰
    doc = await coll.find_one({"_id": oid}, {"_id":1, "station_id":1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "pm" / station_id / report_id / "pre" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    total = 0
    for f in files:
        ext = _ext(f.filename or "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        total += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        # URL à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸ªà¸”à¸‡à¸šà¸™ Frontend
        url_path = f"/uploads/pm/{station_id}/{report_id}/pre/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            # "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    # à¸­à¸±à¸›à¹€à¸”à¸•à¹€à¸­à¸à¸ªà¸²à¸£ PMReport: push à¸¥à¸‡ photos.<group>
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos_pre.{group}": {"$each": saved}}}
    )

    return {"ok": True, "count": len(saved), "group": group, "files": saved}


@app.post("/pmreport/{report_id}/post/photos")
async def pmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # à¹€à¸Šà¹ˆà¸™ "g1" .. "g10"
    files: list[UploadFile] = File(...),
    remark: str | None = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"g\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_pmreport_collection_for(station_id)
    from bson import ObjectId
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # à¸¢à¸·à¸™à¸¢à¸±à¸™à¸§à¹ˆà¸²à¸£à¸²à¸¢à¸‡à¸²à¸™à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ station à¸™à¸µà¹‰
    doc = await coll.find_one({"_id": oid}, {"_id":1, "station_id":1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "pm"  / station_id / report_id / "post" /  group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    total = 0
    for f in files:
        ext = _ext(f.filename or "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        total += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        # URL à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸ªà¸”à¸‡à¸šà¸™ Frontend
        url_path = f"/uploads/pm/{station_id}/{report_id}/post/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    # à¸­à¸±à¸›à¹€à¸”à¸•à¹€à¸­à¸à¸ªà¸²à¸£ PMReport: push à¸¥à¸‡ photos.<group>
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos.{group}": {"$each": saved}}}
    )

    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/pmreport/{report_id}/finalize")
async def pmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_pmreport_collection_for(station_id)
    
    from bson import ObjectId
    oid = ObjectId(report_id)
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

def parse_report_date_to_utc(s: str) -> datetime:
    # 'YYYY-MM-DD' => à¸•à¸µà¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¸•à¹‰à¸™à¸§à¸±à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢ à¹à¸¥à¹‰à¸§à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ UTC
    if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
        tz_th = ZoneInfo("Asia/Bangkok")
        dt_th = datetime.fromisoformat(s + "T00:00:00").replace(tzinfo=tz_th)
        return dt_th.astimezone(timezone.utc)
    # ISO à¸—à¸µà¹ˆà¸¥à¸‡à¸—à¹‰à¸²à¸¢ Z à¸«à¸£à¸·à¸­à¸¡à¸µà¸­à¸­à¸Ÿà¹€à¸‹à¹‡à¸•
    if s.endswith("Z"):
        return datetime.fromisoformat(s.replace("Z", "+00:00")).astimezone(timezone.utc)
    if re.search(r"[+\-]\d{2}:\d{2}$", s):
        return datetime.fromisoformat(s).astimezone(timezone.utc)
    # à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™ â†’ à¸–à¸·à¸­à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢
    return datetime.fromisoformat(s + "+07:00").astimezone(timezone.utc)

def get_pmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    coll = PMUrlDB.get_collection(str(station_id))
    # # à¹€à¸à¹‡à¸šà¸§à¸±à¸™à¸—à¸µà¹ˆà¹à¸šà¸š Date à¸ˆà¸£à¸´à¸‡à¹„à¸§à¹‰ query à¸Šà¹ˆà¸§à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆ
    # try:
    #     coll.create_index([("reportDate", 1)])
    #     coll.create_index([("createdAt", -1), ("_id", -1)])
    # except Exception:
    #     pass
    return coll

# --- à¹€à¸žà¸´à¹ˆà¸¡à¹ƒà¸«à¹‰à¸£à¸­à¸‡à¸£à¸±à¸š PDF ---
ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif","pdf"}  # <<-- à¹€à¸žà¸´à¹ˆà¸¡ pdf
MAX_FILE_MB = 20  # à¹€à¸œà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸à¹ˆà¸‚à¸¶à¹‰à¸™

def _safe_name(name: str) -> str:
    base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
    return base[:120] or secrets.token_hex(4)

def normalize_pm_date(s: str) -> str:
    """
    à¸£à¸±à¸šà¹„à¸”à¹‰à¸—à¸±à¹‰à¸‡:
      - 'YYYY-MM-DD'           -> à¸„à¸·à¸™à¹€à¸”à¸´à¸¡
      - ISO (à¸¡à¸µ Z/offset à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸¡à¸µ) -> à¸•à¸µà¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢ à¹à¸¥à¹‰à¸§à¸„à¸·à¸™ date().isoformat()
    à¸„à¸·à¸™à¸„à¹ˆà¸²à¹€à¸›à¹‡à¸™ 'YYYY-MM-DD' (à¹„à¸¡à¹ˆà¹€à¸à¹‡à¸šà¹€à¸§à¸¥à¸²)
    """
    if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
        return s
    # à¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸²
    if s.endswith("Z") or re.search(r"[+\-]\d{2}:\d{2}$", s):
        dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
    else:
        # à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸² -> à¸–à¸·à¸­à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢
        dt = datetime.fromisoformat(s).replace(tzinfo=th_tz)
    return dt.astimezone(th_tz).date().isoformat()

@app.post("/pmurl/upload-files", status_code=201)
async def pmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),                 # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO
    files: list[UploadFile] = File(...),
    issue_id: Optional[str] = Form(None),
    doc_name: Optional[str] = Form(None),
    inspector: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # auth (à¸–à¹‰à¸²à¸ˆà¸°à¹€à¸›à¸´à¸”à¹ƒà¸Šà¹‰à¸à¹‡à¹€à¸­à¸² comment à¸­à¸­à¸)
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    # à¸•à¸£à¸§à¸ˆ/à¹€à¸•à¸£à¸µà¸¢à¸¡à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™
    coll = get_pmurl_coll_upload(station_id)

    # à¹à¸›à¸¥à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆà¸ˆà¸²à¸ form à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ 'YYYY-MM-DD' (à¹„à¸—à¸¢) à¸”à¹‰à¸§à¸¢ helper à¹€à¸”à¸´à¸¡
    pm_date = normalize_pm_date(reportDate)

    # parse à¹€à¸›à¹‡à¸™ date object à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸«à¸² seq
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

    # à¸Šà¸™à¸´à¸” PM (à¸•à¸­à¸™à¸™à¸µà¹‰ fix à¹€à¸›à¹‡à¸™ CG)
    pm_type = "CG"

    # -------------------------
    # 1) à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆà¹€à¸¥à¸·à¸­à¸ issue_id
    # -------------------------
    rep_coll = get_pmreport_collection_for(station_id)
    final_issue_id: str | None = None
    client_issue = (issue_id or "").strip()

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        url_exists = await coll.find_one({"issue_id": client_issue})
        rep_exists = await rep_coll.find_one({"issue_id": client_issue})
        unique = not (url_exists or rep_exists)

        if valid_fmt and unique:
            final_issue_id = client_issue

    if not final_issue_id:
        while True:
            candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
            url_exists = await coll.find_one({"issue_id": candidate})
            rep_exists = await rep_coll.find_one({"issue_id": candidate})
            if not url_exists and not rep_exists:
                final_issue_id = candidate
                break

    year_seq: int | None = None

    # à¸žà¸¢à¸²à¸¢à¸²à¸¡ reuse year_seq à¸ˆà¸²à¸ PMReportDB à¸–à¹‰à¸²à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ (issue_id à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™)
    rep = await get_pmreport_collection_for(station_id).find_one(
        {"issue_id": final_issue_id},
        {"year_seq": 1, "pm_date": 1},
    )
    if rep and rep.get("year_seq") is not None:
        year_seq = int(rep["year_seq"])

    # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¹ˆà¸² year_seq à¸ˆà¸²à¸ PMReport â†’ à¸­à¸­à¸à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸ pm_year_sequences
    if year_seq is None:
        year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

    year = d.year
    final_doc_name: str | None = None

    if doc_name:
        candidate = doc_name.strip()

        ok_format = candidate.startswith(f"{station_id}_")

        rep_coll = get_pmreport_collection_for(station_id)
        rep_exists = await rep_coll.find_one({"doc_name": candidate})
        url_exists = await coll.find_one({"doc_name": candidate})
        unique = not (rep_exists or url_exists)

        if ok_format and unique:
            final_doc_name = candidate

    if not final_doc_name:
        final_doc_name = f"{station_id}_{year_seq}/{year}"

    doc_name = final_doc_name
    # -------------------------
    # 3) à¹€à¸‹à¸Ÿà¹„à¸Ÿà¸¥à¹Œ PDF à¸¥à¸‡à¸”à¸´à¸ªà¸à¹Œ
    # -------------------------
    subdir = pm_date  # à¹ƒà¸Šà¹‰ YYYY-MM-DD à¹€à¸›à¹‡à¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸¢à¹ˆà¸­à¸¢
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "pmurl" / station_id / subdir
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls: list[str] = []
    metas: list[dict] = []
    total_size = 0

    for f in files:
        # à¸•à¸£à¸§à¸ˆà¸§à¹ˆà¸²à¹€à¸›à¹‡à¸™ PDF à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™
        ext = (f.filename.rsplit(".", 1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS or ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        total_size += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(
                status_code=413,
                detail=f"File too large (> {MAX_FILE_MB} MB)"
            )

        safe = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        dest = dest_dir / safe
        with open(dest, "wb") as out:
            out.write(data)

        url = f"/uploads/pmurl/{station_id}/{subdir}/{safe}"
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    inspector_clean = (inspector or "").strip() or None
    # -------------------------
    # 4) à¸šà¸±à¸™à¸—à¸¶à¸à¸¥à¸‡ Mongo
    # -------------------------
    now = datetime.now(timezone.utc)
    doc = {
        "station": station_id,
        "pm_date": pm_date,          # 'YYYY-MM-DD'
        "issue_id": final_issue_id,  # PM-CG-YYMM-XX
        "inspector": inspector_clean,
        "year": year,                # 2025
        "year_seq": year_seq,        # 1, 2, 3, ...
        "doc_name": doc_name,            # Klongluang3_CG_1/2025
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    }
    res = await coll.insert_one(doc)

    return {
        "ok": True,
        "inserted_id": str(res.inserted_id),
        "count": len(urls),
        "urls": urls,
        "issue_id": final_issue_id,
        "year_seq": year_seq,
        "doc_name": doc_name,
        "inspector": inspector_clean,
    }

@app.get("/pmurl/list")
async def pmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    """
    à¸”à¸¶à¸‡à¸£à¸²à¸¢à¸à¸²à¸£à¹„à¸Ÿà¸¥à¹Œ PM (PDF) à¸—à¸µà¹ˆà¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸§à¹‰à¸•à¹ˆà¸­à¸ªà¸–à¸²à¸™à¸µ à¸ˆà¸²à¸ PMUrlDB/<station_id>
    - à¸£à¸­à¸‡à¸£à¸±à¸šà¸—à¸±à¹‰à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¹€à¸à¹‡à¸š pm_date (string 'YYYY-MM-DD') à¹à¸¥à¸° reportDate (Date/ISO)
    - à¹€à¸£à¸µà¸¢à¸‡à¸ˆà¸²à¸à¹ƒà¸«à¸¡à¹ˆà¹„à¸›à¹€à¸à¹ˆà¸² (createdAt desc, _id desc)
    - à¸£à¸¹à¸›à¹à¸šà¸šà¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸·à¸­à¸™ /pmreport/list (à¸¡à¸µ file_url à¸ªà¸³à¸«à¸£à¸±à¸šà¸¥à¸´à¸‡à¸à¹Œà¸•à¸±à¸§à¹à¸£à¸)
    """
    coll = get_pmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    # à¸”à¸¶à¸‡à¹€à¸‰à¸žà¸²à¸°à¸Ÿà¸´à¸¥à¸”à¹Œà¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™
    cursor = coll.find(
        {},
        {"_id": 1, "issue_id": 1, "doc_name": 1,"inspector":1, "pm_date": 1, "reportDate": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    def _pm_date_from(doc: dict) -> str | None:
        """
        à¹à¸›à¸¥à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆà¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£à¹ƒà¸«à¹‰à¹„à¸”à¹‰ string 'YYYY-MM-DD'
        - à¸–à¹‰à¸²à¸¡à¸µ pm_date (string) â†’ à¸„à¸·à¸™à¸„à¹ˆà¸²à¸™à¸±à¹‰à¸™
        - à¸–à¹‰à¸²à¸¡à¸µ reportDate (datetime/string) â†’ à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™à¸§à¸±à¸™à¹„à¸—à¸¢ à¹à¸¥à¹‰à¸§ .date().isoformat()
        """
        # à¸£à¸¸à¹ˆà¸™à¹ƒà¸«à¸¡à¹ˆ: à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ pm_date (string)
        s = doc.get("pm_date")
        if isinstance(s, str) and re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
            return s

        # à¸£à¸¸à¹ˆà¸™à¹€à¸à¹ˆà¸²: à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ reportDate (Date/ISO)
        rd = doc.get("reportDate")
        if isinstance(rd, datetime):
            return rd.astimezone(th_tz).date().isoformat()
        if isinstance(rd, str):
            try:
                dt = datetime.fromisoformat(rd.replace("Z", "+00:00"))
            except Exception:
                # à¹€à¸œà¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸² â†’ à¸–à¸·à¸­à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢
                try:
                    dt = datetime.fromisoformat(rd).replace(tzinfo=th_tz)
                except Exception:
                    return None
            return dt.astimezone(th_tz).date().isoformat()

        return None

    items = []
    pm_date_arr = []

    for it in items_raw:
        pm_date_str = _pm_date_from(it)
        if pm_date_str:
            pm_date_arr.append(pm_date_str)

        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""

        items.append({
            "id": str(it["_id"]),
            "pm_date": pm_date_str, 
            "inspector": it.get(("inspector")), 
            "doc_name": it.get("doc_name"),
            "issue_id": it.get("issue_id"),                       # 'YYYY-MM-DD' | None
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,                          # à¹„à¸Ÿà¸¥à¹Œà¹à¸£à¸ (à¹„à¸§à¹‰à¹ƒà¸«à¹‰à¸›à¸¸à¹ˆà¸¡à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”)
            "urls": urls,                                   # à¹€à¸œà¸·à¹ˆà¸­à¸Ÿà¸£à¸­à¸™à¸•à¹Œà¸­à¸¢à¸²à¸à¹à¸ªà¸”à¸‡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
        })

    return {
        "items": items,
        "pm_date": [d for d in pm_date_arr if d],          # à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸·à¸­à¸™ /pmreport/list
        "page": page,
        "pageSize": pageSize,
        "total": total,
    }

# -------------------------------------------------- PMReportPage (MDB)       
def get_mdbpmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    return MDBPMReportDB.get_collection(str(station_id))

def get_mdbpmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    return MDBPMUrlDB.get_collection(str(station_id))

class MDBPMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    station_id: str
    job: Dict[str, Any]         # à¹‚à¸„à¸£à¸‡à¸‡à¸²à¸™ (location/date/inspector à¸¯à¸¥à¸¯)
    measures_pre: Dict[str, Dict[str, Any]] 
    # rows: Dict[str, Dict[str, Any]]  # {"r1": {"pf": "...", "remark": "..."}, ...}
    # measures: Dict[str, Dict[str, Any]]  # {"m4": {...}, "m5": {...}, ..., "m8": {...}}
    # summary: str
    pm_date: str                # "YYYY-MM-DD"
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None
    # summaryCheck: Optional[Literal["PASS","FAIL","NA"]] = None
    inspector: Optional[str] = None 
    # dust_filter: Optional[str] = None

@app.get("/mdbpmreport/preview-issueid")
async def mdbpmreport_preview_issueid(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¹ issue_id à¸–à¸±à¸”à¹„à¸› (PM-CG-YYMM-XX) à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸­à¸­à¸à¹€à¸¥à¸‚à¸ˆà¸£à¸´à¸‡
    à¹ƒà¸Šà¹‰à¸«à¸²à¹€à¸¥à¸‚à¹„à¸›à¹‚à¸Šà¸§à¹Œà¸šà¸™à¸Ÿà¸­à¸£à¹Œà¸¡à¹€à¸‰à¸¢ à¹†
    """
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "MB"

    latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="mdb")

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

@app.get("/mdbpmreport/latest-docname")
async def mdbpmreport_latest_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¶à¸‡ doc_name à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸ªà¸–à¸²à¸™à¸µ (à¸›à¸µà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š pm_date)
    à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸„à¸³à¸™à¸§à¸“à¹€à¸¥à¸‚à¸–à¸±à¸”à¹„à¸›à¸—à¸µà¹ˆ frontend
    """
    # auth à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    
    latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="mdb")
    
    return {
        "doc_name": latest.get("doc_name") if latest else None,
        "station_id": station_id,
        "pm_date": pm_date
    }

@app.get("/mdbpmreport/preview-docname")
async def preview_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(station_id, year,source="mdb")

    if not latest:
        next_doc = f"{station_id}_1/{year}"
    else:
        import re
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{station_id}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

@app.post("/mdbpmreport/pre/submit")
async def mdbpmreport_submit(body: MDBPMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_mdbpmreport_collection_for(station_id)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "MB").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_mdbpmurl_coll_upload(station_id)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    client_issue = body.issue_id 
    issue_id: str | None = None    

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)
        # unique = not await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            issue_id = client_issue

    if not issue_id:
        while True:
            candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
            rep_exists = await coll.find_one({"issue_id": candidate})
            url_exists = await url_coll.find_one({"issue_id": candidate})
            if not rep_exists and not url_exists:
                issue_id = candidate
                break

    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{station_id}_"
        valid_fmt = client_docName.startswith(prefix)

        url_coll = get_mdbpmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, station_id, pm_type, d)
        year = d.year
        doc_name = f"{station_id}_{year_seq}/{year}"

    inspector = body.inspector
    doc = {
        "station_id": station_id,
        "doc_name": doc_name,
        "issue_id": issue_id,
        "job": body.job,
        # "rows": body.rows,
        "measures_pre": body.measures_pre,         # m4..m8
        # "summary": body.summary,
        # "summaryCheck": body.summaryCheck,
        "pm_date": body.pm_date,           # string YYYY-MM-DD (à¸•à¸²à¸¡à¸Ÿà¸£à¸­à¸™à¸•à¹Œ)
        "inspector": inspector,
        # "dust_filter": body.dust_filter,
        "photos_pre": {},
        "status": "draft",
        "side": body.side,
        # "createdAt": datetime.now(timezone.utc),
        # "updatedAt": datetime.now(timezone.utc),
        "timestamp": datetime.now(timezone.utc),

    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id), 
        "issue_id": issue_id,
        "doc_name": doc_name,
    }

class MDBPMPostIn(BaseModel):
    report_id: str | None = None      # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
    station_id: str
    # issue_id: str | None = None
    # job: dict
    rows: dict
    measures: dict
    summary: str
    # pm_date: str
    # doc_name: str | None = None
    summaryCheck: str | None = None
    dust_filter: str | None = None
    side: Literal["post", "after"]

@app.post("/mdbpmreport/submit")
async def mdbpmreport_submit(body: MDBPMPostIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_mdbpmreport_collection_for(station_id)
    db = coll.database

    url_coll = get_mdbpmurl_coll_upload(station_id)

    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "station_id": station_id})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        update_fields = {
            # "job": body.job,
            "rows": body.rows,
            "measures": body.measures,          # à¹ƒà¸Šà¹‰à¹€à¸›à¹‡à¸™à¸„à¹ˆà¸²à¸«à¸¥à¸±à¸‡ PM
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            # "pm_date": body.pm_date,
            # "inspector": inspector,
            "dust_filter": body.dust_filter,
            # "doc_name": doc_name,
            "side": "post",                     # à¸•à¸­à¸™à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¸à¸±à¹ˆà¸‡ post à¹à¸¥à¹‰à¸§
            "timestamp_post": datetime.now(timezone.utc),
        }

        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
            # "issue_id": issue_id,
            # "doc_name": doc_name,
        }
    
    doc = {
        "station_id": station_id,
        # "doc_name": doc_name,
        # "issue_id": issue_id,
        # "job": body.job,
        "rows": body.rows,
        "measures": body.measures,         # m4..m8
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        # "pm_date": body.pm_date,           # string YYYY-MM-DD (à¸•à¸²à¸¡à¸Ÿà¸£à¸­à¸™à¸•à¹Œ)
        # "inspector": inspector,
        "dust_filter": body.dust_filter,
        "status": "draft",
        "photos": {},                      # à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¹ƒà¸™ /photos
        # "createdAt": datetime.now(timezone.utc),
        # "updatedAt": datetime.now(timezone.utc),
        "side": "post",
        "timestamp": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id), 
        # "issue_id": issue_id,
        # "doc_name": doc_name,
    }

@app.get("/mdbpmreport/get")
async def mdbpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    coll = get_mdbpmreport_collection_for(station_id)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    return doc

@app.get("/mdbpmreport/list")
async def mdbpmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_mdbpmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1 , "pm_date": 1,"inspector":1,"side":1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # à¸œà¸¹à¸ URL PDF à¸£à¸²à¸¢à¸§à¸±à¸™à¸ˆà¸²à¸ MDBPMUrlDB (à¸–à¹‰à¸²à¸¡à¸µ)
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    url_by_day: Dict[str, str] = {}
    if pm_dates:
        ucoll = get_mdbpmurl_coll_upload(station_id)
        ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "inspector": it.get(("inspector")),
        "side":it.get("side"), 
        "pm_date": it.get("pm_date"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

@app.post("/mdbpmreport/{report_id}/pre/photos")
async def mdbpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # à¹€à¸Šà¹ˆà¸™ "g1" .. "g10"
    files: list[UploadFile] = File(...),
    # remark: str | None = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    # Accept both formats: "g1", "g2" (simple questions) and "r9_1", "r9_2" (group sub-items)
    if not re.fullmatch(r"(g\d+|r\d+_\d+)", group):
        raise HTTPException(status_code=400, detail=f"Bad group key format: {group}")

    # Map group key to database storage key:
    # - "g1" -> "g1", "g2" -> "g2", etc. (keep g prefix)
    # - "r9_1", "r9_2", "r9_3", "r9_4" -> "g9" (all merged into question 9 with g prefix)
    storage_key = group
    group_match = re.match(r"r(\d+)_\d+", group)
    if group_match:  # Convert r9_1 format to g9
        question_num = group_match.group(1)  # Extract question number (e.g., "9")
        storage_key = f"g{question_num}"  # Add g prefix for database
    # else: keep group as-is (e.g., "g1" stays "g1")

    coll = get_mdbpmreport_collection_for(station_id)
    from bson import ObjectId
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # à¸¢à¸·à¸™à¸¢à¸±à¸™à¸§à¹ˆà¸²à¸£à¸²à¸¢à¸‡à¸²à¸™à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ station à¸™à¸µà¹‰
    doc = await coll.find_one({"_id": oid}, {"_id":1, "station_id":1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡ (use storage_key for consistent path)
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpm" / station_id / report_id  / "pre" / storage_key
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    total = 0
    for f in files:
        ext = _ext(f.filename or "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        total += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        # URL à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸ªà¸”à¸‡à¸šà¸™ Frontend (use storage_key for consistent URL)
        url_path = f"/uploads/mdbpm/{station_id}/{report_id}/pre/{storage_key}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            # "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    # à¸­à¸±à¸›à¹€à¸”à¸•à¹€à¸­à¸à¸ªà¸²à¸£ PMReport: push à¸¥à¸‡ photos_pre.<storage_key>
    # Note: storage_key already calculated above
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos_pre.{storage_key}": {"$each": saved}}}
    )

    return {"ok": True, "count": len(saved), "group": group, "files": saved}



@app.post("/mdbpmreport/{report_id}/post/photos")
async def mdbpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "g1" .. "g11"
    files: List[UploadFile] = File(...),
    remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    # Accept both formats: "g1", "g2" (simple questions) and "r9_1", "r9_2" (group sub-items)
    if not re.fullmatch(r"(g\d+|r\d+_\d+)", group):
        raise HTTPException(status_code=400, detail=f"Bad group key format: {group}")

    # Map group key to database storage key:
    # - "g1" -> "g1", "g2" -> "g2", etc. (keep g prefix)
    # - "r9_1", "r9_2", "r9_3", "r9_4" -> "g9" (all merged into question 9 with g prefix)
    storage_key = group
    group_match = re.match(r"r(\d+)_\d+", group)
    if group_match:  # Convert r9_1 format to g9
        question_num = group_match.group(1)  # Extract question number (e.g., "9")
        storage_key = f"g{question_num}"  # Add g prefix for database
    # else: keep group as-is (e.g., "g1" stays "g1")

    coll = get_mdbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: /uploads/mdbpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpm" / station_id / report_id / "post" / storage_key
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/mdbpm/{station_id}/{report_id}/post/{storage_key}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    # Note: storage_key mapping already done above (before file operations)
    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos.{storage_key}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/mdbpmreport/{report_id}/finalize")
async def mdbpmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_mdbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # (à¸­à¸­à¸›à¸Šà¸±à¸™) à¸•à¸£à¸§à¸ˆà¸„à¸§à¸²à¸¡à¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¸à¹ˆà¸­à¸™ finalize à¹„à¸”à¹‰à¸—à¸µà¹ˆà¸™à¸µà¹ˆ
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/mdbpmurl/upload-files", status_code=201)
async def mdbpmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),            # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO -> à¸ˆà¸° normalize à¹€à¸›à¹‡à¸™ YYYY-MM-DD
    files: List[UploadFile] = File(...),    # à¸­à¸™à¸¸à¸à¸²à¸•à¹€à¸‰à¸žà¸²à¸° .pdf
    issue_id: Optional[str] = Form(None),
    doc_name: Optional[str] = Form(None),
    inspector: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_mdbpmurl_coll_upload(station_id)
    pm_date = normalize_pm_date(reportDate)  # à¸„à¸·à¸™ YYYY-MM-DD

    # à¹€à¸à¹‡à¸šà¹„à¸§à¹‰à¸—à¸µà¹ˆ /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpmurl" / station_id / pm_date
    dest_dir.mkdir(parents=True, exist_ok=True)

    pm_type = "MB"
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

    rep_coll = get_mdbpmreport_collection_for(station_id)
    final_issue_id = None
    client_issue = (issue_id or "").strip()

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        url_exists = await coll.find_one({"issue_id": client_issue})
        rep_exists = await rep_coll.find_one({"issue_id": client_issue})
        unique = not (url_exists or rep_exists)

        if valid_fmt and unique:
            final_issue_id = client_issue

    if not final_issue_id:
        while True:
            candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
            url_exists = await coll.find_one({"issue_id": candidate})
            rep_exists = await rep_coll.find_one({"issue_id": candidate})
            if not url_exists and not rep_exists:
                final_issue_id = candidate
                break

    year_seq: int | None = None

    # à¸žà¸¢à¸²à¸¢à¸²à¸¡ reuse year_seq à¸ˆà¸²à¸ PMReportDB à¸–à¹‰à¸²à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ (issue_id à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™)
    rep = await get_mdbpmreport_collection_for(station_id).find_one(
        {"issue_id": final_issue_id},
        {"year_seq": 1, "pm_date": 1},
    )
    if rep and rep.get("year_seq") is not None:
        year_seq = int(rep["year_seq"])

    # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¹ˆà¸² year_seq à¸ˆà¸²à¸ PMReport â†’ à¸­à¸­à¸à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸ pm_year_sequences
    if year_seq is None:
        year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

    year = d.year
    final_doc_name: str | None = None

    if doc_name:
        candidate = doc_name.strip()

        ok_format = candidate.startswith(f"{station_id}_")

        rep_coll = get_mdbpmreport_collection_for(station_id)
        rep_exists = await rep_coll.find_one({"doc_name": candidate})
        url_exists = await coll.find_one({"doc_name": candidate})
        unique = not (rep_exists or url_exists)

        if ok_format and unique:
            final_doc_name = candidate

    if not final_doc_name:
        final_doc_name = f"{station_id}_{year_seq}/{year}"

    doc_name = final_doc_name

    # à¹€à¸à¹‡à¸šà¹„à¸§à¹‰à¸—à¸µà¹ˆ /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpmurl" / station_id / pm_date
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls, metas = [], []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url = f"/uploads/mdbpmurl/{station_id}/{pm_date}/{fname}"
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    inspector_clean = (inspector or "").strip() or None
    now = datetime.now(timezone.utc)
    res = await coll.insert_one({
        "station": station_id,
        "pm_date": pm_date,
        "inspector": inspector_clean,
        "doc_name": doc_name,
        "issue_id": final_issue_id, 
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    })
    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean,}

@app.get("/mdbpmurl/list")
async def mdbpmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_mdbpmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find(
        {},
        {"_id": 1, "issue_id": 1, "doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    items = []
    for it in items_raw:
        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""
        items.append({
            "id": str(it["_id"]),
            "pm_date": it.get("pm_date"),
            "issue_id": it.get("issue_id"),
            "inspector": it.get(("inspector")), 
            "doc_name": it.get("doc_name"),
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,
            "urls": urls,
        })

    return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# -------------------------------------------------- PMReportPage (CCB)       
def get_ccbpmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    return CCBPMReportDB.get_collection(str(station_id))

def get_ccbpmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    return CCBPMUrlDB.get_collection(str(station_id))

class CCBPMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    station_id: str
    job: Dict[str, Any]         # à¹‚à¸„à¸£à¸‡à¸‡à¸²à¸™ (location/date/inspector à¸¯à¸¥à¸¯)
    # rows: Dict[str, Dict[str, Any]]  # {"r1": {"pf": "...", "remark": "..."}, ...}
    measures_pre : Dict[str, Dict[str, Any]]  # {"m4": {...}, "m5": {...}, ..., "m8": {...}}
    # summary: str
    pm_date: str                # "YYYY-MM-DD"
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None 
    # summaryCheck: Optional[Literal["PASS","FAIL","NA"]] = None
    inspector: Optional[str] = None

@app.get("/ccbpmreport/preview-issueid")
async def ccbpmreport_preview_issueid(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¹ issue_id à¸–à¸±à¸”à¹„à¸› (PM-CG-YYMM-XX) à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸­à¸­à¸à¹€à¸¥à¸‚à¸ˆà¸£à¸´à¸‡
    à¹ƒà¸Šà¹‰à¸«à¸²à¹€à¸¥à¸‚à¹„à¸›à¹‚à¸Šà¸§à¹Œà¸šà¸™à¸Ÿà¸­à¸£à¹Œà¸¡à¹€à¸‰à¸¢ à¹†
    """
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "CC"

    latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="ccb")

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

@app.get("/ccbpmreport/latest-docname")
async def ccbpmreport_latest_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¶à¸‡ doc_name à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸ªà¸–à¸²à¸™à¸µ (à¸›à¸µà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š pm_date)
    à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸„à¸³à¸™à¸§à¸“à¹€à¸¥à¸‚à¸–à¸±à¸”à¹„à¸›à¸—à¸µà¹ˆ frontend
    """
    # auth à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    
    latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="ccb")
    
    return {
        "doc_name": latest.get("doc_name") if latest else None,
        "station_id": station_id,
        "pm_date": pm_date
    }

@app.get("/ccbpmreport/preview-docname")
async def preview_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(station_id, year,source="ccb")

    if not latest:
        next_doc = f"{station_id}_1/{year}"
    else:
        import re
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{station_id}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

@app.post("/ccbpmreport/pre/submit")
async def ccbpmreport_submit(body: CCBPMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_ccbpmreport_collection_for(station_id)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "CC").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_ccbpmurl_coll_upload(station_id)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    client_issue = body.issue_id 
    issue_id: str | None = None    

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        # â­ à¹€à¸Šà¹‡à¸„à¸—à¸±à¹‰à¸‡ PMReportDB + PMUrlDB
        # url_coll = get_pmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            issue_id = client_issue
    
    # if not issue_id:
    #     issue_id = await _next_issue_id(db, station_id, pm_type, d, pad=2)
    if not issue_id:
        while True:
            candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
            rep_exists = await coll.find_one({"issue_id": candidate})
            url_exists = await url_coll.find_one({"issue_id": candidate})
            if not rep_exists and not url_exists:
                issue_id = candidate
                break
   
    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{station_id}_"
        valid_fmt = client_docName.startswith(prefix)

        url_coll = get_ccbpmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, station_id, pm_type, d)
        year = d.year
        doc_name = f"{station_id}_{year_seq}/{year}"

    inspector = body.inspector
    # à¹€à¸à¹‡à¸šà¹€à¸­à¸à¸ªà¸²à¸£à¹€à¸›à¹‡à¸™ draft à¸à¹ˆà¸­à¸™
    doc = {
        "station_id": station_id,
        "doc_name": doc_name,
        "issue_id": issue_id,
        "job": body.job,
        # "rows": body.rows,
        "measures_pre": body.measures_pre,         # m4..m8
        # "summary": body.summary,
        # "summaryCheck": body.summaryCheck,
        "pm_date": body.pm_date,           # string YYYY-MM-DD (à¸•à¸²à¸¡à¸Ÿà¸£à¸­à¸™à¸•à¹Œ)
        "status": "draft",
        # "photos": {},                      # à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¹ƒà¸™ /photos
        "photos_pre": {},
        "inspector": inspector,
        "side": body.side,
        # "createdAt": datetime.now(timezone.utc),
        # "updatedAt": datetime.now(timezone.utc),
        "timestamp": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id), 
        "issue_id": issue_id,
        "doc_name": doc_name,
    }

class CCBPMPostIn(BaseModel):
    report_id: str | None = None      # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
    station_id: str
    # issue_id: str | None = None
    # job: dict
    rows: dict
    measures: dict
    summary: str
    # pm_date: str
    # doc_name: str | None = None
    summaryCheck: str | None = None
    # dust_filter: str | None = None
    side: Literal["post", "after"]

@app.post("/ccbpmreport/submit")
async def ccbpmreport_submit(body: CCBPMPostIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_ccbpmreport_collection_for(station_id)
    db = coll.database

    url_coll = get_ccbpmurl_coll_upload(station_id)

   
    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "station_id": station_id})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        # reuse à¸„à¹ˆà¸²à¹€à¸”à¸´à¸¡ à¹„à¸¡à¹ˆ gen à¹ƒà¸«à¸¡à¹ˆ
        # issue_id = existing.get("issue_id")
        # doc_name = existing.get("doc_name")
        # inspector = body.inspector or existing.get("inspector") or current.username

        update_fields = {
            # "job": body.job,
            "rows": body.rows,
            "measures": body.measures,          # à¹ƒà¸Šà¹‰à¹€à¸›à¹‡à¸™à¸„à¹ˆà¸²à¸«à¸¥à¸±à¸‡ PM
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            # "pm_date": body.pm_date,
            # "inspector": inspector,
            # "dust_filter": body.dust_filter,
            # "doc_name": doc_name,
            "side": "post",                     # à¸•à¸­à¸™à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¸à¸±à¹ˆà¸‡ post à¹à¸¥à¹‰à¸§
            "timestamp_post": datetime.now(timezone.utc),
        }

        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
            # "issue_id": issue_id,
            # "doc_name": doc_name,
        }
    
    doc = {
        "station_id": station_id,
        # "doc_name": doc_name,
        # "issue_id": issue_id,
        # "job": body.job,
        "rows": body.rows,
        "measures": body.measures,         # m4..m8
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        # "pm_date": body.pm_date,           # string YYYY-MM-DD (à¸•à¸²à¸¡à¸Ÿà¸£à¸­à¸™à¸•à¹Œ)
        # "inspector": inspector,
        # "dust_filter": body.dust_filter,
        "status": "draft",
        "photos": {},                      # à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¹ƒà¸™ /photos
        # "createdAt": datetime.now(timezone.utc),
        # "updatedAt": datetime.now(timezone.utc),
        "side": "post",
        "timestamp": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id), 
        # "issue_id": issue_id,
        # "doc_name": doc_name,
    }

@app.get("/ccbpmreport/get")
async def ccbpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    coll = get_ccbpmreport_collection_for(station_id)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    return doc

@app.get("/ccbpmreport/list")
async def ccbpmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_ccbpmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side":1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # à¸œà¸¹à¸ URL PDF à¸£à¸²à¸¢à¸§à¸±à¸™à¸ˆà¸²à¸ CCBPMUrlDB (à¸–à¹‰à¸²à¸¡à¸µ)
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    url_by_day: Dict[str, str] = {}
    if pm_dates:
        ucoll = get_ccbpmurl_coll_upload(station_id)
        ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "pm_date": it.get("pm_date"),
        "side":it.get("side"), 
        "inspector": it.get("inspector"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

@app.post("/ccbpmreport/{report_id}/pre/photos")
async def ccbpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "r1" .. "r10", "r9_0" .. "r9_5"
    files: List[UploadFile] = File(...),
    # remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"r\d+(_\d+)?", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_ccbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: /uploads/ccbpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "ccbpm" / station_id / report_id / "pre" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/ccbpm/{station_id}/{report_id}/pre/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            # "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos_pre.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}


@app.post("/ccbpmreport/{report_id}/post/photos")
async def ccbpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "r1" .. "r10", "r9_0" .. "r9_5"
    files: List[UploadFile] = File(...),
    remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"r\d+(_\d+)?", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_ccbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: /uploads/ccbpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "ccbpm" / station_id / report_id / "post" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/ccbpm/{station_id}/{report_id}/post/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/ccbpmreport/{report_id}/finalize")
async def ccbpmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_ccbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # (à¸­à¸­à¸›à¸Šà¸±à¸™) à¸•à¸£à¸§à¸ˆà¸„à¸§à¸²à¸¡à¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¸à¹ˆà¸­à¸™ finalize à¹„à¸”à¹‰à¸—à¸µà¹ˆà¸™à¸µà¹ˆ
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/ccbpmurl/upload-files", status_code=201)
async def ccbpmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),            # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO -> à¸ˆà¸° normalize à¹€à¸›à¹‡à¸™ YYYY-MM-DD
    files: List[UploadFile] = File(...),    # à¸­à¸™à¸¸à¸à¸²à¸•à¹€à¸‰à¸žà¸²à¸° .pdf
    # current: UserClaims = Depends(get_current_user),
    issue_id: Optional[str] = Form(None),
    doc_name: Optional[str] = Form(None),
    inspector: Optional[str] = Form(None),
):
    coll = get_ccbpmurl_coll_upload(station_id)
    pm_date = normalize_pm_date(reportDate)  # à¸„à¸·à¸™ YYYY-MM-DD

    pm_type = "CC"
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

    rep_coll = get_ccbpmreport_collection_for(station_id)
    final_issue_id = None
    client_issue = (issue_id or "").strip()

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        url_exists = await coll.find_one({"issue_id": client_issue})
        rep_exists = await rep_coll.find_one({"issue_id": client_issue})
        unique = not (url_exists or rep_exists)

        if valid_fmt and unique:
            final_issue_id = client_issue

    if not final_issue_id:
        while True:
            candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
            url_exists = await coll.find_one({"issue_id": candidate})
            rep_exists = await rep_coll.find_one({"issue_id": candidate})
            if not url_exists and not rep_exists:
                final_issue_id = candidate
                break
        
    year_seq: int | None = None

    # à¸žà¸¢à¸²à¸¢à¸²à¸¡ reuse year_seq à¸ˆà¸²à¸ PMReportDB à¸–à¹‰à¸²à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ (issue_id à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™)
    rep = await get_ccbpmreport_collection_for(station_id).find_one(
        {"issue_id": final_issue_id},
        {"year_seq": 1, "pm_date": 1},
    )
    if rep and rep.get("year_seq") is not None:
        year_seq = int(rep["year_seq"])

    # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¹ˆà¸² year_seq à¸ˆà¸²à¸ PMReport â†’ à¸­à¸­à¸à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸ pm_year_sequences
    if year_seq is None:
        year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

    year = d.year
    final_doc_name: str | None = None

    if doc_name:
        candidate = doc_name.strip()

        ok_format = candidate.startswith(f"{station_id}_")

        rep_coll = get_ccbpmreport_collection_for(station_id)
        rep_exists = await rep_coll.find_one({"doc_name": candidate})
        url_exists = await coll.find_one({"doc_name": candidate})
        unique = not (rep_exists or url_exists)

        if ok_format and unique:
            final_doc_name = candidate

    if not final_doc_name:
        final_doc_name = f"{station_id}_{year_seq}/{year}"

    doc_name = final_doc_name
    
    # à¹€à¸à¹‡à¸šà¹„à¸§à¹‰à¸—à¸µà¹ˆ /uploads/ccbpmurl/<station_id>/<YYYY-MM-DD>/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "ccbpmurl" / station_id / pm_date
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls, metas = [], []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url = f"/uploads/ccbpmurl/{station_id}/{pm_date}/{fname}"
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    inspector_clean = (inspector or "").strip() or None
    now = datetime.now(timezone.utc)
    res = await coll.insert_one({
        "station": station_id,
        "pm_date": pm_date,
        "issue_id": final_issue_id, 
        "inspector": inspector_clean,
        "doc_name": doc_name,
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    })
    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean,}

@app.get("/ccbpmurl/list")
async def ccbpmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_ccbpmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find(
        {},
        {"_id": 1, "issue_id": 1,"doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    items = []
    for it in items_raw:
        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""
        items.append({
            "id": str(it["_id"]),
            "pm_date": it.get("pm_date"),
            "issue_id": it.get("issue_id"),
            "inspector": it.get(("inspector")), 
            "doc_name": it.get("doc_name"),
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,
            "urls": urls,
        })

    return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# -------------------------------------------------- PMReportPage (CB-BOX)       
def get_cbboxpmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    return CBBOXPMReportDB.get_collection(str(station_id))

def get_cbboxpmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    return CBBOXPMUrlDB.get_collection(str(station_id))

class CBBOXPMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    station_id: str
    job: Dict[str, Any]         # à¹‚à¸„à¸£à¸‡à¸‡à¸²à¸™ (location/date/inspector à¸¯à¸¥à¸¯)
    # rows: Dict[str, Dict[str, Any]]  # {"r1": {"pf": "...", "remark": "..."}, ...}
    measures_pre: Dict[str, Dict[str, Any]]  # {"m4": {...}, "m5": {...}, ..., "m8": {...}}
    # summary: str
    pm_date: str                # "YYYY-MM-DD"
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None 
    # summaryCheck: Optional[Literal["PASS","FAIL","NA"]] = None
    inspector: Optional[str] = None
    dropdownQ1: Optional[str] = None  # âœ… à¹€à¸žà¸´à¹ˆà¸¡ dropdown Q1
    dropdownQ2: Optional[str] = None  # âœ… à¹€à¸žà¸´à¹ˆà¸¡ dropdown Q2

@app.get("/cbboxpmreport/preview-issueid")
async def cbboxpmreport_preview_issueid(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¹ issue_id à¸–à¸±à¸”à¹„à¸› (PM-CG-YYMM-XX) à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸­à¸­à¸à¹€à¸¥à¸‚à¸ˆà¸£à¸´à¸‡
    à¹ƒà¸Šà¹‰à¸«à¸²à¹€à¸¥à¸‚à¹„à¸›à¹‚à¸Šà¸§à¹Œà¸šà¸™à¸Ÿà¸­à¸£à¹Œà¸¡à¹€à¸‰à¸¢ à¹†
    """
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "CB"

    latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="cbbox")

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

@app.get("/cbboxpmreport/latest-docname")
async def cbboxpmreport_latest_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¶à¸‡ doc_name à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸ªà¸–à¸²à¸™à¸µ (à¸›à¸µà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š pm_date)
    à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸„à¸³à¸™à¸§à¸“à¹€à¸¥à¸‚à¸–à¸±à¸”à¹„à¸›à¸—à¸µà¹ˆ frontend
    """
    # auth à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    
    latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="cbbox")
    
    return {
        "doc_name": latest.get("doc_name") if latest else None,
        "station_id": station_id,
        "pm_date": pm_date
    }

@app.get("/cbboxpmreport/preview-docname")
async def preview_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(station_id, year,source="cbbox")

    if not latest:
        next_doc = f"{station_id}_1/{year}"
    else:
        import re
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{station_id}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

@app.post("/cbboxpmreport/pre/submit")
async def cbboxpmreport_submit(body: CBBOXPMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_cbboxpmreport_collection_for(station_id)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "CB").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_cbboxpmurl_coll_upload(station_id)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    client_issue = body.issue_id 
    issue_id: str | None = None

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        if valid_fmt and unique:
            issue_id = client_issue

    # if not issue_id:
    #     issue_id = await _next_issue_id(db, station_id, pm_type, d, pad=2)
        
    if not issue_id:
        while True:
            candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
            rep_exists = await coll.find_one({"issue_id": candidate})
            url_exists = await url_coll.find_one({"issue_id": candidate})
            if not rep_exists and not url_exists:
                issue_id = candidate
                break

    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{station_id}_"
        valid_fmt = client_docName.startswith(prefix)

        url_coll = get_cbboxpmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, station_id, pm_type, d)
        year = d.year
        doc_name = f"{station_id}_{year_seq}/{year}"

    doc = {
        "station_id": station_id,
        "doc_name": doc_name,
        "issue_id": issue_id,
        "job": body.job,
        "dropdownQ1" : body.dropdownQ1,  # âœ… à¹€à¸žà¸´à¹ˆà¸¡ dropdown Q1
        "dropdownQ2" : body.dropdownQ2,  # âœ… à¹€à¸žà¸´à¹ˆà¸¡ dropdown Q2
        # "rows": body.rows,
        "measures_pre": body.measures_pre,         # m4..m8
        # "summary": body.summary,
        # "summaryCheck": body.summaryCheck,
        "pm_date": body.pm_date,           # string YYYY-MM-DD (à¸•à¸²à¸¡à¸Ÿà¸£à¸­à¸™à¸•à¹Œ)
        "status": "draft",
        "photos_pre": {},                      # à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¹ƒà¸™ /photos
        "inspector": body.inspector,
        "side": body.side,
        "createdAt": datetime.now(timezone.utc),
        # "updatedAt": datetime.now(timezone.utc),
        # "timestamp": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id),
        "issue_id": issue_id,
        "doc_name": doc_name,
    }
       
class CBBOXPMPostIn(BaseModel):
    report_id: str | None = None      # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
    station_id: str
    # issue_id: str | None = None
    # job: dict
    rows: dict
    measures: dict
    summary: str
    # pm_date: str
    # doc_name: str | None = None
    summaryCheck: str | None = None
    dropdownQ1: Optional[str] = None  # âœ… à¹€à¸žà¸´à¹ˆà¸¡ dropdown Q1
    dropdownQ2: Optional[str] = None  # âœ… à¹€à¸žà¸´à¹ˆà¸¡ dropdown Q2
    # dust_filter: str | None = None
    side: Literal["post", "after"]

@app.post("/cbboxpmreport/submit")
async def cbboxpmreport_submit(body: CBBOXPMPostIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_cbboxpmreport_collection_for(station_id)
    db = coll.database

    url_coll = get_cbboxpmurl_coll_upload(station_id)

    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "station_id": station_id})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        update_fields = {
            # "job": body.job,
            "rows": body.rows,
            "measures": body.measures,          # à¹ƒà¸Šà¹‰à¹€à¸›à¹‡à¸™à¸„à¹ˆà¸²à¸«à¸¥à¸±à¸‡ PM
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            # "pm_date": body.pm_date,
            # "inspector": inspector,
            # "dust_filter": body.dust_filter,
            # "doc_name": doc_name,
            "dropdownQ1": body.dropdownQ1,  # âœ… à¹€à¸žà¸´à¹ˆà¸¡ dropdown Q1
            "dropdownQ2": body.dropdownQ2,  # âœ… à¹€à¸žà¸´à¹ˆà¸¡ dropdown Q2
            "side": "post",                     # à¸•à¸­à¸™à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¸à¸±à¹ˆà¸‡ post à¹à¸¥à¹‰à¸§
            "updatedAt": datetime.now(timezone.utc),
        }

        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
            # "issue_id": issue_id,
            # "doc_name": doc_name,
        }

    doc = {
        "station_id": station_id,
        # "doc_name": doc_name,
        # "issue_id": issue_id,
        # "job": body.job,
        "rows": body.rows,
        # "measures_pre": body.measures_pre,         # m4..m8
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        "measures": body.measures, 
        # "pm_date": body.pm_date,           # string YYYY-MM-DD (à¸•à¸²à¸¡à¸Ÿà¸£à¸­à¸™à¸•à¹Œ)
        "status": "draft",
        "photos": {},                      # à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¹ƒà¸™ /photos
        # "inspector": body.inspector,
        "side": body.side,
        # "createdAt": datetime.now(timezone.utc),
        "updatedAt": datetime.now(timezone.utc),
        # "timestamp": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id),
        # "issue_id": issue_id,
        # "doc_name": doc_name,
    }

@app.get("/cbboxpmreport/get")
async def cbboxpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    coll = get_cbboxpmreport_collection_for(station_id)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    return doc


@app.get("/cbboxpmreport/list")
async def cbboxpmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_cbboxpmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side":1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # à¸œà¸¹à¸ URL PDF à¸£à¸²à¸¢à¸§à¸±à¸™à¸ˆà¸²à¸ CCBPMUrlDB (à¸–à¹‰à¸²à¸¡à¸µ)
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    url_by_day: Dict[str, str] = {}
    if pm_dates:
        ucoll = get_cbboxpmurl_coll_upload(station_id)
        ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "pm_date": it.get("pm_date"),
        "inspector": it.get("inspector"),
        "side":it.get("side"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

@app.post("/cbboxpmreport/{report_id}/pre/photos")
async def cbboxpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "g1" .. "g11"
    files: List[UploadFile] = File(...),
    # remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"g\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_cbboxpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: /uploads/cbboxpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cbboxpm" / station_id / report_id / "pre" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/cbboxpm/{station_id}/{report_id}/pre/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            # "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos_pre.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}


@app.post("/cbboxpmreport/{report_id}/post/photos")
async def cbboxpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "g1" .. "g11"
    files: List[UploadFile] = File(...),
    remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"g\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_cbboxpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: /uploads/cbboxpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cbboxpm" / station_id / report_id / "post" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/cbboxpm/{station_id}/{report_id}/post/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/cbboxpmreport/{report_id}/finalize")
async def cbboxpmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_cbboxpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # (à¸­à¸­à¸›à¸Šà¸±à¸™) à¸•à¸£à¸§à¸ˆà¸„à¸§à¸²à¸¡à¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¸à¹ˆà¸­à¸™ finalize à¹„à¸”à¹‰à¸—à¸µà¹ˆà¸™à¸µà¹ˆ
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/cbboxpmurl/upload-files", status_code=201)
async def cbboxpmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),            # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO -> à¸ˆà¸° normalize à¹€à¸›à¹‡à¸™ YYYY-MM-DD
    files: List[UploadFile] = File(...),    # à¸­à¸™à¸¸à¸à¸²à¸•à¹€à¸‰à¸žà¸²à¸° .pdf
    # current: UserClaims = Depends(get_current_user),
    issue_id: Optional[str] = Form(None),
    doc_name: Optional[str] = Form(None),
    inspector: Optional[str] = Form(None),
):
    coll = get_cbboxpmurl_coll_upload(station_id)
    pm_date = normalize_pm_date(reportDate)  # à¸„à¸·à¸™ YYYY-MM-DD

    pm_type = "CB"
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

    rep_coll = get_cbboxpmreport_collection_for(station_id)
    final_issue_id = None
    client_issue = (issue_id or "").strip()

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        # à¸•à¸£à¸§à¸ˆ uniqueness à¹ƒà¸™à¸—à¸±à¹‰à¸‡ 2 à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™
        url_exists = await coll.find_one({"issue_id": client_issue})
        rep_exists = await rep_coll.find_one({"issue_id": client_issue})
        unique = not (url_exists or rep_exists)

        if valid_fmt and unique:
            final_issue_id = client_issue

    # if not final_issue_id:
    #     # à¸­à¸­à¸à¹€à¸¥à¸‚à¸–à¸±à¸”à¹„à¸›à¹à¸šà¸š atomic à¸ˆà¸²à¸ pm_sequences
    #     final_issue_id = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
    if not final_issue_id:
        while True:
            candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
            url_exists = await coll.find_one({"issue_id": candidate})
            rep_exists = await rep_coll.find_one({"issue_id": candidate})
            if not url_exists and not rep_exists:
                final_issue_id = candidate
                break

    year_seq: int | None = None
    # à¸žà¸¢à¸²à¸¢à¸²à¸¡ reuse year_seq à¸ˆà¸²à¸ PMReportDB à¸–à¹‰à¸²à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ (issue_id à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™)
    rep = await get_cbboxpmreport_collection_for(station_id).find_one(
        {"issue_id": final_issue_id},
        {"year_seq": 1, "pm_date": 1},
    )
    if rep and rep.get("year_seq") is not None:
        year_seq = int(rep["year_seq"])

    # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¹ˆà¸² year_seq à¸ˆà¸²à¸ PMReport â†’ à¸­à¸­à¸à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸ pm_year_sequences
    if year_seq is None:
        year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

    year = d.year
    final_doc_name: str | None = None

    if doc_name:
        candidate = doc_name.strip()

        ok_format = candidate.startswith(f"{station_id}_")

        rep_coll = get_cbboxpmreport_collection_for(station_id)
        rep_exists = await rep_coll.find_one({"doc_name": candidate})
        url_exists = await coll.find_one({"doc_name": candidate})
        unique = not (rep_exists or url_exists)

        if ok_format and unique:
            final_doc_name = candidate

    if not final_doc_name:
        final_doc_name = f"{station_id}_{year_seq}/{year}"

    doc_name = final_doc_name

    # à¹€à¸à¹‡à¸šà¹„à¸§à¹‰à¸—à¸µà¹ˆ /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cbboxpmurl" / station_id / pm_date
    dest_dir.mkdir(parents=True, exist_ok=True)


    urls, metas = [], []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url = f"/uploads/cbboxpmurl/{station_id}/{pm_date}/{fname}"
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})
    
    inspector_clean = (inspector or "").strip() or None
    now = datetime.now(timezone.utc)
    res = await coll.insert_one({
        "station": station_id,
        "issue_id": final_issue_id, 
        "inspector": inspector_clean,
        "doc_name": doc_name,
        "pm_date": pm_date,
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    })
    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean}

@app.get("/cbboxpmurl/list")
async def cbboxpmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_cbboxpmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find(
        {},
        {"_id": 1, "issue_id": 1,"doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    items = []
    for it in items_raw:
        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""
        items.append({
            "id": str(it["_id"]),
            "pm_date": it.get("pm_date"),
            "issue_id": it.get("issue_id"),
            "inspector": it.get(("inspector")), 
            "doc_name": it.get("doc_name"),
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,
            "urls": urls,
        })

    return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# -------------------------------------------------- PMReportPage (station)       
def get_stationpmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    return stationPMReportDB.get_collection(str(station_id))

def get_stationpmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    return stationPMUrlDB.get_collection(str(station_id))

class stationPMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    station_id: str
    job: Dict[str, Any]         # à¹‚à¸„à¸£à¸‡à¸‡à¸²à¸™ (location/date/inspector à¸¯à¸¥à¸¯)
    # rows: Dict[str, Dict[str, Any]]  # {"r1": {"pf": "...", "remark": "..."}, ...}
    # measures: Dict[str, Dict[str, Any]]  # {"m4": {...}, "m5": {...}, ..., "m8": {...}}
    # summary: str
    pm_date: str                # "YYYY-MM-DD"
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None 
    # summaryCheck: Optional[Literal["PASS","FAIL","NA"]] = None
    inspector: Optional[str] = None

@app.get("/stationpmreport/preview-issueid")
async def stationpmreport_preview_issueid(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¹ issue_id à¸–à¸±à¸”à¹„à¸› (PM-CG-YYMM-XX) à¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¸­à¸­à¸à¹€à¸¥à¸‚à¸ˆà¸£à¸´à¸‡
    à¹ƒà¸Šà¹‰à¸«à¸²à¹€à¸¥à¸‚à¹„à¸›à¹‚à¸Šà¸§à¹Œà¸šà¸™à¸Ÿà¸­à¸£à¹Œà¸¡à¹€à¸‰à¸¢ à¹†
    """
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "ST"

    latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="station")

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

@app.get("/stationpmreport/latest-docname")
async def stationpmreport_latest_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    à¸”à¸¶à¸‡ doc_name à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸‚à¸­à¸‡à¸ªà¸–à¸²à¸™à¸µ (à¸›à¸µà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š pm_date)
    à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¸„à¸³à¸™à¸§à¸“à¹€à¸¥à¸‚à¸–à¸±à¸”à¹„à¸›à¸—à¸µà¹ˆ frontend
    """
    latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="station")
    
    return {
        "doc_name": latest.get("doc_name") if latest else None,
        "station_id": station_id,
        "pm_date": pm_date
    }

@app.get("/stationpmreport/preview-docname")
async def preview_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(station_id, year,source="station")

    if not latest:
        next_doc = f"{station_id}_1/{year}"
    else:
        import re
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{station_id}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

@app.post("/stationpmreport/pre/submit")
async def stationpmreport_submit(body: stationPMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_stationpmreport_collection_for(station_id)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "ST").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_stationpmurl_coll_upload(station_id)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    client_issue = body.issue_id 
    issue_id: str | None = None
    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)
        
        rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        if valid_fmt and unique:
            issue_id = client_issue

    # if not issue_id:
    #     issue_id = await _next_issue_id(db, station_id, pm_type, d, pad=2)
    if not issue_id:
            while True:
                candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
                rep_exists = await coll.find_one({"issue_id": candidate})
                url_exists = await url_coll.find_one({"issue_id": candidate})
                if not rep_exists and not url_exists:
                    issue_id = candidate
                    break  

    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{station_id}_"
        valid_fmt = client_docName.startswith(prefix)

        url_coll = get_stationpmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, station_id, pm_type, d)
        year = d.year
        doc_name = f"{station_id}_{year_seq}/{year}"

    # à¹€à¸à¹‡à¸šà¹€à¸­à¸à¸ªà¸²à¸£à¹€à¸›à¹‡à¸™ draft à¸à¹ˆà¸­à¸™
    doc = {
        "station_id": station_id,
        "issue_id": issue_id,
        "doc_name": doc_name,
        "job": body.job,
        # "rows": body.rows,
        # "measures": body.measures,         # m4..m8
        # "summary": body.summary,
        # "summaryCheck": body.summaryCheck,
        "pm_date": body.pm_date,           # string YYYY-MM-DD (à¸•à¸²à¸¡à¸Ÿà¸£à¸­à¸™à¸•à¹Œ)
        "status": "draft",
        "photos_pre": {},                      # à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¹ƒà¸™ /photos
        "side": body.side,
        "inspector": body.inspector,
        "createdAt": datetime.now(timezone.utc),
        # "updatedAt": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id),
        "issue_id": issue_id,
        "doc_name": doc_name,
    }

class stationPMPostIn(BaseModel):
    report_id: str | None = None      # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
    station_id: str
    # issue_id: str | None = None
    # job: dict
    rows: dict
    # measures: dict
    summary: str
    # pm_date: str
    # doc_name: str | None = None
    summaryCheck: str | None = None
    # dust_filter: str | None = None
    side: Literal["post", "after"]

@app.post("/stationpmreport/submit")
async def stationpmreport_submit(body: stationPMPostIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_stationpmreport_collection_for(station_id)
    db = coll.database

    # pm_type = str(body.job.get("pm_type") or "ST").upper()
    # body.job["pm_type"] = pm_type

    url_coll = get_stationpmurl_coll_upload(station_id)

    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "station_id": station_id})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        update_fields = {
            # "job": body.job,
            "rows": body.rows,
            # "measures": body.measures,          # à¹ƒà¸Šà¹‰à¹€à¸›à¹‡à¸™à¸„à¹ˆà¸²à¸«à¸¥à¸±à¸‡ PM
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            # "pm_date": body.pm_date,
            # "inspector": inspector,
            # "dust_filter": body.dust_filter,
            # "doc_name": doc_name,
            "side": "post",                     # à¸•à¸­à¸™à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¸à¸±à¹ˆà¸‡ post à¹à¸¥à¹‰à¸§
            "updatedAt": datetime.now(timezone.utc),
        }
    
        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
            # "issue_id": issue_id,
            # "doc_name": doc_name,
        }
    # à¹€à¸à¹‡à¸šà¹€à¸­à¸à¸ªà¸²à¸£à¹€à¸›à¹‡à¸™ draft à¸à¹ˆà¸­à¸™
    doc = {
        "station_id": station_id,
        # "issue_id": issue_id,
        # "doc_name": doc_name,
        # "job": body.job,
        "rows": body.rows,
        # "measures": body.measures,         # m4..m8
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        # "pm_date": body.pm_date,           # string YYYY-MM-DD (à¸•à¸²à¸¡à¸Ÿà¸£à¸­à¸™à¸•à¹Œ)
        "status": "draft",
        "photos": {},                      # à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¹ƒà¸™ /photos
        # "inspector": body.inspector,
        # "createdAt": datetime.now(timezone.utc),
        "side": "post",
        "updatedAt": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {"ok": True, "report_id": str(res.inserted_id)}

@app.get("/stationpmreport/get")
async def stationpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    coll = get_stationpmreport_collection_for(station_id)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    return doc

@app.get("/stationpmreport/list")
async def ccbpmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_stationpmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side":1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # à¸œà¸¹à¸ URL PDF à¸£à¸²à¸¢à¸§à¸±à¸™à¸ˆà¸²à¸ MDBPMUrlDB (à¸–à¹‰à¸²à¸¡à¸µ)
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    url_by_day: Dict[str, str] = {}
    if pm_dates:
        ucoll = get_stationpmurl_coll_upload(station_id)
        ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "pm_date": it.get("pm_date"),
        "side":it.get("side"),
        "inspector": it.get("inspector"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

@app.post("/stationpmreport/{report_id}/pre/photos")
async def stationpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "r1" .. "r10"
    files: List[UploadFile] = File(...),
    # remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"r\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_stationpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: /uploads/mdbpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "stationpm" / station_id / report_id / "pre" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/stationpm/{station_id}/{report_id}/pre/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            # "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos_pre.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/stationpmreport/{report_id}/post/photos")
async def stationpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "r1" .. "r10"
    files: List[UploadFile] = File(...),
    remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"r\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_stationpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: /uploads/mdbpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "stationpm" / station_id / report_id / "post" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/stationpm/{station_id}/{report_id}/post/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/stationpmreport/{report_id}/finalize")
async def stationpmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_stationpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # (à¸­à¸­à¸›à¸Šà¸±à¸™) à¸•à¸£à¸§à¸ˆà¸„à¸§à¸²à¸¡à¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¸à¹ˆà¸­à¸™ finalize à¹„à¸”à¹‰à¸—à¸µà¹ˆà¸™à¸µà¹ˆ
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/stationpmurl/upload-files", status_code=201)
async def stationmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),            # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO -> à¸ˆà¸° normalize à¹€à¸›à¹‡à¸™ YYYY-MM-DD
    files: List[UploadFile] = File(...),    # à¸­à¸™à¸¸à¸à¸²à¸•à¹€à¸‰à¸žà¸²à¸° .pdf
    # current: UserClaims = Depends(get_current_user),
    issue_id: Optional[str] = Form(None),
    doc_name: Optional[str] = Form(None),
    inspector: Optional[str] = Form(None),
):
    coll = get_stationpmurl_coll_upload(station_id)
    pm_date = normalize_pm_date(reportDate)  # à¸„à¸·à¸™ YYYY-MM-DD

    pm_type = "ST"
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

    rep_coll = get_stationpmreport_collection_for(station_id)
    final_issue_id = None
    client_issue = (issue_id or "").strip()

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        # à¸•à¸£à¸§à¸ˆ uniqueness à¹ƒà¸™à¸—à¸±à¹‰à¸‡ 2 à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™
        url_exists = await coll.find_one({"issue_id": client_issue})
        rep_exists = await rep_coll.find_one({"issue_id": client_issue})
        unique = not (url_exists or rep_exists)

        if valid_fmt and unique:
            final_issue_id = client_issue

    if not final_issue_id:
        while True:
            candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
            url_exists = await coll.find_one({"issue_id": candidate})
            rep_exists = await rep_coll.find_one({"issue_id": candidate})
            if not url_exists and not rep_exists:
                final_issue_id = candidate
                break

    year_seq: int | None = None
    # à¸žà¸¢à¸²à¸¢à¸²à¸¡ reuse year_seq à¸ˆà¸²à¸ PMReportDB à¸–à¹‰à¸²à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§ (issue_id à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™)
    rep = await get_stationpmreport_collection_for(station_id).find_one(
        {"issue_id": final_issue_id},
        {"year_seq": 1, "pm_date": 1},
    )
    if rep and rep.get("year_seq") is not None:
        year_seq = int(rep["year_seq"])

    # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸„à¹ˆà¸² year_seq à¸ˆà¸²à¸ PMReport â†’ à¸­à¸­à¸à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸ pm_year_sequences
    if year_seq is None:
        year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

    year = d.year
    final_doc_name: str | None = None

    if doc_name:
        candidate = doc_name.strip()

        ok_format = candidate.startswith(f"{station_id}_")

        rep_coll = get_stationpmreport_collection_for(station_id)
        rep_exists = await rep_coll.find_one({"doc_name": candidate})
        url_exists = await coll.find_one({"doc_name": candidate})
        unique = not (rep_exists or url_exists)

        if ok_format and unique:
            final_doc_name = candidate

    if not final_doc_name:
        final_doc_name = f"{station_id}_{year_seq}/{year}"

    doc_name = final_doc_name

    # à¹€à¸à¹‡à¸šà¹„à¸§à¹‰à¸—à¸µà¹ˆ /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "stationpmurl" / station_id / pm_date
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls, metas = [], []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url = f"/uploads/stationpmurl/{station_id}/{pm_date}/{fname}"
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    inspector_clean = (inspector or "").strip() or None
    now = datetime.now(timezone.utc)
    res = await coll.insert_one({
        "station": station_id,
        "pm_date": pm_date,
        "issue_id": final_issue_id, 
        "inspector": inspector_clean,
        "doc_name": doc_name,
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    })
    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean}

@app.get("/stationpmurl/list")
async def stationpmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_stationpmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find(
        {},
        {"_id": 1, "issue_id": 1,"doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    items = []
    for it in items_raw:
        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""
        items.append({
            "id": str(it["_id"]),
            "pm_date": it.get("pm_date"),
            "issue_id": it.get("issue_id"),
            "inspector": it.get(("inspector")), 
            "doc_name": it.get("doc_name"),
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,
            "urls": urls,
        })

    return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}


#---------------------------------------------------------------------- CM Report
def get_cmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    coll = CMReportDB.get_collection(str(station_id))
    return coll

def get_cmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    coll = CMUrlDB.get_collection(str(station_id))
    return coll

@app.get("/cmreport/list")
async def cmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_cmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "cm_date": 1, "status": 1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)
    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # --- à¸”à¸¶à¸‡à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸²à¸ PMReportURL à¹‚à¸”à¸¢ map à¸”à¹‰à¸§à¸¢ pm_date (string) ---
    cm_dates = [it.get("cm_date") for it in items_raw if it.get("cm_date")]
    urls_coll = get_cmurl_coll_upload(station_id)
    url_by_day: dict[str, str] = {}

    if cm_dates:
        ucur = urls_coll.find({"cm_date": {"$in": cm_dates}}, {"cm_date": 1, "status": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("cm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "cm_date": it.get("cm_date"),
        "status": it.get("status"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("cm_date") or "", ""),
    } for it in items_raw]

    cm_date_arr = [it.get("cm_date") for it in items_raw if it.get("cm_date")]
    status_arr = [it.get("status") for it in items_raw if it.get("status")]
    return {"items": items, "cm_date": cm_date_arr, "status": status_arr, "page": page, "pageSize": pageSize, "total": total}

# à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸šà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¹€à¸‹à¸´à¸£à¹Œà¸Ÿà¹€à¸§à¸­à¸£à¹Œ
UPLOADS_ROOT = os.getenv("UPLOADS_ROOT", "./uploads")
os.makedirs(UPLOADS_ROOT, exist_ok=True)

# à¹€à¸ªà¸´à¸£à¹Œà¸Ÿà¹„à¸Ÿà¸¥à¹Œà¸„à¸·à¸™à¹ƒà¸«à¹‰ Frontend à¸œà¹ˆà¸²à¸™ /uploads/...
app.mount("/uploads", StaticFiles(directory=UPLOADS_ROOT, html=False), name="uploads")

# ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif"}
# MAX_FILE_MB = 10

def _safe_name(name: str) -> str:
    # à¸à¸±à¸™ path traversal à¹à¸¥à¸°à¸­à¸±à¸à¸‚à¸£à¸°à¹à¸›à¸¥à¸ à¹†
    base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
    return base[:120] or secrets.token_hex(4)

def _ext(fname: str) -> str:
    return (fname.rsplit(".",1)[-1].lower() if "." in fname else "")

@app.post("/cmreport/{report_id}/photos")
async def cmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # à¹€à¸Šà¹ˆà¸™ "g1" .. "g10"
    files: list[UploadFile] = File(...),
    remark: str | None = Form(None),
    current: UserClaims = Depends(get_current_user),
):
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"g\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_cmreport_collection_for(station_id)
    from bson import ObjectId
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # à¸¢à¸·à¸™à¸¢à¸±à¸™à¸§à¹ˆà¸²à¸£à¸²à¸¢à¸‡à¸²à¸™à¸™à¸µà¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™ station à¸™à¸µà¹‰
    doc = await coll.find_one({"_id": oid}, {"_id":1, "station_id":1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cm" / station_id / report_id / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    total = 0
    for f in files:
        ext = _ext(f.filename or "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        total += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        # URL à¸ªà¸³à¸«à¸£à¸±à¸šà¹à¸ªà¸”à¸‡à¸šà¸™ Frontend
        url_path = f"/uploads/cm/{station_id}/{report_id}/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    # à¸­à¸±à¸›à¹€à¸”à¸•à¹€à¸­à¸à¸ªà¸²à¸£ PMReport: push à¸¥à¸‡ photos.<group>
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos.{group}": {"$each": saved}}}
    )

    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/cmreport/{report_id}/finalize")
async def cmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    current: UserClaims = Depends(get_current_user),
):
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_cmreport_collection_for(station_id)
    from bson import ObjectId
    oid = ObjectId(report_id)
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/cmurl/upload-files", status_code=201)
async def cmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),                 # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO
    files: list[UploadFile] = File(...),
    status: str = Form(...),  
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # à¸•à¸£à¸§à¸ˆ/à¹€à¸•à¸£à¸µà¸¢à¸¡à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™
    coll = get_cmurl_coll_upload(station_id)

    # parse à¸§à¸±à¸™à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ UTC datetime (à¸¡à¸µà¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§)
    cm_date = normalize_pm_date(reportDate)

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡: /uploads/pmurl/<station_id>/<YYYY-MM-DD>/
    # subdir = report_dt_utc.astimezone(th_tz).date().isoformat()
    subdir = cm_date
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cmurl" / station_id / subdir
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls = []
    metas = []
    total_size = 0

    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if "." in f.filename else "")
        if ext not in ALLOWED_EXTS or ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        total_size += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        safe = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        dest = dest_dir / safe
        with open(dest, "wb") as out:
            out.write(data)

        url = f"/uploads/cmurl/{station_id}/{subdir}/{safe}"   # â† à¸ˆà¸°à¹€à¸ªà¸´à¸£à¹Œà¸Ÿà¹„à¸”à¹‰à¸ˆà¸²à¸ StaticFiles à¸—à¸µà¹ˆ mount à¹„à¸§à¹‰à¹à¸¥à¹‰à¸§
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    now = datetime.now(timezone.utc)

    doc = {
        "station": station_id,
        "cm_date": cm_date,
        "status": (status or "").strip(), 
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    }
    res = await coll.insert_one(doc)

    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls}

@app.get("/cmurl/list")
async def cmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    status: str | None = Query(None),
):
    coll = get_cmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    # --- à¸ªà¸£à¹‰à¸²à¸‡ filter à¸•à¸²à¸¡à¸ªà¸–à¸²à¸™à¸° (optional à¹à¸•à¹ˆà¹à¸™à¸°à¸™à¸³) ---
    mongo_filter: dict = {}
    if status:
        want = (status or "").strip()
        mongo_filter["$or"] = [
            {"status": {"$regex": f"^{re.escape(want)}$", "$options": "i"}},
            {"job.status": {"$regex": f"^{re.escape(want)}$", "$options": "i"}},
        ]

    # --- à¸‚à¸­à¸Ÿà¸´à¸¥à¸”à¹Œ status à¸¡à¸²à¸”à¹‰à¸§à¸¢ ---
    projection = {
        "_id": 1, "cm_date": 1, "reportDate": 1,
        "urls": 1, "createdAt": 1,
        "status": 1, "job": 1,   # ðŸ‘ˆ à¹€à¸žà¸´à¹ˆà¸¡
    }

    cursor = (
        coll.find(mongo_filter, projection)
            .sort([("createdAt", -1), ("_id", -1)])
            .skip(skip)
            .limit(pageSize)
    )

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents(mongo_filter)

    def _cm_date_from(doc: dict) -> str | None:
        s = doc.get("cm_date")
        if isinstance(s, str) and re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
            return s
        rd = doc.get("reportDate")
        if isinstance(rd, datetime):
            return rd.astimezone(th_tz).date().isoformat()
        if isinstance(rd, str):
            try:
                dt = datetime.fromisoformat(rd.replace("Z", "+00:00"))
            except Exception:
                try:
                    dt = datetime.fromisoformat(rd).replace(tzinfo=th_tz)
                except Exception:
                    return None
            return dt.astimezone(th_tz).date().isoformat()
        return None

    items = []
    cm_date_arr = []

    for it in items_raw:
        cm_date_str = _cm_date_from(it)
        if cm_date_str:
            cm_date_arr.append(cm_date_str)

        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""

        items.append({
            "id": str(it["_id"]),
            "cm_date": cm_date_str,
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "status": (it.get("status") or (it.get("job") or {}).get("status") or ""),  # ðŸ‘ˆ à¸”à¸¶à¸‡à¸•à¸£à¸‡à¹†
            "file_url": first_url,
            "urls": urls,
        })

    return {
        "items": items,
        "cm_date": [d for d in cm_date_arr if d],
        # à¸ˆà¸° echo à¸„à¹ˆà¸² query à¸à¸¥à¸±à¸šà¸”à¹‰à¸§à¸¢à¸à¹‡à¹„à¸”à¹‰ à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™:
        # "status": (status or "").strip(),
        "page": page,
        "pageSize": pageSize,
        "total": total,
    }


class CMSubmitIn(BaseModel):
    station_id: str
    job: Dict[str, Any]          # à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸•à¸²à¸¡à¸Ÿà¸­à¸£à¹Œà¸¡ (issue_id, found_date, ... )
    summary: str = ""            # à¸ªà¸£à¸¸à¸›/à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸à¹à¸šà¸šà¸¢à¸²à¸§ (à¹à¸¥à¹‰à¸§à¹à¸•à¹ˆà¸ˆà¸°à¹ƒà¸Šà¹‰)
    cm_date: Optional[str] = None  # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO; à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡à¸¡à¸²à¸ˆà¸° fallback à¹€à¸›à¹‡à¸™ job.found_date

async def _ensure_cm_indexes(coll):
    try:
        await coll.create_index([("createdAt", -1), ("_id", -1)])
        # à¸–à¹‰à¸²à¸­à¸¢à¸²à¸à¸à¸±à¸™à¸‹à¹‰à¸³à¹€à¸¥à¸‚à¹ƒà¸šà¸‡à¸²à¸™à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸ªà¸–à¸²à¸™à¸µ: à¹€à¸›à¸´à¸” unique issue_id à¸à¹‡à¹„à¸”à¹‰ (à¸–à¹‰à¸²à¹à¸™à¹ˆà¹ƒà¸ˆà¸§à¹ˆà¸² unique)
        # await coll.create_index("issue_id", unique=True, sparse=True)
    except Exception:
        pass

@app.post("/cmreport/submit")
async def cmreport_submit(body: CMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    # Auth: admin à¸œà¹ˆà¸²à¸™à¸«à¸¡à¸”, à¸„à¸™à¸—à¸±à¹ˆà¸§à¹„à¸›à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œà¹ƒà¸™ station à¸™à¸µà¹‰
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_cmreport_collection_for(station_id)
    await _ensure_cm_indexes(coll)

    # à¸à¸³à¸«à¸™à¸” cm_date (string 'YYYY-MM-DD') à¹ƒà¸«à¹‰à¸ªà¸­à¸”à¸„à¸¥à¹‰à¸­à¸‡ /cmreport/list
    # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡à¸¡à¸² â†’ à¹ƒà¸Šà¹‰ job.found_date â†’ à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸­à¸µà¸ â†’ à¹ƒà¸Šà¹‰à¸§à¸±à¸™à¸™à¸µà¹‰ (à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢)
    cm_date_src = body.cm_date or body.job.get("found_date")
    if cm_date_src:
        cm_date = normalize_pm_date(cm_date_src)   # à¸„à¸·à¸™ "YYYY-MM-DD"
    else:
        cm_date = datetime.now(th_tz).date().isoformat()

    doc = {
        "station_id": station_id,
        "cm_date": cm_date,
        "job": body.job,              # à¹€à¸à¹‡à¸šà¸Ÿà¸­à¸£à¹Œà¸¡à¸—à¸±à¹‰à¸‡à¸à¹‰à¸­à¸™ (issue_id, severity, etc.)
        "summary": body.summary,
        "issue_id": body.job.get("issue_id"),
        "status": body.job.get("status", "Open"),      # à¹€à¸œà¸·à¹ˆà¸­à¸­à¸¢à¸²à¸ query
        "createdAt": datetime.now(timezone.utc),
        "updatedAt": datetime.now(timezone.utc),
        "photos": {},                 # à¸£à¸¹à¸›à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¸ à¸²à¸¢à¸«à¸¥à¸±à¸‡à¸—à¸µà¹ˆ /cmreport/{report_id}/photos
    }

    res = await coll.insert_one(doc)
    return {"ok": True, "report_id": str(res.inserted_id)}


@app.get("/cmreport/{report_id}")
async def cmreport_detail_path(
    report_id: str,
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_cmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid, "station_id": station_id})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")

    return {
        "id": str(doc["_id"]),
        "station_id": doc.get("station_id"),
        "cm_date": doc.get("cm_date"),
        "issue_id": doc.get("issue_id"),
        "status": doc.get("status"),
        "summary": doc.get("summary", ""),
        "job": doc.get("job", {}),
        "photos": doc.get("photos", {}),
        "createdAt": _ensure_utc_iso(doc.get("createdAt")),
        "updatedAt": _ensure_utc_iso(doc.get("updatedAt")),
    }

@app.get("/cmreport/detail")
async def cmreport_detail_query(
    id: str = Query(..., alias="id"),
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    return await cmreport_detail_path(id, station_id, current)  # reuse logic

class CMStatusUpdateIn(BaseModel):
    station_id: str
    status: Literal["Open", "In Progress", "Closed"]
    job: Optional[Dict[str, Any]] = None
    summary: Optional[str] = None
    cm_date: Optional[str] = None  # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO

ALLOWED_STATUS: set[str] = {"Open", "In Progress", "Closed"}

@app.patch("/cmreport/{report_id}/status")
async def cmreport_update_status(
    report_id: str,
    body: CMStatusUpdateIn,
    current: UserClaims = Depends(get_current_user),
):
    station_id = body.station_id.strip()
    if body.status not in ALLOWED_STATUS:
        raise HTTPException(status_code=400, detail="Invalid status")

    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_cmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    updates: Dict[str, Any] = {
        "status": body.status,          # top-level
        "job.status": body.status,      # sync à¹ƒà¸™ job
    }

    if body.summary is not None:
        updates["summary"] = body.summary

    if body.cm_date is not None:
        updates["cm_date"] = normalize_pm_date(body.cm_date)

    if body.job is not None:
        # à¹€à¸¥à¸·à¸­à¸à¹€à¸‰à¸žà¸²à¸°à¸„à¸µà¸¢à¹Œà¸—à¸µà¹ˆà¸­à¸™à¸¸à¸à¸²à¸•à¸ˆà¸²à¸à¸Ÿà¸­à¸£à¹Œà¸¡
        allowed_job_keys = {
            "issue_id","found_date","location","wo","sn",
            "equipment_list","problem_details","problem_type","severity",
            "reported_by","assignee","initial_cause","corrective_actions",
            "resolved_date","repair_result","preventive_action","remarks"
        }
        # à¸–à¹‰à¸² job.status à¸–à¸¹à¸à¸ªà¹ˆà¸‡à¸¡à¸² à¹ƒà¸«à¹‰à¸•à¸£à¸§à¸ˆà¹à¸¥à¸° sync
        if "status" in body.job:
            js = body.job["status"]
            if js not in ALLOWED_STATUS:
                raise HTTPException(status_code=400, detail="Invalid job.status")
            updates["status"] = js
            updates["job.status"] = js

        for k, v in body.job.items():
            if k in allowed_job_keys:
                updates[f"job.{k}"] = v

        # optional: sync cm_date à¸ˆà¸²à¸ found_date
        if "found_date" in body.job and body.job.get("found_date"):
            try:
                updates.setdefault("cm_date", normalize_pm_date(body.job["found_date"]))
            except Exception:
                pass

    updates["updatedAt"] = datetime.now(timezone.utc)

    res = await coll.update_one({"_id": oid, "station_id": station_id}, {"$set": updates})
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")

    return {"ok": True, "status": updates["status"]}

#---------------------------------------------------------------------- Test Report (DC)
def get_dc_testreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    coll = DCTestReportDB.get_collection(str(station_id))
    return coll

def get_dcurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    coll = DCUrlDB.get_collection(str(station_id))
    return coll

@app.get("/dctestreport/list")
async def dctestreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_dc_testreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "inspection_date": 1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)
    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # --- à¸”à¸¶à¸‡à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸²à¸ PMReportURL à¹‚à¸”à¸¢ map à¸”à¹‰à¸§à¸¢ pm_date (string) ---
    dc_dates = [it.get("inspection_date") for it in items_raw if it.get("inspection_date")]
    urls_coll = get_dcurl_coll_upload(station_id)
    url_by_day: dict[str, str] = {}

    if dc_dates:
        ucur = urls_coll.find({"inspection_date": {"$in": dc_dates}}, {"inspection_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("inspection_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "inspection_date": it.get("inspection_date"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("inspection_date") or "", ""),
    } for it in items_raw]

    dc_date_arr = [it.get("inspection_date") for it in items_raw if it.get("inspection_date")]
    # status_arr = [it.get("status") for it in items_raw if it.get("status")]
    return {"items": items, "inspection_date": dc_date_arr, "page": page, "pageSize": pageSize, "total": total}

# à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸šà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¹€à¸‹à¸´à¸£à¹Œà¸Ÿà¹€à¸§à¸­à¸£à¹Œ
UPLOADS_ROOT = os.getenv("UPLOADS_ROOT", "./uploads")
os.makedirs(UPLOADS_ROOT, exist_ok=True)

# à¹€à¸ªà¸´à¸£à¹Œà¸Ÿà¹„à¸Ÿà¸¥à¹Œà¸„à¸·à¸™à¹ƒà¸«à¹‰ Frontend à¸œà¹ˆà¸²à¸™ /uploads/...
app.mount("/uploads", StaticFiles(directory=UPLOADS_ROOT, html=False), name="uploads")

# ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif"}
# MAX_FILE_MB = 10

def _safe_name(name: str) -> str:
    # à¸à¸±à¸™ path traversal à¹à¸¥à¸°à¸­à¸±à¸à¸‚à¸£à¸°à¹à¸›à¸¥à¸ à¹†
    base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
    return base[:120] or secrets.token_hex(4)

def _ext(fname: str) -> str:
    return (fname.rsplit(".",1)[-1].lower() if "." in fname else "")

# ---- config à¹„à¸Ÿà¸¥à¹Œ/à¸­à¸±à¸›à¹‚à¸«à¸¥à¸” (à¸–à¹‰à¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸«à¹‰à¸§à¸²à¸‡à¹„à¸§à¹‰à¸”à¹‰à¸²à¸™à¸šà¸™) ----
ALLOWED_IMAGE_EXTS = {"jpg", "jpeg", "png", "webp", "gif"}
MAX_IMAGE_FILE_MB = 10

PHOTO_GROUP_KEYS = [
    "nameplate",       # index 0
    "charger",         # index 1
    "circuit_breaker", # index 2
    "rcd",             # index 3
    "gun1",            # index 4
    "gun2",            # index 5
]

def _key_for_index(i: int) -> str:
    return PHOTO_GROUP_KEYS[i] if 0 <= i < len(PHOTO_GROUP_KEYS) else f"extra{i-5}"

@app.post("/dctestreport/{report_id}/photos")
async def dc_testreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    item_index: int = Form(...),               # <<-- à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸ˆà¸²à¸ group â†’ index
    files: list[UploadFile] = File(...),
    remark: str | None = Form(None),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # à¸£à¸²à¸¢à¸‡à¸²à¸™à¸•à¹‰à¸­à¸‡à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸ªà¸–à¸²à¸™à¸µà¸™à¸µà¹‰
    coll = get_dc_testreport_collection_for(station_id)
    from bson import ObjectId
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹à¸›à¸¥à¸‡ index â†’ à¸Šà¸·à¹ˆà¸­à¸„à¸µà¸¢à¹Œà¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ/à¸„à¸µà¸¢à¹Œà¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£
    key = _key_for_index(item_index)  # e.g. nameplate/charger/.../extra1

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "dctest" / station_id / report_id / key
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".", 1)[-1].lower() if "." in (f.filename or "") else "")
        if ext not in ALLOWED_IMAGE_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_IMAGE_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_IMAGE_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/dctest/{station_id}/{report_id}/{key}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc),
            "index": item_index,          # à¹€à¸à¹‡à¸š index à¹€à¸œà¸·à¹ˆà¸­à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¸à¸¥à¸±à¸š
        })

    # à¸­à¸±à¸›à¹€à¸”à¸•à¹€à¸­à¸à¸ªà¸²à¸£à¸£à¸²à¸¢à¸‡à¸²à¸™: push à¸¥à¸‡ photos.<key>
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos.{key}": {"$each": saved}}, "$set": {"updatedAt": datetime.now(timezone.utc)}}
    )

    return {"ok": True, "count": len(saved), "key": key, "files": saved}

@app.post("/dctestreport/{report_id}/finalize")
async def dc_testreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    current: UserClaims = Depends(get_current_user),
):
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_dc_testreport_collection_for(station_id)
    from bson import ObjectId
    oid = ObjectId(report_id)
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/dcurl/upload-files", status_code=201)
async def dcurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),                 # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO
    files: list[UploadFile] = File(...),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # à¸•à¸£à¸§à¸ˆ/à¹€à¸•à¸£à¸µà¸¢à¸¡à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™
    coll = get_dcurl_coll_upload(station_id)

    # parse à¸§à¸±à¸™à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ UTC datetime (à¸¡à¸µà¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§)
    dc_date = normalize_pm_date(reportDate)

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡: /uploads/pmurl/<station_id>/<YYYY-MM-DD>/
    # subdir = report_dt_utc.astimezone(th_tz).date().isoformat()
    subdir = dc_date
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "dcurl" / station_id / subdir
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls = []
    metas = []
    total_size = 0

    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if "." in f.filename else "")
        if ext not in ALLOWED_EXTS or ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        total_size += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        safe = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        dest = dest_dir / safe
        with open(dest, "wb") as out:
            out.write(data)

        url = f"/uploads/dcurl/{station_id}/{subdir}/{safe}"   # â† à¸ˆà¸°à¹€à¸ªà¸´à¸£à¹Œà¸Ÿà¹„à¸”à¹‰à¸ˆà¸²à¸ StaticFiles à¸—à¸µà¹ˆ mount à¹„à¸§à¹‰à¹à¸¥à¹‰à¸§
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    now = datetime.now(timezone.utc)
    doc = {
        "station": station_id,
        "dc_date": dc_date,   
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    }
    res = await coll.insert_one(doc)

    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls}

@app.get("/dcurl/list")
async def dcurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    """
    à¸”à¸¶à¸‡à¸£à¸²à¸¢à¸à¸²à¸£à¹„à¸Ÿà¸¥à¹Œ PM (PDF) à¸—à¸µà¹ˆà¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸§à¹‰à¸•à¹ˆà¸­à¸ªà¸–à¸²à¸™à¸µ à¸ˆà¸²à¸ PMUrlDB/<station_id>
    - à¸£à¸­à¸‡à¸£à¸±à¸šà¸—à¸±à¹‰à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¹€à¸à¹‡à¸š pm_date (string 'YYYY-MM-DD') à¹à¸¥à¸° reportDate (Date/ISO)
    - à¹€à¸£à¸µà¸¢à¸‡à¸ˆà¸²à¸à¹ƒà¸«à¸¡à¹ˆà¹„à¸›à¹€à¸à¹ˆà¸² (createdAt desc, _id desc)
    - à¸£à¸¹à¸›à¹à¸šà¸šà¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸·à¸­à¸™ /pmreport/list (à¸¡à¸µ file_url à¸ªà¸³à¸«à¸£à¸±à¸šà¸¥à¸´à¸‡à¸à¹Œà¸•à¸±à¸§à¹à¸£à¸)
    """
    coll = get_dcurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    # à¸”à¸¶à¸‡à¹€à¸‰à¸žà¸²à¸°à¸Ÿà¸´à¸¥à¸”à¹Œà¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™
    cursor = coll.find(
        {},
        {"_id": 1, "dc_date": 1, "reportDate": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    def _dc_date_from(doc: dict) -> str | None:
        """
        à¹à¸›à¸¥à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆà¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£à¹ƒà¸«à¹‰à¹„à¸”à¹‰ string 'YYYY-MM-DD'
        - à¸–à¹‰à¸²à¸¡à¸µ pm_date (string) â†’ à¸„à¸·à¸™à¸„à¹ˆà¸²à¸™à¸±à¹‰à¸™
        - à¸–à¹‰à¸²à¸¡à¸µ reportDate (datetime/string) â†’ à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™à¸§à¸±à¸™à¹„à¸—à¸¢ à¹à¸¥à¹‰à¸§ .date().isoformat()
        """
        # à¸£à¸¸à¹ˆà¸™à¹ƒà¸«à¸¡à¹ˆ: à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ pm_date (string)
        s = doc.get("dc_date")
        if isinstance(s, str) and re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
            return s

        # à¸£à¸¸à¹ˆà¸™à¹€à¸à¹ˆà¸²: à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ reportDate (Date/ISO)
        rd = doc.get("reportDate")
        if isinstance(rd, datetime):
            return rd.astimezone(th_tz).date().isoformat()
        if isinstance(rd, str):
            try:
                dt = datetime.fromisoformat(rd.replace("Z", "+00:00"))
            except Exception:
                # à¹€à¸œà¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸² â†’ à¸–à¸·à¸­à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢
                try:
                    dt = datetime.fromisoformat(rd).replace(tzinfo=th_tz)
                except Exception:
                    return None
            return dt.astimezone(th_tz).date().isoformat()

        return None

    items = []
    dc_date_arr = []

    for it in items_raw:
        dc_date_str = _dc_date_from(it)
        if dc_date_str:
            dc_date_arr.append(dc_date_str)

        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""

        items.append({
            "id": str(it["_id"]),
            "dc_date": dc_date_str,                         # 'YYYY-MM-DD' | None
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,                          # à¹„à¸Ÿà¸¥à¹Œà¹à¸£à¸ (à¹„à¸§à¹‰à¹ƒà¸«à¹‰à¸›à¸¸à¹ˆà¸¡à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”)
            "urls": urls,                                   # à¹€à¸œà¸·à¹ˆà¸­à¸Ÿà¸£à¸­à¸™à¸•à¹Œà¸­à¸¢à¸²à¸à¹à¸ªà¸”à¸‡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
        })

    return {
        "items": items,
        "dc_date": [d for d in dc_date_arr if d],          # à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸·à¸­à¸™ /pmreport/list
        "page": page,
        "pageSize": pageSize,
        "total": total,
    }

class EquipmentBlock(BaseModel):
    manufacturers: List[str] = []
    models: List[str] = []
    serialNumbers: List[str] = []

SymbolLiteral = Literal["", "pass", "notPass", "notTest"]
PhaseLiteral  = Literal["", "L1L2L3", "L3L2L1"]

class PersonSig(BaseModel):
    name: str = ""
    signature: str = ""   # à¹€à¸à¹‡à¸š path/à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸²à¸¢à¹€à¸‹à¹‡à¸™ (à¸«à¸£à¸·à¸­à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡)
    date: str = ""        # "YYYY-MM-DD"
    company: str = ""

class ResponsibilityBlock(BaseModel):
    performed: PersonSig = PersonSig()
    approved:  PersonSig = PersonSig()
    witnessed: PersonSig = PersonSig()

class SignatureBlock(BaseModel):
    responsibility: ResponsibilityBlock = ResponsibilityBlock()

class DCSubmitIn(BaseModel):
    station_id: str
    issue_id: Optional[str] = None 
    job: Dict[str, Any]          # à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸•à¸²à¸¡à¸Ÿà¸­à¸£à¹Œà¸¡ (issue_id, found_date, ... )
    head: Dict[str,Any]
    inspection_date: Optional[str] = None  # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO; à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡à¸¡à¸²à¸ˆà¸° fallback à¹€à¸›à¹‡à¸™ job.found_date
    equipment: Optional[EquipmentBlock] = None
    electrical_safety: Dict[str, Any] = Field(default_factory=dict)
    charger_safety: Dict[str, Any] = Field(default_factory=dict)
    remarks: Dict[str, Any] = Field(default_factory=dict)
    symbol: Optional[SymbolLiteral] = None
    phaseSequence: Optional[PhaseLiteral] = None
    signature: Optional[SignatureBlock] = None 

async def _ensure_dc_indexes(coll):
    try:
        await coll.create_index([("createdAt", -1), ("_id", -1)])
        # à¸–à¹‰à¸²à¸­à¸¢à¸²à¸à¸à¸±à¸™à¸‹à¹‰à¸³à¹€à¸¥à¸‚à¹ƒà¸šà¸‡à¸²à¸™à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸ªà¸–à¸²à¸™à¸µ: à¹€à¸›à¸´à¸” unique issue_id à¸à¹‡à¹„à¸”à¹‰ (à¸–à¹‰à¸²à¹à¸™à¹ˆà¹ƒà¸ˆà¸§à¹ˆà¸² unique)
        # await coll.create_index("issue_id", unique=True, sparse=True)
    except Exception:
        pass

def _normalize_tick_to_pass(obj):
    if isinstance(obj, dict):
        return {k: _normalize_tick_to_pass(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_normalize_tick_to_pass(v) for v in obj]
    if isinstance(obj, str):
        return "pass" if obj == "âœ“" else obj
    return obj

@app.post("/dcreport/submit")
async def dcreport_submit(body: DCSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    # Auth: admin à¸œà¹ˆà¸²à¸™à¸«à¸¡à¸”, à¸„à¸™à¸—à¸±à¹ˆà¸§à¹„à¸›à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œà¹ƒà¸™ station à¸™à¸µà¹‰
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_dc_testreport_collection_for(station_id)
    await _ensure_dc_indexes(coll)

    # à¸à¸³à¸«à¸™à¸” cm_date (string 'YYYY-MM-DD') à¹ƒà¸«à¹‰à¸ªà¸­à¸”à¸„à¸¥à¹‰à¸­à¸‡ /cmreport/list
    # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡à¸¡à¸² â†’ à¹ƒà¸Šà¹‰ job.found_date â†’ à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸­à¸µà¸ â†’ à¹ƒà¸Šà¹‰à¸§à¸±à¸™à¸™à¸µà¹‰ (à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢)
    dc_date_src = body.inspection_date or body.head.get("inspection_date")
    if dc_date_src:
        dc_date = normalize_pm_date(dc_date_src)   # à¸„à¸·à¸™ "YYYY-MM-DD"
    else:
        dc_date = datetime.now(th_tz).date().isoformat()

    issue_id = (body.head or {}).get("issue_id")  or (body.job or {}).get("issue_id") 

    electrical_safety = _normalize_tick_to_pass(body.electrical_safety or {})

    charger_safety = _normalize_tick_to_pass(body.charger_safety or {})
    doc = {
        "station_id": station_id,
        "issue_id": issue_id,
        "inspection_date": dc_date,
        "head": body.head,
        "equipment": body.equipment.dict() if body.equipment else {"manufacturers": [], "models": [], "serialNumbers": []},
        "electrical_safety": electrical_safety,
        "charger_safety": charger_safety,
        "remarks": body.remarks or {},
        "symbol": body.symbol,
        "phaseSequence": body.phaseSequence,
        "signature": body.signature.dict() if body.signature else None,
        "createdAt": datetime.now(timezone.utc),
        "updatedAt": datetime.now(timezone.utc),
        "photos": {},                 # à¸£à¸¹à¸›à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¸ à¸²à¸¢à¸«à¸¥à¸±à¸‡à¸—à¸µà¹ˆ /cmreport/{report_id}/photos
    }

    res = await coll.insert_one(doc)
    return {"ok": True, "report_id": str(res.inserted_id)}

#---------------------------------------------------------------------- Test Report (AC)
def get_ac_testreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    coll = ACTestReportDB.get_collection(str(station_id))
    return coll

def get_acurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    coll = ACUrlDB.get_collection(str(station_id))
    return coll

@app.get("/actestreport/list")
async def actestreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_ac_testreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "inspection_date": 1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)
    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # --- à¸”à¸¶à¸‡à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸²à¸ PMReportURL à¹‚à¸”à¸¢ map à¸”à¹‰à¸§à¸¢ pm_date (string) ---
    ac_dates = [it.get("inspection_date") for it in items_raw if it.get("inspection_date")]
    urls_coll = get_acurl_coll_upload(station_id)
    url_by_day: dict[str, str] = {}

    if ac_dates:
        ucur = urls_coll.find({"inspection_date": {"$in": ac_dates}}, {"inspection_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("inspection_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "inspection_date": it.get("inspection_date"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("inspection_date") or "", ""),
    } for it in items_raw]

    ac_date_arr = [it.get("inspection_date") for it in items_raw if it.get("inspection_date")]
    # status_arr = [it.get("status") for it in items_raw if it.get("status")]
    return {"items": items, "inspection_date": ac_date_arr, "page": page, "pageSize": pageSize, "total": total}

# à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸šà¸™à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¹€à¸‹à¸´à¸£à¹Œà¸Ÿà¹€à¸§à¸­à¸£à¹Œ
UPLOADS_ROOT = os.getenv("UPLOADS_ROOT", "./uploads")
os.makedirs(UPLOADS_ROOT, exist_ok=True)

# à¹€à¸ªà¸´à¸£à¹Œà¸Ÿà¹„à¸Ÿà¸¥à¹Œà¸„à¸·à¸™à¹ƒà¸«à¹‰ Frontend à¸œà¹ˆà¸²à¸™ /uploads/...
app.mount("/uploads", StaticFiles(directory=UPLOADS_ROOT, html=False), name="uploads")

# ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif"}
# MAX_FILE_MB = 10

def _safe_name(name: str) -> str:
    # à¸à¸±à¸™ path traversal à¹à¸¥à¸°à¸­à¸±à¸à¸‚à¸£à¸°à¹à¸›à¸¥à¸ à¹†
    base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
    return base[:120] or secrets.token_hex(4)

def _ext(fname: str) -> str:
    return (fname.rsplit(".",1)[-1].lower() if "." in fname else "")

# ---- config à¹„à¸Ÿà¸¥à¹Œ/à¸­à¸±à¸›à¹‚à¸«à¸¥à¸” (à¸–à¹‰à¸²à¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸«à¹‰à¸§à¸²à¸‡à¹„à¸§à¹‰à¸”à¹‰à¸²à¸™à¸šà¸™) ----
ALLOWED_IMAGE_EXTS = {"jpg", "jpeg", "png", "webp", "gif"}
MAX_IMAGE_FILE_MB = 10

PHOTO_GROUP_KEYS = [
    "nameplate",       # index 0
    "charger",         # index 1
    "circuit_breaker", # index 2
    "rcd",             # index 3
    "gun1",            # index 4
    "gun2",            # index 5
]

def _key_for_index(i: int) -> str:
    return PHOTO_GROUP_KEYS[i] if 0 <= i < len(PHOTO_GROUP_KEYS) else f"extra{i-5}"

@app.post("/actestreport/{report_id}/photos")
async def ac_testreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    item_index: int = Form(...),               # <<-- à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸ˆà¸²à¸ group â†’ index
    files: list[UploadFile] = File(...),
    remark: str | None = Form(None),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # à¸£à¸²à¸¢à¸‡à¸²à¸™à¸•à¹‰à¸­à¸‡à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸ªà¸–à¸²à¸™à¸µà¸™à¸µà¹‰
    coll = get_ac_testreport_collection_for(station_id)
    from bson import ObjectId
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # à¹à¸›à¸¥à¸‡ index â†’ à¸Šà¸·à¹ˆà¸­à¸„à¸µà¸¢à¹Œà¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ/à¸„à¸µà¸¢à¹Œà¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£
    key = _key_for_index(item_index)  # e.g. nameplate/charger/.../extra1

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "actest" / station_id / report_id / key
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".", 1)[-1].lower() if "." in (f.filename or "") else "")
        if ext not in ALLOWED_IMAGE_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_IMAGE_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_IMAGE_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/actest/{station_id}/{report_id}/{key}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc),
            "index": item_index,          # à¹€à¸à¹‡à¸š index à¹€à¸œà¸·à¹ˆà¸­à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¸à¸¥à¸±à¸š
        })

    # à¸­à¸±à¸›à¹€à¸”à¸•à¹€à¸­à¸à¸ªà¸²à¸£à¸£à¸²à¸¢à¸‡à¸²à¸™: push à¸¥à¸‡ photos.<key>
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos.{key}": {"$each": saved}}, "$set": {"updatedAt": datetime.now(timezone.utc)}}
    )

    return {"ok": True, "count": len(saved), "key": key, "files": saved}


@app.post("/actestreport/{report_id}/finalize")
async def ac_testreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    current: UserClaims = Depends(get_current_user),
):
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_ac_testreport_collection_for(station_id)
    from bson import ObjectId
    oid = ObjectId(report_id)
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/acurl/upload-files", status_code=201)
async def acurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),                 # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO
    files: list[UploadFile] = File(...),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # à¸•à¸£à¸§à¸ˆ/à¹€à¸•à¸£à¸µà¸¢à¸¡à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™
    coll = get_acurl_coll_upload(station_id)

    # parse à¸§à¸±à¸™à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ UTC datetime (à¸¡à¸µà¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§)
    ac_date = normalize_pm_date(reportDate)

    # à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸›à¸¥à¸²à¸¢à¸—à¸²à¸‡: /uploads/pmurl/<station_id>/<YYYY-MM-DD>/
    # subdir = report_dt_utc.astimezone(th_tz).date().isoformat()
    subdir = ac_date
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "acurl" / station_id / subdir
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls = []
    metas = []
    total_size = 0

    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if "." in f.filename else "")
        if ext not in ALLOWED_EXTS or ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        total_size += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        safe = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        dest = dest_dir / safe
        with open(dest, "wb") as out:
            out.write(data)

        url = f"/uploads/acurl/{station_id}/{subdir}/{safe}"   # â† à¸ˆà¸°à¹€à¸ªà¸´à¸£à¹Œà¸Ÿà¹„à¸”à¹‰à¸ˆà¸²à¸ StaticFiles à¸—à¸µà¹ˆ mount à¹„à¸§à¹‰à¹à¸¥à¹‰à¸§
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    now = datetime.now(timezone.utc)
    doc = {
        "station": station_id,
        "ac_date": ac_date,   
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    }
    res = await coll.insert_one(doc)

    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls}

@app.get("/acurl/list")
async def acurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    """
    à¸”à¸¶à¸‡à¸£à¸²à¸¢à¸à¸²à¸£à¹„à¸Ÿà¸¥à¹Œ PM (PDF) à¸—à¸µà¹ˆà¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸§à¹‰à¸•à¹ˆà¸­à¸ªà¸–à¸²à¸™à¸µ à¸ˆà¸²à¸ PMUrlDB/<station_id>
    - à¸£à¸­à¸‡à¸£à¸±à¸šà¸—à¸±à¹‰à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¹€à¸à¹‡à¸š pm_date (string 'YYYY-MM-DD') à¹à¸¥à¸° reportDate (Date/ISO)
    - à¹€à¸£à¸µà¸¢à¸‡à¸ˆà¸²à¸à¹ƒà¸«à¸¡à¹ˆà¹„à¸›à¹€à¸à¹ˆà¸² (createdAt desc, _id desc)
    - à¸£à¸¹à¸›à¹à¸šà¸šà¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸·à¸­à¸™ /pmreport/list (à¸¡à¸µ file_url à¸ªà¸³à¸«à¸£à¸±à¸šà¸¥à¸´à¸‡à¸à¹Œà¸•à¸±à¸§à¹à¸£à¸)
    """
    coll = get_acurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    # à¸”à¸¶à¸‡à¹€à¸‰à¸žà¸²à¸°à¸Ÿà¸´à¸¥à¸”à¹Œà¸—à¸µà¹ˆà¸ˆà¸³à¹€à¸›à¹‡à¸™
    cursor = coll.find(
        {},
        {"_id": 1, "ac_date": 1, "reportDate": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    def _ac_date_from(doc: dict) -> str | None:
        """
        à¹à¸›à¸¥à¸‡à¸§à¸±à¸™à¸—à¸µà¹ˆà¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£à¹ƒà¸«à¹‰à¹„à¸”à¹‰ string 'YYYY-MM-DD'
        - à¸–à¹‰à¸²à¸¡à¸µ pm_date (string) â†’ à¸„à¸·à¸™à¸„à¹ˆà¸²à¸™à¸±à¹‰à¸™
        - à¸–à¹‰à¸²à¸¡à¸µ reportDate (datetime/string) â†’ à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™à¸§à¸±à¸™à¹„à¸—à¸¢ à¹à¸¥à¹‰à¸§ .date().isoformat()
        """
        # à¸£à¸¸à¹ˆà¸™à¹ƒà¸«à¸¡à¹ˆ: à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ pm_date (string)
        s = doc.get("ac_date")
        if isinstance(s, str) and re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
            return s

        # à¸£à¸¸à¹ˆà¸™à¹€à¸à¹ˆà¸²: à¹€à¸à¹‡à¸šà¹€à¸›à¹‡à¸™ reportDate (Date/ISO)
        rd = doc.get("reportDate")
        if isinstance(rd, datetime):
            return rd.astimezone(th_tz).date().isoformat()
        if isinstance(rd, str):
            try:
                dt = datetime.fromisoformat(rd.replace("Z", "+00:00"))
            except Exception:
                # à¹€à¸œà¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸¡à¸µà¹‚à¸‹à¸™à¹€à¸§à¸¥à¸² â†’ à¸–à¸·à¸­à¹€à¸›à¹‡à¸™à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢
                try:
                    dt = datetime.fromisoformat(rd).replace(tzinfo=th_tz)
                except Exception:
                    return None
            return dt.astimezone(th_tz).date().isoformat()

        return None

    items = []
    ac_date_arr = []

    for it in items_raw:
        ac_date_str = _ac_date_from(it)
        if ac_date_str:
            ac_date_arr.append(ac_date_str)

        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""

        items.append({
            "id": str(it["_id"]),
            "ac_date": ac_date_str,                         # 'YYYY-MM-DD' | None
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,                          # à¹„à¸Ÿà¸¥à¹Œà¹à¸£à¸ (à¹„à¸§à¹‰à¹ƒà¸«à¹‰à¸›à¸¸à¹ˆà¸¡à¸”à¸²à¸§à¸™à¹Œà¹‚à¸«à¸¥à¸”)
            "urls": urls,                                   # à¹€à¸œà¸·à¹ˆà¸­à¸Ÿà¸£à¸­à¸™à¸•à¹Œà¸­à¸¢à¸²à¸à¹à¸ªà¸”à¸‡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
        })

    return {
        "items": items,
        "ac_date": [d for d in ac_date_arr if d],          # à¹ƒà¸«à¹‰à¹€à¸«à¸¡à¸·à¸­à¸™ /pmreport/list
        "page": page,
        "pageSize": pageSize,
        "total": total,
    }

class ACSubmitIn(BaseModel):
    station_id: str
    issue_id: Optional[str] = None 
    # job: Dict[str, Any]          # à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸•à¸²à¸¡à¸Ÿà¸­à¸£à¹Œà¸¡ (issue_id, found_date, ... )
    head: Dict[str,Any]
    inspection_date: Optional[str] = None  # "YYYY-MM-DD" à¸«à¸£à¸·à¸­ ISO; à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡à¸¡à¸²à¸ˆà¸° fallback à¹€à¸›à¹‡à¸™ job.found_date
    equipment: Optional[EquipmentBlock] = None
    electrical_safety: Dict[str, Any] = Field(default_factory=dict)
    charger_safety: Dict[str, Any] = Field(default_factory=dict)
    remarks: Dict[str, Any] = Field(default_factory=dict)
    symbol: Optional[SymbolLiteral] = None
    phaseSequence: Optional[PhaseLiteral] = None
    signature: Optional[SignatureBlock] = None 

async def _ensure_dc_indexes(coll):
    try:
        await coll.create_index([("createdAt", -1), ("_id", -1)])
        # à¸–à¹‰à¸²à¸­à¸¢à¸²à¸à¸à¸±à¸™à¸‹à¹‰à¸³à¹€à¸¥à¸‚à¹ƒà¸šà¸‡à¸²à¸™à¹ƒà¸™à¹à¸•à¹ˆà¸¥à¸°à¸ªà¸–à¸²à¸™à¸µ: à¹€à¸›à¸´à¸” unique issue_id à¸à¹‡à¹„à¸”à¹‰ (à¸–à¹‰à¸²à¹à¸™à¹ˆà¹ƒà¸ˆà¸§à¹ˆà¸² unique)
        # await coll.create_index("issue_id", unique=True, sparse=True)
    except Exception:
        pass

def _normalize_tick_to_pass(obj):
    if isinstance(obj, dict):
        return {k: _normalize_tick_to_pass(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_normalize_tick_to_pass(v) for v in obj]
    if isinstance(obj, str):
        return "pass" if obj == "âœ“" else obj
    return obj

@app.post("/acreport/submit")
async def acreport_submit(body: ACSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    # Auth: admin à¸œà¹ˆà¸²à¸™à¸«à¸¡à¸”, à¸„à¸™à¸—à¸±à¹ˆà¸§à¹„à¸›à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸ªà¸´à¸—à¸˜à¸´à¹Œà¹ƒà¸™ station à¸™à¸µà¹‰
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_ac_testreport_collection_for(station_id)
    await _ensure_dc_indexes(coll)

    # à¸à¸³à¸«à¸™à¸” cm_date (string 'YYYY-MM-DD') à¹ƒà¸«à¹‰à¸ªà¸­à¸”à¸„à¸¥à¹‰à¸­à¸‡ /cmreport/list
    # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡à¸¡à¸² â†’ à¹ƒà¸Šà¹‰ job.found_date â†’ à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸­à¸µà¸ â†’ à¹ƒà¸Šà¹‰à¸§à¸±à¸™à¸™à¸µà¹‰ (à¹€à¸§à¸¥à¸²à¹„à¸—à¸¢)
    ac_date_src = body.inspection_date or body.head.get("inspection_date")
    if ac_date_src:
        ac_date = normalize_pm_date(ac_date_src)   # à¸„à¸·à¸™ "YYYY-MM-DD"
    else:
        ac_date = datetime.now(th_tz).date().isoformat()

    issue_id = (body.head or {}).get("issue_id")  or (body.head or {}).get("issue_id") 

    electrical_safety = _normalize_tick_to_pass(body.electrical_safety or {})

    charger_safety = _normalize_tick_to_pass(body.charger_safety or {})
    doc = {
        "station_id": station_id,
        "issue_id": issue_id,
        "inspection_date": ac_date,
        "head": body.head,
        "equipment": body.equipment.dict() if body.equipment else {"manufacturers": [], "models": [], "serialNumbers": []},
        "electrical_safety": electrical_safety,
        "charger_safety": charger_safety,
        "remarks": body.remarks or {},
        "symbol": body.symbol,
        "phaseSequence": body.phaseSequence,
        "signature": body.signature.dict() if body.signature else None,
        "createdAt": datetime.now(timezone.utc),
        "updatedAt": datetime.now(timezone.utc),
        "photos": {},                 # à¸£à¸¹à¸›à¸ˆà¸°à¸–à¸¹à¸à¹€à¸•à¸´à¸¡à¸ à¸²à¸¢à¸«à¸¥à¸±à¸‡à¸—à¸µà¹ˆ /cmreport/{report_id}/photos
    }

    res = await coll.insert_one(doc)
    return {"ok": True, "report_id": str(res.inserted_id)}

# ----------------------------------------------------------------------- device page
def get_device_collection_for(station_id: str):
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return deviceDB.get_collection(str(station_id))

# (à¹€à¸¥à¸·à¸­à¸à¹„à¸”à¹‰) à¸ªà¸£à¹‰à¸²à¸‡à¸”à¸±à¸Šà¸™à¸µà¹à¸šà¸š lazy à¸•à¹ˆà¸­à¸ªà¸–à¸²à¸™à¸µà¸—à¸µà¹ˆà¸–à¸¹à¸à¹€à¸£à¸µà¸¢à¸à¹ƒà¸Šà¹‰
async def _ensure_util_index(coll):
    try:
        await coll.create_index([("timestamp", -1), ("_id", -1)])
    except Exception:
        pass

@app.get("/utilization/stream")
async def utilization_stream(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_device_collection_for(station_id)
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    async def event_generator():
        # à¸ªà¹ˆà¸‡ snapshot à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸à¹ˆà¸­à¸™
        latest = await coll.find_one({}, sort=[("timestamp", -1), ("_id", -1)])
        if latest:
            latest["_id"] = str(latest["_id"])
            latest["timestamp_utc"] = _ensure_utc_iso(latest.get("timestamp_utc"))
            yield f"event: init\ndata: {json.dumps(latest)}\n\n"

        # à¸•à¹ˆà¸­à¸”à¹‰à¸§à¸¢ change stream (à¸•à¹‰à¸­à¸‡à¹€à¸›à¹‡à¸™ replica set / Atlas tier à¸—à¸µà¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š)
        try:
            async with coll.watch(full_document='updateLookup') as stream:
                async for change in stream:
                    if await request.is_disconnected():
                        break
                    doc = change.get("fullDocument")
                    if not doc:
                        continue
                    doc["_id"] = str(doc["_id"])
                    doc["timestamp_utc"] = _ensure_utc_iso(doc.get("timestamp_utc"))
                    yield f"data: {json.dumps(doc)}\n\n"
        except Exception:
            # fallback: à¸–à¹‰à¸²à¹ƒà¸Šà¹‰à¹„à¸¡à¹ˆà¹„à¸”à¹‰ (à¹€à¸Šà¹ˆà¸™ standalone) à¹ƒà¸«à¹‰ polling
            last_id = latest.get("_id") if latest else None
            while not await request.is_disconnected():
                doc = await coll.find_one({}, sort=[("timestamp_utc", -1), ("_id", -1)])
                if doc and str(doc["_id"]) != str(last_id):
                    last_id = str(doc["_id"])
                    doc["_id"] = last_id
                    doc["timestamp_utc"] = _ensure_utc_iso(doc.get("timestamp_utc"))
                    yield f"data: {json.dumps(doc)}\n\n"
                else:
                    yield ": keep-alive\n\n"
                await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

#-------------------------------------------------------------------- setting page
def get_setting_collection_for(station_id: str):
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return settingDB.get_collection(str(station_id))

# (à¹€à¸¥à¸·à¸­à¸à¹„à¸”à¹‰) à¸ªà¸£à¹‰à¸²à¸‡à¸”à¸±à¸Šà¸™à¸µà¹à¸šà¸š lazy à¸•à¹ˆà¸­à¸ªà¸–à¸²à¸™à¸µà¸—à¸µà¹ˆà¸–à¸¹à¸à¹€à¸£à¸µà¸¢à¸à¹ƒà¸Šà¹‰
async def _ensure_util_index(coll):
    try:
        await coll.create_index([("timestamp", -1), ("_id", -1)])
    except Exception:
        pass

@app.get("/setting/stream")
async def setting_stream(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_setting_collection_for(station_id)
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    async def event_generator():
        # à¸ªà¹ˆà¸‡ snapshot à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸à¹ˆà¸­à¸™
        latest = await coll.find_one({}, sort=[("timestamp", -1), ("_id", -1)])
        if latest:
            latest["_id"] = str(latest["_id"])
            latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
            yield f"event: init\ndata: {json.dumps(latest)}\n\n"

        # à¸•à¹ˆà¸­à¸”à¹‰à¸§à¸¢ change stream (à¸•à¹‰à¸­à¸‡à¹€à¸›à¹‡à¸™ replica set / Atlas tier à¸—à¸µà¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š)
        try:
            async with coll.watch(full_document='updateLookup') as stream:
                async for change in stream:
                    if await request.is_disconnected():
                        break
                    doc = change.get("fullDocument")
                    if not doc:
                        continue
                    doc["_id"] = str(doc["_id"])
                    doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                    yield f"data: {json.dumps(doc)}\n\n"
        except Exception:
            # fallback: à¸–à¹‰à¸²à¹ƒà¸Šà¹‰à¹„à¸¡à¹ˆà¹„à¸”à¹‰ (à¹€à¸Šà¹ˆà¸™ standalone) à¹ƒà¸«à¹‰ polling
            last_id = latest.get("_id") if latest else None
            while not await request.is_disconnected():
                doc = await coll.find_one({}, sort=[("timestamp", -1), ("_id", -1)])
                if doc and str(doc["_id"]) != str(last_id):
                    last_id = str(doc["_id"])
                    doc["_id"] = last_id
                    doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                    yield f"data: {json.dumps(doc)}\n\n"
                else:
                    yield ": keep-alive\n\n"
                await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

@app.get("/setting")
async def setting_query(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    """
    SSE à¹à¸šà¸š query param:
    - à¸ªà¹ˆà¸‡ snapshot à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸—à¸±à¸™à¸—à¸µ (event: init)
    - à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™ polling à¸‚à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¹€à¸›à¹‡à¸™à¸Šà¹ˆà¸§à¸‡ à¹†
    """
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }
    coll = get_setting_collection_for(station_id)

    async def event_generator():
        last_id = None
        latest = await coll.find_one({}, sort=[("_id", -1)])  # â¬…ï¸ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ filter station_id à¸ à¸²à¸¢à¹ƒà¸™à¹à¸¥à¹‰à¸§
        if latest:
            latest["timestamp"] = latest.get("timestamp")
            last_id = latest.get("_id")
            yield "retry: 3000\n"
            yield "event: init\n"
            yield f"data: {to_json(latest)}\n\n"
        else:
            yield "retry: 3000\n\n"

        while True:
            if await request.is_disconnected():
                break

            doc = await coll.find_one({}, sort=[("_id", -1)])
            if doc and doc.get("_id") != last_id:
                # doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                doc["timestamp"] = doc.get("timestamp")
                last_id = doc.get("_id")
                yield f"data: {to_json(doc)}\n\n"
            else:
                yield ": keep-alive\n\n"

            await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

from pdf import pdf_routes1
app.include_router(pdf_routes1.router)

class PLCMaxSetting(BaseModel):
    station_id: str = Field(..., min_length=1)
    dynamic_max_current1: Optional[float] = None   # A
    dynamic_max_power1: Optional[float] = None  

@app.post("/setting/PLC/MAX")
async def setting_plc(payload: PLCMaxSetting):
    now_iso = datetime.now().isoformat()

    # à¸”à¸¶à¸‡à¹€à¸‰à¸žà¸²à¸°à¸„à¸µà¸¢à¹Œà¸—à¸µà¹ˆ client à¸ªà¹ˆà¸‡à¸¡à¸²à¸ˆà¸£à¸´à¸‡ à¹† (à¹„à¸¡à¹ˆà¸”à¸¶à¸‡ default None)
    try:
        incoming = payload.model_dump(exclude_unset=True)   # Pydantic v2
    except Exception:
        incoming = payload.dict(exclude_unset=True)         # Pydantic v1

    station_id = incoming.get("station_id", payload.station_id)

    # à¸ªà¸£à¹‰à¸²à¸‡ changes à¸ˆà¸²à¸à¹€à¸‰à¸žà¸²à¸°à¸„à¸µà¸¢à¹Œà¸—à¸µà¹ˆà¸¡à¸µà¹ƒà¸™ request
    keys = ("dynamic_max_current1", "dynamic_max_power1")
    changes = {k: float(incoming[k]) for k in keys if k in incoming}

    # logging à¸Šà¸±à¸”à¹€à¸ˆà¸™
    print(f"[{now_iso}] à¸£à¸±à¸šà¸„à¹ˆà¸²à¸ˆà¸²à¸ Front:")
    print(f"  station_id = {station_id}")
    print("  dynamic_max_current1 =", changes.get("dynamic_max_current1", "(no change)"), "A")
    print("  dynamic_max_power1  =", changes.get("dynamic_max_power1",  "(no change)"), "kW")

    # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µà¸ªà¸±à¸à¸Ÿà¸´à¸¥à¸”à¹Œ â†’ à¹„à¸¡à¹ˆ publish (à¸ˆà¸°à¸•à¸­à¸š 200 à¸«à¸£à¸·à¸­ 400 à¸à¹‡à¹„à¸”à¹‰à¹à¸¥à¹‰à¸§à¹à¸•à¹ˆà¸”à¸µà¹„à¸‹à¸™à¹Œ)
    if not changes:
        return {
            "ok": True,
            "message": "à¹„à¸¡à¹ˆà¸¡à¸µà¸Ÿà¸´à¸¥à¸”à¹Œà¸—à¸µà¹ˆà¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡ (à¹„à¸¡à¹ˆà¸ªà¹ˆà¸‡ MQTT)",
            "timestamp": now_iso,
            "mqtt": {
                "broker": f"{BROKER_HOST}:{BROKER_PORT}",
                "topic": MQTT_TOPIC,
                "published": False,
            },
            "data": {"station_id": station_id, "timestamp": now_iso},
        }

    # à¸›à¸£à¸°à¸à¸­à¸š payload MQTT à¹€à¸‰à¸žà¸²à¸°à¸„à¸µà¸¢à¹Œà¸—à¸µà¹ˆà¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™
    msg = {
        "station_id": station_id,
        **changes,
        "timestamp": now_iso,
        # "source": "fastapi/setting_plc"
    }
    payload_str = json.dumps(msg, ensure_ascii=False)

    # à¸ªà¹ˆà¸‡à¸‚à¸¶à¹‰à¸™ MQTT
    published = False
    try:
        pub_result = mqtt_client.publish(MQTT_TOPIC, payload_str, qos=1, retain=False)
        pub_result.wait_for_publish(timeout=2.0)
        published = pub_result.is_published()
        rc = pub_result.rc
        print(f"[MQTT] publish rc={rc}, published={published}, topic={MQTT_TOPIC}")
    except Exception as e:
        print(f"[MQTT] publish error: {e}")
        published = False

    # à¸•à¸­à¸šà¸à¸¥à¸±à¸š frontend
    return {
        "ok": True,
        "message": "à¸£à¸±à¸šà¸„à¹ˆà¸²à¸ˆà¸²à¸ frontend à¹à¸¥à¹‰à¸§ à¹à¸¥à¸°à¸žà¸¢à¸²à¸¢à¸²à¸¡à¸ªà¹ˆà¸‡ MQTT à¹à¸¥à¹‰à¸§ (à¹€à¸‰à¸žà¸²à¸°à¸Ÿà¸´à¸¥à¸”à¹Œà¸—à¸µà¹ˆà¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™)",
        "timestamp": now_iso,
        "mqtt": {
            "broker": f"{BROKER_HOST}:{BROKER_PORT}",
            "topic": MQTT_TOPIC,
            "published": bool(published),
        },
        "data": msg,
    }
class PLCCPCommand(BaseModel):
    station_id: str
    cp_status1: Literal["start", "stop"]

@app.post("/setting/PLC/CP")
async def setting_plc(payload: PLCCPCommand):
    now_iso = datetime.now().isoformat()

    # log à¸à¸±à¹ˆà¸‡à¹€à¸‹à¸´à¸£à¹Œà¸Ÿà¹€à¸§à¸­à¸£à¹Œ
    print(f"[{now_iso}] à¸£à¸±à¸šà¸„à¹ˆà¸²à¸ˆà¸²à¸ Front:")
    print(f"  station_id = {payload.station_id}")
    print(f"  cp_status1 = {payload.cp_status1}")
    # à¹€à¸•à¸£à¸µà¸¢à¸¡ message à¸—à¸µà¹ˆà¸ˆà¸°à¸ªà¹ˆà¸‡à¸‚à¸¶à¹‰à¸™ MQTT (à¹ƒà¸ªà¹ˆ timestamp à¹€à¸žà¸´à¹ˆà¸¡à¹ƒà¸«à¹‰)
    msg = {
        "station_id": payload.station_id,
        "cp_status1": payload.cp_status1,
        "timestamp": now_iso,
        # "source": "fastapi/setting_plc"
    }
    payload_str = json.dumps(msg, ensure_ascii=False)

    # à¸ªà¹ˆà¸‡à¸‚à¸¶à¹‰à¸™ MQTT (QoS 1, à¹„à¸¡à¹ˆ retain)
    try:
        pub_result = mqtt_client.publish(MQTT_TOPIC, payload_str, qos=1, retain=False)
        # à¸£à¸­à¹ƒà¸«à¹‰à¸ªà¹ˆà¸‡à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸šà¸šà¸ªà¸±à¹‰à¸™ à¹† (à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸„à¸§à¸²à¸¡à¸Šà¸±à¸§à¸£à¹Œ)
        pub_result.wait_for_publish(timeout=2.0)
        published = pub_result.is_published()
        rc = pub_result.rc
        print(f"[MQTT] publish rc={rc}, published={published}, topic={MQTT_TOPIC}")
    except Exception as e:
        print(f"[MQTT] publish error: {e}")
        published = False

    # à¸•à¸­à¸šà¸à¸¥à¸±à¸š frontend
    return {
        "ok": True,
        "message": "à¸£à¸±à¸šà¸„à¹ˆà¸²à¸ˆà¸²à¸ frontend à¹à¸¥à¹‰à¸§ à¹à¸¥à¸°à¸žà¸¢à¸²à¸¢à¸²à¸¡à¸ªà¹ˆà¸‡ MQTT à¹à¸¥à¹‰à¸§",
        "timestamp": now_iso,
        "mqtt": {
            "broker": f"{BROKER_HOST}:{BROKER_PORT}",
            "topic": MQTT_TOPIC,
            "published": bool(published),
        },
        "data": msg,
    }

class PLCH2MaxSetting(BaseModel):
    station_id: str = Field(..., min_length=1)
    dynamic_max_current2: Optional[float] = None   # A  â† optional
    dynamic_max_power2: Optional[float] = None     # kW (à¸ˆà¸²à¸ front)

@app.post("/setting/PLC/MAXH2")
async def setting_plc(payload: PLCH2MaxSetting):
    now_iso = datetime.now().isoformat()

    # à¸”à¸¶à¸‡à¹€à¸‰à¸žà¸²à¸° fields à¸—à¸µà¹ˆ client à¸ªà¹ˆà¸‡à¸¡à¸² (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰à¸„à¹ˆà¸² default)
    try:
        incoming = payload.model_dump(exclude_unset=True)  # Pydantic v2
    except Exception:
        incoming = payload.dict(exclude_unset=True)        # Pydantic v1

    station_id = incoming.get("station_id", payload.station_id)

    # à¸šà¸±à¸‡à¸„à¸±à¸šà¸§à¹ˆà¸²à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸­à¸¢à¹ˆà¸²à¸‡à¸™à¹‰à¸­à¸¢ 1 à¸Ÿà¸´à¸¥à¸”à¹Œà¸ˆà¸²à¸à¸ªà¸­à¸‡à¸•à¸±à¸§à¸™à¸µà¹‰
    keys = ("dynamic_max_current2", "dynamic_max_power2")
    changes = {k: float(incoming[k]) for k in keys if k in incoming}

    if not changes:
        # à¸–à¹‰à¸²à¸­à¸¢à¸²à¸à¹ƒà¸«à¹‰à¹„à¸¡à¹ˆà¸–à¸·à¸­à¹€à¸›à¹‡à¸™ error à¸à¹‡ return ok=False à¹„à¸”à¹‰à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™
        raise HTTPException(
            status_code=400,
            detail="At least one of dynamic_max_current2 or dynamic_max_power2 is required"
        )

    print(f"[{now_iso}] à¸£à¸±à¸šà¸„à¹ˆà¸²à¸ˆà¸²à¸ Front: station_id={station_id}")
    print("  dynamic_max_current2 =", changes.get("dynamic_max_current2", "(no change)"), "A")
    print("  dynamic_max_power2  =", changes.get("dynamic_max_power2", "(no change)"), "kW")

    msg = {"station_id": station_id, **changes, "timestamp": now_iso}
    payload_str = json.dumps(msg, ensure_ascii=False)

    published = False
    try:
        pub_result = mqtt_client.publish(MQTT_TOPIC, payload_str, qos=1, retain=False)
        pub_result.wait_for_publish(timeout=2.0)
        published = pub_result.is_published()
        rc = pub_result.rc
        print(f"[MQTT] publish rc={rc}, published={published}, topic={MQTT_TOPIC}")
    except Exception as e:
        print(f"[MQTT] publish error: {e}")
        published = False

    return {
        "ok": True,
        "message": "à¸ªà¹ˆà¸‡à¹€à¸‰à¸žà¸²à¸°à¸Ÿà¸´à¸¥à¸”à¹Œà¸—à¸µà¹ˆà¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™",
        "timestamp": now_iso,
        "mqtt": {
            "broker": f"{BROKER_HOST}:{BROKER_PORT}",
            "topic": MQTT_TOPIC,
            "published": bool(published),
        },
        "data": msg,
    }

class PLCH2CPCommand(BaseModel):
    station_id: str
    cp_status2: Literal["start", "stop"]

@app.post("/setting/PLC/CPH2")
async def setting_plc(payload: PLCH2CPCommand):
    now_iso = datetime.now().isoformat()

    # log à¸à¸±à¹ˆà¸‡à¹€à¸‹à¸´à¸£à¹Œà¸Ÿà¹€à¸§à¸­à¸£à¹Œ
    print(f"[{now_iso}] à¸£à¸±à¸šà¸„à¹ˆà¸²à¸ˆà¸²à¸ Front:")
    print(f"  station_id = {payload.station_id}")
    print(f"  cp_status2 = {payload.cp_status2}")
    # à¹€à¸•à¸£à¸µà¸¢à¸¡ message à¸—à¸µà¹ˆà¸ˆà¸°à¸ªà¹ˆà¸‡à¸‚à¸¶à¹‰à¸™ MQTT (à¹ƒà¸ªà¹ˆ timestamp à¹€à¸žà¸´à¹ˆà¸¡à¹ƒà¸«à¹‰)
    msg = {
        "station_id": payload.station_id,
        "cp_status2": payload.cp_status2,
        "timestamp": now_iso,
        # "source": "fastapi/setting_plc"
    }
    payload_str = json.dumps(msg, ensure_ascii=False)

    # à¸ªà¹ˆà¸‡à¸‚à¸¶à¹‰à¸™ MQTT (QoS 1, à¹„à¸¡à¹ˆ retain)
    try:
        pub_result = mqtt_client.publish(MQTT_TOPIC, payload_str, qos=1, retain=False)
        # à¸£à¸­à¹ƒà¸«à¹‰à¸ªà¹ˆà¸‡à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸šà¸šà¸ªà¸±à¹‰à¸™ à¹† (à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸„à¸§à¸²à¸¡à¸Šà¸±à¸§à¸£à¹Œ)
        pub_result.wait_for_publish(timeout=2.0)
        published = pub_result.is_published()
        rc = pub_result.rc
        print(f"[MQTT] publish rc={rc}, published={published}, topic={MQTT_TOPIC}")
    except Exception as e:
        print(f"[MQTT] publish error: {e}")
        published = False

    # à¸•à¸­à¸šà¸à¸¥à¸±à¸š frontend
    return {
        "ok": True,
        "message": "à¸£à¸±à¸šà¸„à¹ˆà¸²à¸ˆà¸²à¸ frontend à¹à¸¥à¹‰à¸§ à¹à¸¥à¸°à¸žà¸¢à¸²à¸¢à¸²à¸¡à¸ªà¹ˆà¸‡ MQTT à¹à¸¥à¹‰à¸§",
        "timestamp": now_iso,
        "mqtt": {
            "broker": f"{BROKER_HOST}:{BROKER_PORT}",
            "topic": MQTT_TOPIC,
            "published": bool(published),
        },
        "data": msg,
    }

# --------------------------------------------------------------------- CBM Page
def get_cbm_collection_for(station_id: str):
    # à¸à¸±à¸™à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™à¹à¸›à¸¥à¸ à¹† / injection: à¸­à¸™à¸¸à¸à¸²à¸• a-z A-Z 0-9 _ -
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return CBM_DB.get_collection(str(station_id))

@app.get("/CBM")
async def cbm_query(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    """
    SSE à¹à¸šà¸š query param:
    - à¸ªà¹ˆà¸‡ snapshot à¸¥à¹ˆà¸²à¸ªà¸¸à¸”à¸—à¸±à¸™à¸—à¸µ (event: init)
    - à¸ˆà¸²à¸à¸™à¸±à¹‰à¸™ polling à¸‚à¸­à¸‡à¹ƒà¸«à¸¡à¹ˆà¹€à¸›à¹‡à¸™à¸Šà¹ˆà¸§à¸‡ à¹†
    """
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }
    coll = get_cbm_collection_for(station_id)

    async def event_generator():
        last_id = None
        latest = await coll.find_one({}, sort=[("_id", -1)])  # â¬…ï¸ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ filter station_id à¸ à¸²à¸¢à¹ƒà¸™à¹à¸¥à¹‰à¸§
        if latest:
            # latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
            latest["timestamp"] = latest.get("timestamp")
            last_id = latest.get("_id")
            yield "retry: 3000\n"
            yield "event: init\n"
            yield f"data: {to_json(latest)}\n\n"
        else:
            yield "retry: 3000\n\n"

        while True:
            if await request.is_disconnected():
                break

            doc = await coll.find_one({}, sort=[("_id", -1)])
            if doc and doc.get("_id") != last_id:
                # doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                doc["timestamp"] = doc.get("timestamp")
                last_id = doc.get("_id")
                yield f"data: {to_json(doc)}\n\n"
            else:
                yield ": keep-alive\n\n"

            await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

# -------------------------------------------------------------- AI
def get_outputModule6_collection_for(station_id: str):
    # à¸à¸±à¸™à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¹€à¸¥à¸à¸Šà¸±à¸™à¹à¸›à¸¥à¸ à¹† / injection: à¸­à¸™à¸¸à¸à¸²à¸• a-z A-Z 0-9 _ -
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return outputModule6.get_collection(str(station_id))

@app.get("/outputModule6")
async def get_latest(
    request: Request,
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    coll = get_outputModule6_collection_for(station_id)
    doc = await coll.find_one({}, sort=[("_id", -1)])
    if not doc:
        raise HTTPException(status_code=404, detail="No data")

    # âœ… à¸•à¹‰à¸­à¸‡à¹ƒà¸ªà¹ˆà¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸«à¸¡à¸²à¸¢à¸­à¸±à¸à¸›à¸£à¸°à¸à¸²à¸¨à¸£à¸­à¸š ETag à¸•à¸²à¸¡à¸ªà¹€à¸›à¸
    etag = f"\"{str(doc['_id'])}\""

    inm = request.headers.get("if-none-match")
    # âœ… à¹ƒà¸Šà¹‰ Response à¹€à¸›à¸¥à¹ˆà¸² à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸¡à¸µ body/JSONResponse
    if inm and etag in inm:
        return Response(status_code=304)

    # à¸–à¹‰à¸²à¸ˆà¸°à¸ªà¹ˆà¸‡ body à¸•à¹‰à¸­à¸‡à¹à¸›à¸¥à¸‡ _id à¸à¹ˆà¸­à¸™
    doc["_id"] = str(doc["_id"])
    payload = jsonable_encoder(doc)

    resp = JSONResponse(content=payload)
    resp.headers["ETag"] = etag
    # à¸–à¹‰à¸² endpoint à¸¡à¸µ auth à¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰ private
    resp.headers["Cache-Control"] = "private, max-age=86400, stale-while-revalidate=604800"
    return resp

@app.get("/outputModule6/progress")
async def get_module6_progress(
    request: Request,
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """à¸”à¸¶à¸‡à¸„à¹ˆà¸² progress/health index à¸ˆà¸²à¸ MongoDB à¸ªà¸³à¸«à¸£à¸±à¸š module 6"""
    coll = get_outputModule6_collection_for(station_id)
    doc = await coll.find_one({}, sort=[("_id", -1)])
    
    if not doc:
        return JSONResponse(content={"progress": 0})
    
    # à¸ªà¸¡à¸¡à¸•à¸´à¸§à¹ˆà¸²à¸„à¹ˆà¸² health index/progress à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸Ÿà¸´à¸¥à¸”à¹Œ 'health_index' à¸«à¸£à¸·à¸­ 'rul_percentage'
    # à¸›à¸£à¸±à¸šà¸•à¸²à¸¡à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸£à¸´à¸‡à¸‚à¸­à¸‡à¸„à¸¸à¸“
    progress = doc.get("health_index", 0)  # à¸«à¸£à¸·à¸­ doc.get("rul_percentage", 0)
    
    etag = f"\"{str(doc['_id'])}\""
    inm = request.headers.get("if-none-match")
    
    if inm and etag in inm:
        return Response(status_code=304)
    
    resp = JSONResponse(content={"progress": progress})
    resp.headers["ETag"] = etag
    resp.headers["Cache-Control"] = "private, max-age=60, stale-while-revalidate=300"
    return resp

@app.get("/modules/detail")
async def get_module_detail(
    request: Request,
    module_id: str = Query(..., description="à¹€à¸Šà¹ˆà¸™ module1..module7"),
    station_id: str = Query(..., description="à¸Šà¸·à¹ˆà¸­à¸ªà¸–à¸²à¸™à¸µ à¹€à¸Šà¹ˆà¸™ Klongluang3"),
    current: UserClaims = Depends(get_current_user),
):
    if module_id not in MODULES:
        raise HTTPException(status_code=400, detail="Unknown module_id")

    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")

    input_doc = None
    output_doc = None

    # --- OUTPUT ---
    try:
        out_db = OUTPUT_DBS.get(module_id)
        if out_db is not None:
            out_coll = out_db.get_collection(str(station_id))
            output_doc = await out_coll.find_one({}, sort=[("_id", -1)])
    except Exception as e:
        print("get_module_detail output error:", e)

    # --- INPUT ---
    try:
        in_db = INPUT_DBS.get(module_id)
        if in_db is not None:
            in_coll = in_db.get_collection(str(station_id))
            input_doc = await in_coll.find_one({}, sort=[("_id", -1)])
    except Exception as e:
        print("get_module_detail input error:", e)

    # ðŸ”§ à¹à¸›à¸¥à¸‡ ObjectId â†’ str à¸”à¹‰à¸§à¸¢ custom_encoder
    input_enc = jsonable_encoder(
        input_doc,
        custom_encoder={ObjectId: str},
    ) if input_doc else None

    output_enc = jsonable_encoder(
        output_doc,
        custom_encoder={ObjectId: str},
    ) if output_doc else None

    payload = {
        "module_id": module_id,
        "station_id": station_id,
        "input": input_enc,
        "output": output_enc,
    }

    resp = JSONResponse(content=payload)
    resp.headers["Cache-Control"] = "no-store"
    return resp



# à¸ªà¸³à¸«à¸£à¸±à¸š modules à¸­à¸·à¹ˆà¸™à¹† à¸—à¸³à¹à¸šà¸šà¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™
@app.get("/modules/progress")
async def get_all_modules_progress(
    request: Request,
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """à¸”à¸¶à¸‡à¸„à¹ˆà¸² progress à¸‚à¸­à¸‡à¸—à¸¸à¸ modules à¸žà¸£à¹‰à¸­à¸¡à¸à¸±à¸™"""
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    
    result = {}
    
    # Module 1 - MDB Filters
    try:
        coll1 = outputModule1.get_collection(str(station_id))
        doc1 = await coll1.find_one({}, sort=[("_id", -1)])
        result["module1"] = doc1.get("health").get("health_index", 0) if doc1 else 0
    except:
        result["module1"] = 0
    try:
        coll2 = outputModule2.get_collection(str(station_id))
        doc2 = await coll2.find_one({}, sort=[("_id", -1)])
        result["module2"] = doc2.get("health_index_percent", 0) if doc2 else 0
    except:
        result["module2"] = 0
    
    # Module 3 - Online/Offline
    try:
        coll3 = outputModule3.get_collection(str(station_id))
        doc3 = await coll3.find_one({}, sort=[("_id", -1)])
        result["module3"] = doc3.get("health_index", 0) if doc3 else 0
    except:
        result["module3"] = 0
    
    # Module 4 - Power Supply
    try:
        coll4 = outputModule4.get_collection(str(station_id))
        doc4 = await coll4.find_one({}, sort=[("_id", -1)])
        result["module4"] = doc4.get("health_index", 0) if doc4 else 0
    except:
        result["module4"] = 0
    
    # Module 5 - Network
    try:
        coll5 = outputModule5.get_collection(str(station_id))
        doc5 = await coll5.find_one({}, sort=[("_id", -1)])
        result["module5"] = doc5.get("health").get("health_index",0) if doc5 else 0
    except:
        result["module5"] = 0
    
    # Module 6 - RUL
    try:
        coll6 = outputModule6.get_collection(str(station_id))
        doc6 = await coll6.find_one({}, sort=[("_id", -1)])
        result["module6"] = doc6.get("overall_health").get("average_health_index",0) if doc6 else 0
    except:
        result["module6"] = 0
    
    # Module 7 - Root Cause Analysis
    try:
        coll7 = outputModule7.get_collection(str(station_id))
        doc7 = await coll7.find_one({}, sort=[("_id", -1)])
        result["module7"] = doc7.get("health_index",0) if doc7 else 0
    except:
        result["module7"] = 0
    
    resp = JSONResponse(content=result)
    resp.headers["Cache-Control"] = "private, max-age=60"
    return resp


def get_station_collection():
    return station_collection

class ModuleToggleIn(BaseModel):
    enabled: bool

@app.patch("/station/{station_id}/{module_id}", status_code=204)
async def patch_module_toggle(
    station_id: str,
    module_id: str,
    body: ModuleToggleIn,
    current: UserClaims = Depends(get_current_user),
):
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² station_id à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", station_id):
        raise HTTPException(status_code=400, detail="Bad station_id")

    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² module_id à¹€à¸›à¹‡à¸™à¹‚à¸¡à¸”à¸¹à¸¥à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
    if module_id not in MODULES:
        raise HTTPException(status_code=400, detail="Invalid module_id")

    coll = get_station_collection()
    now = datetime.now(timezone.utc)

    # à¸ªà¸£à¹‰à¸²à¸‡à¸Šà¸·à¹ˆà¸­à¸‚à¸­à¸‡à¸Ÿà¸´à¸¥à¸”à¹Œà¸•à¸²à¸¡ module_id à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸£à¸±à¸šà¸¡à¸²
    module_field = f"{module_id}_isActive"

    # à¸­à¸±à¸›à¹€à¸”à¸•à¸ªà¸–à¸²à¸™à¸°à¹‚à¸¡à¸”à¸¹à¸¥
    res = coll.update_one(
        {"station_id": station_id},
        {"$set": {
            module_field: body.enabled,
            "updatedAt": now,
            # "updated_by": getattr(current, "sub", None),
        }},
        upsert=False,   # à¸–à¹‰à¸²à¸­à¸¢à¸²à¸à¸ªà¸£à¹‰à¸²à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¹ƒà¸«à¸¡à¹ˆà¹€à¸¡à¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¹€à¸ˆà¸­ à¹ƒà¸«à¹‰ True
    )

    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Station not found")
