"use client"
from zoneinfo import ZoneInfo
from fastapi import FastAPI,HTTPException,Depends, status,Request,Query,APIRouter, WebSocket, WebSocketDisconnect
from fastapi.encoders import jsonable_encoder 
from fastapi.security import OAuth2PasswordRequestForm,OAuth2PasswordBearer
from jose import JWTError,jwt
from jose.exceptions import ExpiredSignatureError
from datetime import datetime, timedelta, UTC, timezone, time, date
from pymongo.errors import OperationFailure, PyMongoError,DuplicateKeyError
from pymongo import MongoClient,ReturnDocument
from pydantic import BaseModel,EmailStr,constr, Field
from bson.objectid import ObjectId
from bson.errors import InvalidId  
from fastapi.middleware.cors import CORSMiddleware
from motor.motor_asyncio import AsyncIOMotorClient
import json, os, asyncio

from fastapi.responses import StreamingResponse,Response
from typing import List, Any,Dict, Optional, Union, Literal,Mapping
import bcrypt
from dateutil import parser as dtparser
from bson.decimal128 import Decimal128
from fastapi import Path,UploadFile, File, Form
# from fastapi import UploadFile, File, Form
# from pathlib import Path 
from starlette.staticfiles import StaticFiles
import uuid
from zoneinfo import ZoneInfo
import re
from fastapi import HTTPException, Depends
from fastapi.responses import JSONResponse
from dateutil.relativedelta import relativedelta
import pathlib, secrets
from email.message import EmailMessage
import aiosmtplib
import paho.mqtt.client as mqtt
from contextlib import asynccontextmanager

SECRET_KEY = "supersecret"  # ‡πÉ‡∏ä‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô env
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES_TECHNICIAN = 1440  # 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
ACCESS_TOKEN_EXPIRE_MINUTES_DEFAULT = 1440  # 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (user ‡∏≠‡∏∑‡πà‡∏ô)
SESSION_IDLE_MINUTES_TECHNICIAN = None  # technician ‡πÑ‡∏°‡πà‡∏°‡∏µ idle timeout
SESSION_IDLE_MINUTES_DEFAULT = 15  # user ‡∏≠‡∏∑‡πà‡∏ô idle 15 ‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏î‡πâ‡∏á
REFRESH_TOKEN_EXPIRE_DAYS = 7
th_tz = ZoneInfo("Asia/Bangkok")

# .env ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°
SMTP_HOST = os.getenv("SMTP_HOST", "smtp.gmail.com")
SMTP_PORT = int(os.getenv("SMTP_PORT", "587"))
SMTP_USER = os.getenv("SMTP_USER", "eds194655@gmail.com")
SMTP_PASS = os.getenv("SMTP_PASS", "depllvpufjwtpysc")
SENDER_EMAIL = os.getenv("SENDER_EMAIL", "eds194655@gmail.com")

# BASE = Path(__file__).parent
app = FastAPI()

client1 = MongoClient("mongodb://imps_platform:eds_imps@203.154.130.132:27017/")
client = AsyncIOMotorClient("mongodb://imps_platform:eds_imps@203.154.130.132:27017/")

deviceDB = client["utilizationFactor"]
settingDB = client["settingParameter"]
errorDB = client["errorCode"]

db = client1["iMPS"]
users_collection = db["users"]
station_collection = db["stations"]
charger_collection = db["charger"]

station_collection.create_index("station_id", unique=True)
charger_collection.create_index("station_id")  # foreign key
charger_collection.create_index("chargeBoxID")

charger_onoff = client["stationsOnOff"]

MDB_DB = client["MDB"]

CBM_DB = client["monitorCBM"]

PMReportDB = client["PMReport"]
PMUrlDB = client["PMReportURL"]

MDBPMReportDB = client["MDBPMReport"]
MDBPMUrlDB = client["MDBPMReportURL"]

CCBPMReportDB = client["CCBPMReport"]
CCBPMUrlDB = client["CCBPMReportURL"]

CBBOXPMReportDB = client["CBBOXPMReport"]
CBBOXPMUrlDB = client["CBBOXPMReportURL"]

DCTestReportDB = client["DCTestReport"]
DCUrlDB = client["DCUrl"]

ACTestReportDB = client["ACTestReport"]
ACUrlDB = client["ACUrl"]

stationPMReportDB = client["stationPMReport"]
stationPMUrlDB = client["stationPMReportURL"]

CMReportDB = client["CMReport"]
CMUrlDB = client["CMReportURL"]

outputModule1 = client["OutputModule1"]
outputModule2 = client["OutputModule2"]
outputModule3 = client["OutputModule3"]
outputModule4 = client["OutputModule4"]
outputModule5 = client["OutputModule5"]
outputModule6 = client["OutputModule6"]
outputModule7 = client["OutputModule7"]

inputModule1 = client["module1MdbDustPrediction"]
inputModule2 = client["module2ChargerDustPrediction"]
inputModule3 = client["module3ChargerOfflineAnalysis"]
inputModule4 = client["module4AbnormalPowerPrediction"]
inputModule5 = client["module5NetworkProblemPrediction"]
inputModule6 = client["module6DcChargerRulPrediction"]
inputModule7 = client["module7ChargerPowerIssue"]

# ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡∏î‡∏π‡∏•
MODULES = ["module1", "module2", "module3", "module4", "module5", "module6", "module7"]

OUTPUT_DBS = {
    "module1": outputModule1,
    "module2": outputModule2,
    "module3": outputModule3,
    "module4": outputModule4,
    "module5": outputModule5,
    "module6": outputModule6,
    "module7": outputModule7,
}

INPUT_DBS = {
    "module1": inputModule1,
    "module2": inputModule2,
    "module3": inputModule3,
    "module4": inputModule4,
    "module5": inputModule5,
    "module6": inputModule6,
    "module7": inputModule7,
}


imps_db_async = client["iMPS"]
stations_coll_async = imps_db_async["stations"]
users_coll_async = imps_db_async["users"]
email_log_coll = imps_db_async["errorEmailLog"]

MDB_collection = MDB_DB["Klongluang3"]

BROKER_HOST = "212.80.215.42"
BROKER_PORT = 1883
MQTT_TOPIC  = "iMPS/Test/settingPLC"
MQTT_CLIENT_ID = "imps-backend-setting-plc"
mqtt_client = mqtt.Client(client_id=MQTT_CLIENT_ID, clean_session=True)

def _on_connect(client, userdata, flags, rc):
    pass

def _on_disconnect(client, userdata, rc):
    pass

mqtt_client.on_connect = _on_connect
mqtt_client.on_disconnect = _on_disconnect

async def lifespan(app: FastAPI):
    # ‡πÉ‡∏ä‡πâ connect_async + loop_start ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà block event loop
    mqtt_client.connect_async(BROKER_HOST, BROKER_PORT, keepalive=60)
    mqtt_client.loop_start()
    try:
        yield
    finally:
        try:
            mqtt_client.loop_stop()
            mqtt_client.disconnect()
        except Exception as e:
            print(f"[MQTT] disconnect error: {e}")

app = FastAPI(lifespan=lifespan)

# CORS (‡∏£‡∏∞‡∏ö‡∏∏ origin ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡∏î‡∏±‡∏Å‡∏ä‡∏±‡∏ô)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["ETag"],              # ‡πÉ‡∏´‡πâ FE ‡∏≠‡πà‡∏≤‡∏ô ETag ‡πÑ‡∏î‡πâ (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô /outputModule6)
    max_age=86400  
)

def _validate_station_id(station_id: str):
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")

def get_mdb_collection_for(station_id: str):
    # ‡∏Å‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏Å ‡πÜ / injection: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï a-z A-Z 0-9 _ -
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return MDB_DB.get_collection(str(station_id))

def to_json(obj) -> str:
    # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô single-line ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö UTF-8
    return json.dumps(obj, ensure_ascii=False, separators=(",", ":"))

def get_errorCode_collection_for(station_id: str):
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return errorDB.get_collection(str(station_id))

def _ensure_utc_iso(v):
    """
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏ï‡∏£‡∏¥‡∏á ISO-8601 (UTC 'Z') ‡πÄ‡∏™‡∏°‡∏≠
    - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô datetime ‚Üí ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô UTC + ‡πÄ‡∏ï‡∏¥‡∏° 'Z'
    - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏ï‡∏£‡∏¥‡∏á ISO ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí ‡πÄ‡∏ï‡∏¥‡∏° 'Z'
    - ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô ‚Üí ‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏î‡∏¥‡∏°
    """
    if isinstance(v, datetime):
        return v.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

    if isinstance(v, str) and re.match(r'^\d{4}-\d{2}-\d{2}T', v) and not re.search(r'(Z|[+\-]\d{2}:\d{2})$', v):
        return v + 'Z'
    return v

def create_access_token(data: dict, expires_delta: int | timedelta = 15):
    to_encode = dict(data)
    expire = (datetime.now(timezone.utc) + (timedelta(minutes=expires_delta) if isinstance(expires_delta, int) else expires_delta))
    to_encode["exp"] = int(expire.timestamp())
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

class LoginRequest(BaseModel):
    email: str
    password: str

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:3001",
        "http://127.0.0.1:3000",
        "http://203.154.130.132:3000",
        "http://203.154.130.132:3001",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["ETag"],              # ‡πÉ‡∏´‡πâ FE ‡∏≠‡πà‡∏≤‡∏ô ETag ‡πÑ‡∏î‡πâ (‡πÉ‡∏ä‡πâ‡πÉ‡∏ô /outputModule6)
    max_age=86400  
)



oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/login/")

class UserClaims(BaseModel):
    sub: str
    user_id: Optional[str] = None
    username: str
    role: str = "user"
    company: Optional[str] = None
    station_ids: List[str] = []
    

def get_current_user(request: Request) -> UserClaims:
    # 1) ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ (‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö SSE)
    token = request.cookies.get(ACCESS_COOKIE_NAME)

    # 2) ‡∏™‡∏≥‡∏£‡∏≠‡∏á: Authorization: Bearer ...
    if not token:
        auth = request.headers.get("Authorization", "")
        if auth.startswith("Bearer "):
            token = auth.removeprefix("Bearer ").strip()

    if not token:
        raise HTTPException(status_code=401, detail="Not authenticated")

    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        sub = payload.get("sub")
        if not sub:
            raise HTTPException(status_code=401, detail="invalid_token")

        station_ids = payload.get("station_ids") or []
        if not isinstance(station_ids, list):
            station_ids = [station_ids]

        return UserClaims(
            sub=sub,
            user_id=payload.get("user_id"),
            username=payload.get("username"),
            role=payload.get("role", "user"),
            company=payload.get("company"),
            station_ids=station_ids,
        )
    except ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="token_expired")
    except JWTError:
        raise HTTPException(status_code=401, detail="invalid_token")

ACCESS_COOKIE_NAME = "access_token"

@app.post("/login/")
def login(body: LoginRequest, response: Response):
    user = users_collection.find_one(
        {"email": body.email},
        {"_id": 1, "email": 1, "username": 1, "password": 1, "role": 1, "company": 1, "station_id": 1},
    )
    if not user or not bcrypt.checkpw(body.password.encode("utf-8"), user["password"].encode("utf-8")):
        raise HTTPException(status_code=401, detail="Invalid email or password")

    station_ids = user.get("station_id", [])
    if not isinstance(station_ids, list):
        station_ids = [station_ids]

    # üëá ‡∏™‡∏£‡πâ‡∏≤‡∏á session id + ‡∏ï‡∏µ‡∏ï‡∏£‡∏≤‡πÄ‡∏ß‡∏•‡∏≤
    now = datetime.now(timezone.utc)
    sid = str(uuid.uuid4())
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î token expire time ‡∏ï‡∏≤‡∏°‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó
    user_role = user.get("role", "user")
    if user_role == "technician":
        token_expire_minutes = ACCESS_TOKEN_EXPIRE_MINUTES_TECHNICIAN  # 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
    else:
        token_expire_minutes = ACCESS_TOKEN_EXPIRE_MINUTES_DEFAULT  # 15 ‡∏ô‡∏≤‡∏ó‡∏µ

    jwt_token = create_access_token({
        "sub": user["email"],
        "user_id": str(user["_id"]),
        "username": user.get("username"),
        "role": user_role,
        "company": user.get("company"),
        "station_ids": station_ids,
        "sid": sid,  # ‚¨ÖÔ∏è ‡πÅ‡∏ô‡∏ö session id ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô access token
    }, expires_delta=timedelta(minutes=token_expire_minutes))

    refresh_token = create_access_token({"sub": user["email"]}, expires_delta=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS))

    # ‚úÖ ‡∏ú‡∏π‡∏Å session ‡πÉ‡∏ô DB (‡πÄ‡∏Å‡πá‡∏ö lastActiveAt ‡πÑ‡∏ß‡πâ‡πÄ‡∏ä‡πá‡∏Ñ idle)
    users_collection.update_one(
        {"_id": user["_id"]},
        {"$set": {
            "refreshTokens": [{
                "sid": sid,
                "token": refresh_token,
                "createdAt": now,
                "lastActiveAt": now,
                "expiresAt": now + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS),
            }]
        }}
    )

    response.set_cookie(
        key=ACCESS_COOKIE_NAME,
        value=jwt_token,
        httponly=True,
        secure=False,
        samesite="lax",
        max_age=int(timedelta(minutes=token_expire_minutes).total_seconds()),
        path="/",
    )

    return {
        "message": "ok",
        "access_token": jwt_token,
        "refresh_token": refresh_token,
        "user": {
            "user_id": str(user["_id"]),
            "username": user.get("username"),
            "email": user["email"],
            "role": user.get("role", "user"),
            "company": user.get("company"),
            "station_id": station_ids,
        }
    }

@app.get("/me")
def me(current: UserClaims = Depends(get_current_user)):
    if not current.user_id:
        raise HTTPException(status_code=401, detail="Missing uid in token")

    u = users_collection.find_one(
        {"_id": ObjectId(current.user_id)},
        {"_id": 1, "username": 1, "email": 1, "role": 1, "company": 1, "tel": 1,"station_id":1},
    )
    if not u:
        raise HTTPException(status_code=404, detail="User not found")

    return {
        "id": str(u["_id"]),
        "username": u.get("username") or "",
        "email": u.get("email") or "",
        "role": u.get("role") or "",
        "company": u.get("company") or "",
        "tel": u.get("tel") or "",
        "station_id": u.get("station_id") or [],
    }

@app.get("/my-stations/detail")
def my_stations_detail(current: UserClaims = Depends(get_current_user)):
    proj = {"_id": 0, "station_id": 1, "station_name": 1}

    if current.role == "admin":
        docs = list(station_collection.find({}, proj))
        return {"stations": docs}

    # Technician ‚Üí ‡∏î‡∏∂‡∏á stations ‡∏à‡∏≤‡∏Å station_id ‡πÉ‡∏ô user profile
    if current.role == "technician":
        # station_ids ‡∏°‡∏≤‡∏à‡∏≤‡∏Å JWT token ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
        if not current.station_ids or len(current.station_ids) == 0:
            return {"stations": []}
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ stations ‡∏ó‡∏µ‡πà‡∏°‡∏µ station_id ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö station_ids ‡∏Ç‡∏≠‡∏á user
        docs = list(station_collection.find(
            {"station_id": {"$in": current.station_ids}},
            proj
        ))
        return {"stations": docs}

    # Owner/Other roles ‚Üí ‡∏´‡∏≤ station ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á user ‡∏ô‡∏µ‡πâ (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á str ‡πÅ‡∏•‡∏∞ ObjectId)
    conds = [{"user_id": current.user_id}]
    try:
        conds.append({"user_id": ObjectId(current.user_id)})
    except Exception:
        pass

    docs = list(station_collection.find({"$or": conds}, proj))
    return {"stations": docs}

@app.get("/station/info")
def station_info(
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),   # ‡∏î‡∏∂‡∏á claims ‡∏à‡∏≤‡∏Å JWT
):
    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô stations
    doc = station_collection.find_one(
        {"station_id": station_id},
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å field ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏Ñ‡∏∑‡∏ô (‡∏ï‡∏±‡∏î _id ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î serialize ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ObjectId)
        {
            "_id": 0, 
            "station_id": 1, 
            "station_name": 1, 
            "SN": 1, 
            "WO": 1,
            "brand":1, 
            "PLCFirmware": 1, 
            "PIFirmware": 1, 
            "RTFirmware": 1, 
            "chargeBoxID": 1, 
            "commit_date" :1,
            "warranty_year": 1,
            "model": 1, 
            "status": 1, 
            "module1_isActive":1, 
            "module2_isActive":1, 
            "module3_isActive":1, 
            "module4_isActive":1, 
            "module5_isActive":1, 
            "module6_isActive":1, 
            "module7_isActive":1,
            "chargerNo":1,
            "chargingCables":1
        }
    )
    if not doc:
        raise HTTPException(status_code=404, detail="Station not found")

    return {"station": doc}

# @app.get("/station/info/public")
# def station_info_public(
#     station_id: str = Query(...)
# ):
#     doc = station_collection.find_one(
#         {"station_id": station_id},
#         {"_id": 0, "station_id": 1, "station_name": 1, "SN": 1, "WO": 1,"chargeBoxID": 1,
#          "model": 1,"power":1, "status": 1,"brand":1,"chargerNo":1,"chargingCables":1}
#     )
#     if not doc:
#         raise HTTPException(status_code=404, detail="Station not found")
#     return {"station": doc}

@app.get("/station/info/public")
def station_info_public(
    station_id: Optional[str] = Query(None),
    sn: Optional[str] = Query(None),
):
    """
    Get charger/station info by station_id OR sn
    """
    if not station_id and not sn:
        raise HTTPException(status_code=400, detail="Either station_id or sn is required")

    doc = None

    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ sn ‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å charger collection ‡∏Å‡πà‡∏≠‡∏ô
    if sn:
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ charger ‡∏à‡∏≤‡∏Å SN
        charger_doc = charger_collection.find_one(
            {"SN": sn},
            {
                "_id": 0,
                "SN": 1,
                "chargeBoxID": 1,
                "station_id": 1,
                "brand": 1,
                "model": 1,
                "power": 1,
                "chargerNo": 1,
                "numberOfCables": 1,
                "is_active": 1,
            }
        )
        if not charger_doc:
            raise HTTPException(status_code=404, detail=f"Charger with SN '{sn}' not found")

        # ‡∏î‡∏∂‡∏á station_id ‡∏à‡∏≤‡∏Å charger
        station_id = charger_doc.get("station_id")

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• station ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤ station_name
        station_doc = None
        if station_id:
            station_doc = station_collection.find_one(
                {"station_id": station_id},
                {"_id": 0, "station_name": 1}
            )

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á response ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å charger + station
        doc = {
            "station_id": station_id,
            "station_name": station_doc.get("station_name", "") if station_doc else "",
            "SN": charger_doc.get("SN"),
            "chargeBoxID": charger_doc.get("chargeBoxID"),
            "brand": charger_doc.get("brand"),
            "model": charger_doc.get("model"),
            "power": charger_doc.get("power"),
            "chargerNo": charger_doc.get("chargerNo"),
            "chargingCables": charger_doc.get("numberOfCables", 1),
            "status": charger_doc.get("is_active", True),
        }

    else:
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å station_id (logic ‡πÄ‡∏î‡∏¥‡∏°)
        doc = station_collection.find_one(
            {"station_id": station_id},
            {
                "_id": 0,
                "station_id": 1,
                "station_name": 1,
                "SN": 1,
                "WO": 1,
                "chargeBoxID": 1,
                "model": 1,
                "power": 1,
                "status": 1,
                "brand": 1,
                "chargerNo": 1,
                "chargingCables": 1,
            }
        )
        if not doc:
            raise HTTPException(status_code=404, detail="Station not found")

    return {"station": doc}

@app.get("/get_history")
def get_history(
    station_id: str = Query(...),
    start: str = Query(...),
    end: str = Query(...),
    current: UserClaims = Depends(get_current_user),  # ‚Üê ‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏à‡∏≤‡∏Å JWT
):
    # ‚úÖ ‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏¥‡∏ß‡∏£‡∏µ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    if station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

class RefreshIn(BaseModel):
    refresh_token: str

@app.post("/refresh")
def refresh(body: RefreshIn, response: Response):
    try:
        payload = jwt.decode(body.refresh_token, SECRET_KEY, algorithms=[ALGORITHM])
        email = payload.get("sub")
        if not email:
            raise HTTPException(status_code=401, detail="Invalid token")

        user = users_collection.find_one({"email": email})
        if not user:
            raise HTTPException(status_code=401, detail="User not found")

        entry = next((t for t in user.get("refreshTokens", []) if t.get("token") == body.refresh_token), None)
        if not entry:
            raise HTTPException(status_code=401, detail="Invalid refresh token")

        now = datetime.now(timezone.utc)
        if entry.get("expiresAt") and now > entry["expiresAt"]:
            raise HTTPException(status_code=401, detail="refresh_token_expired")

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î idle timeout ‡∏ï‡∏≤‡∏°‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó
        user_role = user.get("role", "user")
        if user_role == "technician":
            idle_timeout = SESSION_IDLE_MINUTES_TECHNICIAN  # None = ‡πÑ‡∏°‡πà‡∏°‡∏µ idle timeout
        else:
            idle_timeout = SESSION_IDLE_MINUTES_DEFAULT  # 15 ‡∏ô‡∏≤‡∏ó‡∏µ
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö idle timeout
        idle_at = entry.get("lastActiveAt")
        if idle_timeout is not None and idle_at and (now - idle_at) > timedelta(minutes=idle_timeout):
            raise HTTPException(status_code=401, detail="session_idle_timeout")

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î token expire time ‡∏ï‡∏≤‡∏°‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó
        if user_role == "technician":
            token_expire_minutes = ACCESS_TOKEN_EXPIRE_MINUTES_TECHNICIAN  # 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
        else:
            token_expire_minutes = ACCESS_TOKEN_EXPIRE_MINUTES_DEFAULT  # 15 ‡∏ô‡∏≤‡∏ó‡∏µ

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á access ‡πÉ‡∏´‡∏°‡πà (‡∏Ñ‡∏á sid ‡πÄ‡∏î‡∏¥‡∏°)
        station_ids = user.get("station_id", [])
        if not isinstance(station_ids, list):
            station_ids = [station_ids]

        new_access = create_access_token({
            "sub": user["email"],
            "user_id": str(user["_id"]),
            "username": user.get("username"),
            "role": user_role,
            "company": user.get("company"),
            "station_ids": station_ids,
            "sid": entry.get("sid"),
        }, expires_delta=timedelta(minutes=token_expire_minutes))

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï lastActiveAt
        users_collection.update_one(
            {"_id": user["_id"], "refreshTokens.token": body.refresh_token},
            {"$set": {"refreshTokens.$.lastActiveAt": now}}
        )

        # ‚ö†Ô∏è ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡∏∏‡∏Å‡∏Å‡∏µ‡πâ access ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏´‡πâ SSE ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ
        response.set_cookie(
            key=ACCESS_COOKIE_NAME,
            value=new_access,
            httponly=True,
            secure=False,          # ‡πÇ‡∏õ‡∏£‡∏î‡∏î‡∏π‡∏Ç‡πâ‡∏≠ 2 ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
            samesite="lax",        # ‡πÇ‡∏õ‡∏£‡∏î‡∏î‡∏π‡∏Ç‡πâ‡∏≠ 2 ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
            max_age=int(timedelta(minutes=token_expire_minutes).total_seconds()),
            path="/",
        )
        return {"access_token": new_access}
    except ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="refresh_token_expired")
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
    
@app.post("/logout")
async def logout(email: str, refresh_token: str):
    result = users_collection.update_one(
        {"email": email, "refreshTokens.token": refresh_token},
        {"$pull": {"refreshTokens": {"token": refresh_token}}}
    )
    if result.modified_count == 0:
        raise HTTPException(status_code=400, detail="Token not found or already logged out")
    return {"msg": "Logged out successfully"}


@app.get("/username")
async def users():
    # ‚úÖ ‡∏î‡∏∂‡∏á‡∏°‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ role = "owner"
    cursor = users_collection.find({"role": "owner"})
    usernames = [u["username"] for u in cursor]

    if not usernames:
        raise HTTPException(status_code=404, detail="owners not found")

    return {"username": usernames}
    
class register(BaseModel):
    username: str
    email: str
    password: str
    tel: str
    company: str
#create
@app.post("/insert_users/")
async def create_users(users: register):
    # hash password
    hashed_pw = bcrypt.hashpw(users.password.encode("utf-8"), bcrypt.gensalt()).decode("utf-8")
    
    now = datetime.now(timezone.utc)

    users_collection.insert_one(
    {
        "username" : users.username,
        "email":users.email,
        "password":hashed_pw,
        "tel":users.tel,
        "refreshTokens": [],
        "role":"Technician",
        "company":users.company,
        "createdAt": now,   # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°
        "updatedAt": now,   # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°
    })

@app.get("/stations/")
async def get_stations(q:str = ""):
    """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏ô‡∏µ"""
    query = {"station_name":{"$regex":  q, "$options": "i"}} if q else {}
    stations = station_collection.find(query,{"_id":0,"station_name":1})
    return [station["station_name"] for station in stations]

def to_json(doc: dict | None) -> str:
    if not doc:
        return "{}"
    d = dict(doc)
    d.pop("password", None)
    if isinstance(d.get("_id"), ObjectId):
        d["_id"] = str(d["_id"])
    return json.dumps(d, ensure_ascii=False, default=str)

# oauth2_scheme = OAuth2PasswordBearer(tokenUrl="login")  # ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ endpoint login
# decode JWT 
    
@app.get("/owner/stations/")
async def get_stations(q: str = "", current: UserClaims = Depends(get_current_user)):
    # current_user ‡∏Ñ‡∏∑‡∏≠ str(_id)
    user_obj_id = ObjectId(current.user_id)

    # ‡∏î‡∏∂‡∏á station_id ‡∏Ç‡∏≠‡∏á user
    user = users_collection.find_one({"_id": user_obj_id}, {"station_id": 1})
    if not user or "station_id" not in user:
        return []

    station_ids = user["station_id"]

    # filter stations ‡∏ï‡∏≤‡∏° station_id ‡∏Ç‡∏≠‡∏á user + query
    query_filter = {"station_id": {"$in": station_ids}}
    if q:
        query_filter["station_name"] = {"$regex": q, "$options": "i"}

    stations = station_collection.find(query_filter, {"_id": 0, "station_name": 1, "station_id": 1})
    return [{"station_name": s["station_name"], "station_id": s["station_id"]} for s in stations]


# @app.get("/selected/station/{station_id}")
# async def get_station_detail(station_id: str, current: UserClaims = Depends(get_current_user)):
#     station = station_collection.find_one({"station_id": station_id})
#     if not station:
#         raise HTTPException(status_code=404, detail="Station not found")

#     # ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á _id ‡πÄ‡∏õ‡πá‡∏ô string
#     station["_id"] = str(station["_id"])

#     return station

@app.get("/selected/station/{station_id}")
async def get_station_detail(station_id: str, current: UserClaims = Depends(get_current_user)):
    station = station_collection.find_one({"station_id": station_id})
    if not station:
        raise HTTPException(status_code=404, detail="Station not found")

    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ JSON ‡πÑ‡∏î‡πâ
    return jsonable_encoder(
        station,
        custom_encoder={
            ObjectId: str,
            datetime: lambda v: v.isoformat()
        }
    )

async def mdb_query(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    """
    SSE ‡πÅ‡∏ö‡∏ö query param:
    - ‡∏™‡πà‡∏á snapshot ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (event: init)
    - ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô polling ‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á ‡πÜ
    """
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }
    coll = get_mdb_collection_for(station_id)

    async def event_generator():
        last_id = None
        latest = await coll.find_one({}, sort=[("_id", -1)])  # ‚¨ÖÔ∏è ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á filter station_id ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÅ‡∏•‡πâ‡∏ß
        if latest:
            latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
            last_id = latest.get("_id")
            yield "retry: 3000\n"
            yield "event: init\n"
            yield f"data: {to_json(latest)}\n\n"
        else:
            yield "retry: 3000\n\n"

        while True:
            if await request.is_disconnected():
                break

            doc = await coll.find_one({}, sort=[("_id", -1)])
            if doc and doc.get("_id") != last_id:
                doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                last_id = doc.get("_id")
                yield f"data: {to_json(doc)}\n\n"
            else:
                yield ": keep-alive\n\n"

            await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

def _coerce_date_range(start: str, end: str) -> tuple[str, str]:
    def _norm(s: str, is_end: bool=False) -> str:
        if "T" not in s:  # ‡∏ß‡∏±‡∏ô‡∏•‡πâ‡∏ß‡∏ô
            hhmmss = "23:59:59.999" if is_end else "00:00:00.000"
            dt = datetime.fromisoformat(f"{s}T{hhmmss}+07:00")
            # print("521",dt)
            # print("522",dt.astimezone(timezone.utc).isoformat())
            iso_th = dt.astimezone(th_tz).isoformat()
            # print("iso_th", iso_th)

            test = dt.astimezone(timezone.utc).isoformat()
            # print("526",dt)
            # print("527",type(dt.astimezone(timezone.utc).isoformat().replace("+00:00", "T")))
            # print("528",type(dt))
            # return dt.astimezone(timezone.utc).isoformat().replace("+07:00", "T")
            return iso_th
            
            # return dt
        # ‡∏°‡∏µ T ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ timezone ‚Üí ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢
        has_tz = bool(re.search(r'(Z|[+\-]\d{2}:\d{2})$', s))
        if not has_tz:
            dt = datetime.fromisoformat(s + "+07:00")
            # print("528",dt)
        else:
            dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
            # print("531",datetime.fromisoformat(s))
            # print("532",dt)
        # print("533",dt.astimezone(timezone.utc).isoformat())    
        return dt.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")
    try:
        return _norm(start, False), _norm(end, True)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad date range")
    
@app.get("/MDB/history")
async def stream_history(
    request: Request,
    station_id: str = Query(...),
    start: str = Query(...),  
    end: str = Query(...),    
    every: str = Query("5m"), 
    current: UserClaims = Depends(get_current_user),
):
    if not re.fullmatch(r"\d{4}-\d{2}-\d{2}", start) or not re.fullmatch(r"\d{4}-\d{2}-\d{2}", end):
        raise HTTPException(status_code=400, detail="start/end must be YYYY-MM-DD")
    if start > end:
        start, end = end, start

    tz_th = ZoneInfo("Asia/Bangkok")
    now_th = datetime.now(tz_th)

    def coerce_day_bound_th(datestr: str, bound: Literal["start", "end"], now_th: datetime) -> datetime:
        tz_th = ZoneInfo("Asia/Bangkok")
        # ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏ß‡∏ô ‚Üí ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏Ç‡∏≠‡∏ö‡∏ß‡∏±‡∏ô‡πÑ‡∏ó‡∏¢
        if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", datestr):
            if bound == "start":
                dt_th = datetime.fromisoformat(f"{datestr}T00:00:00").replace(tzinfo=tz_th)
                return dt_th.astimezone(timezone.utc)
            else:
                # ‡∏ñ‡πâ‡∏≤ end ‡πÄ‡∏õ‡πá‡∏ô "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ" ‚Üí ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢ (‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏•‡∏≤‡∏Å‡πÑ‡∏õ‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï)
                if datestr == now_th.strftime("%Y-%m-%d"):
                    return now_th.astimezone(timezone.utc)
                # ‡∏°‡∏¥‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô ‡∏õ‡∏•‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡πÑ‡∏ó‡∏¢
                dt_th = datetime.fromisoformat(f"{datestr}T23:59:59.999").replace(tzinfo=tz_th)
                return dt_th.astimezone(timezone.utc)

        # ‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (Z ‡∏´‡∏£‡∏∑‡∏≠ ¬±HH:MM) ‚Üí ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏ô‡∏±‡πâ‡∏ô
        if re.search(r"(Z|[+\-]\d{2}:\d{2})$", datestr):
            return datetime.fromisoformat(datestr.replace("Z", "+00:00")).astimezone(timezone.utc)

        # ‡πÄ‡∏õ‡πá‡∏ô datetime ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô ‚Üí ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô UTC
        return datetime.fromisoformat(datestr).replace(tzinfo=tz_th).astimezone(timezone.utc)

    def ensure_dt_with_current_time_th(datestr: str) -> datetime:
        """
        - 'YYYY-MM-DD'                  -> ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢ (+07:00)
        - 'YYYY-MM-DDTHH[:MM[:SS]]'    -> ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ ‡πÉ‡∏´‡πâ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢ (+07:00)
        - ‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ 'Z' ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ '+/-HH:MM' -> ‡πÉ‡∏ä‡πâ‡πÇ‡∏ã‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏ï‡∏£‡∏¥‡∏á
        ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô UTC datetime
        """
        # ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏ß‡∏ô
        if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", datestr):
            datestr = f"{datestr}T{now_th.strftime('%H:%M:%S')}"

        # ‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡πâ‡∏ß (Z ‡∏´‡∏£‡∏∑‡∏≠ +/-HH:MM)
        if re.search(r"(Z|[+\-]\d{2}:\d{2})$", datestr):
            # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 'Z' ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô +00:00
            dt = datetime.fromisoformat(datestr.replace("Z", "+00:00"))
            return dt.astimezone(timezone.utc)

        # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ -> ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô "‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢" (+07:00) ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô UTC
        naive = datetime.fromisoformat(datestr)        # ‡πÑ‡∏°‡πà‡∏ú‡∏π‡∏Å‡πÇ‡∏ã‡∏ô‡∏Å‡πà‡∏≠‡∏ô
        dt_th = naive.replace(tzinfo=tz_th)            # ‚Üê ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ ‚Äú‡∏ú‡∏π‡∏Å +07:00‚Äù
        return dt_th.astimezone(timezone.utc)          # ‚Üê ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô UTC (‡∏ú‡∏• = ‡∏•‡∏ö 7 ‡∏ä‡∏°.)

    def _ensure_iso_with_tz(val: Any, tz: ZoneInfo) -> str | None:
        """
        ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ datetime (‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏ï‡∏£‡∏¥‡∏á) ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ ISO8601 ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô +07:00)
        """
        if val is None:
            return None
        if isinstance(val, str):
            try:
                # parse ‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ Z/offset ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô
                dt = datetime.fromisoformat(val.replace("Z", "+00:00")) if re.search(r"(Z|[+\-]\d{2}:\d{2})$", val) \
                    else datetime.fromisoformat(val).replace(tzinfo=timezone.utc)
            except Exception:
                return val  # ‡∏ñ‡πâ‡∏≤ parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏Å‡πá‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏° (‡∏Å‡∏±‡∏ô‡∏û‡∏±‡∏á)
        elif isinstance(val, datetime):
            dt = val if val.tzinfo else val.replace(tzinfo=timezone.utc)
        else:
            return None
        return dt.astimezone(tz).isoformat(timespec="milliseconds")
        
    UNIT_MAP = {"s": "second", "m": "minute", "h": "hour"}

    def parse_every(s: str) -> tuple[str, int]:
        """
        ‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏ï‡∏£‡∏¥‡∏á‡πÄ‡∏ä‡πà‡∏ô '5m', '15m', '1h' ‚Üí (unit, binSize)
        ‡∏î‡∏µ‡∏ü‡∏≠‡∏•‡∏ï‡πå = ('minute', 5)
        """
        m = re.fullmatch(r"(\d+)([smh])$", s.strip())
        if not m:
            return ("minute", 5)
        n, u = int(m.group(1)), m.group(2)
        return (UNIT_MAP[u], max(1, n))
    # start_utc = ensure_dt_with_current_time_th(start)
    # print("565",start_utc)
    # end_utc   = ensure_dt_with_current_time_th(end)
    # print("567",end_utc)

    start_utc = coerce_day_bound_th(start, "start", now_th)
    end_utc   = coerce_day_bound_th(end,   "end",   now_th)

    coll = get_mdb_collection_for(station_id)

    # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ $regexReplace/$replaceOne ‚Äî ‡πÅ‡∏¢‡∏Å case ‡∏î‡πâ‡∏ß‡∏¢ $regexMatch + $toDate/$dateFromString
    def _parse_string(varname: str):
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ (Z ‡∏´‡∏£‡∏∑‡∏≠ ¬±HH:MM) ‚Üí ‡πÉ‡∏´‡πâ Mongo ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ $toDate
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢ $dateFromString timezone "+07:00"
        return {
            "$cond": [
                { "$regexMatch": { "input": f"$${varname}", "regex": r"(Z|[+\-]\d{2}:\d{2})$" } },
                { "$toDate": f"$${varname}" },
                { "$dateFromString": {
                    "dateString": f"$${varname}",
                    "timezone": "+07:00",
                    "onError": None,
                    "onNull": None
                } }
            ]
        }

    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ sampling interval
    unit, bin_size = parse_every(every)

    # ---------- prefix ‡πÄ‡∏î‡∏¥‡∏°: ts/dayTH + match 2 ‡∏ä‡∏±‡πâ‡∏ô ----------
    prefix = [
        {   # ‚úÖ ts: ‡πÅ‡∏õ‡∏•‡∏á timestamp/Datetime ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Date
            "$addFields": {
                "ts": {
                    "$let": { "vars": { "t": "$timestamp", "d": "$Datetime" }, "in":
                        { "$cond": [
                            { "$ne": ["$$t", None] },
                            { "$switch": {
                                "branches": [
                                    { "case": { "$eq": [ { "$type": "$$t" }, "date"   ] }, "then": "$$t" },
                                    { "case": { "$eq": [ { "$type": "$$t" }, "string" ] }, "then": {
                                        "$cond": [
                                            { "$regexMatch": { "input": "$$t", "regex": r"(Z|[+\-]\d{2}:\d{2})$" } },
                                            { "$toDate": "$$t" },
                                            { "$dateFromString": { "dateString": "$$t", "timezone": "+07:00", "onError": None, "onNull": None } }
                                        ]
                                    }},
                                ],
                                "default": None
                            }},
                            { "$switch": {
                                "branches": [
                                    { "case": { "$eq": [ { "$type": "$$d" }, "date"   ] }, "then": "$$d" },
                                    { "case": { "$eq": [ { "$type": "$$d" }, "string" ] }, "then": {
                                        "$cond": [
                                            { "$regexMatch": { "input": "$$d", "regex": r"(Z|[+\-]\d{2}:\d{2})$" } },
                                            { "$toDate": "$$d" },
                                            { "$dateFromString": { "dateString": "$$d", "timezone": "+07:00", "onError": None, "onNull": None } }
                                        ]
                                    }},
                                ],
                                "default": None
                            }}
                        ] }
                    }
                }
            }
        },
        { "$addFields": { "dayTH": { "$dateToString": { "date": "$ts", "format": "%Y-%m-%d", "timezone": "+07:00" }}}},
        { "$match": { "dayTH": { "$gte": start, "$lte": end } }},  # ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏ß‡∏±‡∏ô‡πÑ‡∏ó‡∏¢
        { "$match": { "$expr": { "$and": [
            { "$gte": ["$ts", start_utc] },  # ‡∏ä‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: ts ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á
            { "$lte": ["$ts", end_utc] }
        ]}}}
    ]

    # ---------- downsample ‡∏î‡πâ‡∏ß‡∏¢ $dateTrunc ----------
    group_stage = {
        "$group": {
            "_id": {
                "bucket": {
                    "$dateTrunc": {
                        "date": "$ts",
                        "unit": unit,         # second / minute / hour
                        "binSize": bin_size,  # ‡πÄ‡∏ä‡πà‡∏ô 5
                        "timezone": "+07:00"  # ‡∏≠‡∏¥‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢
                    }
                }
            },
            # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≤‡∏ü‡∏™‡∏°‡∏π‡∏ó (‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡πà‡∏≤‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô string ‡∏î‡πâ‡∏ß‡∏¢ $convert)
            "VL1N": {"$avg": {"$convert": {"input": "$VL1N", "to": "double", "onError": None, "onNull": None}}},
            "VL2N": {"$avg": {"$convert": {"input": "$VL2N", "to": "double", "onError": None, "onNull": None}}},
            "VL3N": {"$avg": {"$convert": {"input": "$VL3N", "to": "double", "onError": None, "onNull": None}}},
            "I1":   {"$avg": {"$convert": {"input": "$I1",   "to": "double", "onError": None, "onNull": None}}},
            "I2":   {"$avg": {"$convert": {"input": "$I2",   "to": "double", "onError": None, "onNull": None}}},
            "I3":   {"$avg": {"$convert": {"input": "$I3",   "to": "double", "onError": None, "onNull": None}}},
            "PL1N": {"$avg": {"$convert": {"input": "$PL1N", "to": "double", "onError": None, "onNull": None}}},
            "PL2N": {"$avg": {"$convert": {"input": "$PL2N", "to": "double", "onError": None, "onNull": None}}},
            "PL3N": {"$avg": {"$convert": {"input": "$PL3N", "to": "double", "onError": None, "onNull": None}}},
        }
    }
    sort_stage = { "$sort": { "_id.bucket": 1 } }
    project_stage = {
        "$project": {
            "_id": 0,
            "timestamp": "$_id.bucket",  # ‡∏´‡∏£‡∏∑‡∏≠ "$_id.bucket" ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ dateTrunc
            "VL1N": 1, "VL2N": 1, "VL3N": 1,
            "I1": 1, "I2": 1, "I3": 1,
            "PL1N": 1, "PL2N": 1, "PL3N": 1,
        }
    }

    pipeline = prefix + [group_stage, sort_stage, project_stage]
    cursor = coll.aggregate(pipeline, allowDiskUse=True)


    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    async def event_generator():
        try:
            # base = pipeline[:-2]
            # cnt = await coll.aggregate(base + [{"$count": "n"}]).to_list(length=1)
            # n = cnt[0]["n"] if cnt else 0
            cnt = await coll.aggregate(prefix + [group_stage, {"$count":"n"}]).to_list(length=1)
            n = cnt[0]["n"] if cnt else 0

            yield "retry: 3000\n"
            yield f"event: stats\ndata: {json.dumps({'matched': n})}\n\n"

            sent = 0
            async for doc in cursor:
                if await request.is_disconnected():
                    break

                # ‚úÖ ‡∏≠‡∏¢‡πà‡∏≤‡πÅ‡∏ï‡∏∞ _id ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á KeyError)
                # ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢:
                # _id_val = doc.get("_id", None)
                # if _id_val is not None:
                #     try:
                #         doc["_id"] = str(_id_val)
                #     except Exception:
                #         doc["_id"] = json.dumps(_id_val, default=str, ensure_ascii=False)

                # ‚úÖ ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ timestamp ‡πÄ‡∏õ‡πá‡∏ô ISO string (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á Date/str)
                ts_val = doc.get("timestamp")
                if ts_val is not None:
                    doc["timestamp"] = _ensure_iso_with_tz(ts_val, ZoneInfo("Asia/Bangkok"))

                yield f"data: {json.dumps(doc, ensure_ascii=False)}\n\n"
                sent += 1
                await asyncio.sleep(0.001)

            if sent == 0:
                yield "event: empty\ndata: no documents in range\n\n"
            else:
                yield ": keep-alive\n\n"
        except Exception as e:
            yield f"event: server-error\ndata: {json.dumps({'message': str(e)})}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream", headers=headers)

@app.get("/MDB/history/debug")
async def mdb_history_debug(station_id: str, start: str, end: str):
    start_iso, end_iso = _coerce_date_range(start, end)
    start_key, end_key = start_iso.rstrip("Z"), end_iso.rstrip("Z")
    coll = get_mdb_collection_for(station_id)
    q = {"timestamp": {"$gte": start_key, "$lte": end_key}}
    docs = await coll.find(q, {"_id":0,"timestamp":1}).sort("timestamp", 1).limit(5).to_list(length=5)
    n = await coll.count_documents(q)
    return {"matched": n, "start_key": start_key, "end_key": end_key, "sample": docs}

def extract_token(authorization: str | None, access_token: str | None):
    if authorization and authorization.startswith("Bearer "):
        return authorization.split(" ", 1)[1]
    if access_token:
        return access_token
    raise HTTPException(status_code=401, detail="Not authenticated")
    
# @app.get("/MDB/{station_id}")
# async def mdb(request: Request, station_id: str, current: UserClaims = Depends(get_current_user)):
#     headers = {
#         "Content-Type": "text/event-stream",
#         "Cache-Control": "no-cache",
#         "Connection": "keep-alive",
#         "X-Accel-Buffering": "no",
#     }

#     coll = get_mdb_collection_for(station_id)  # ‚¨ÖÔ∏è ‡πÉ‡∏ä‡πâ coll ‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ

#     async def event_generator():
#         last_id = None

#         latest = await coll.find_one({}, sort=[("_id", -1)])
#         if latest:
#             latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
#             last_id = latest.get("_id")
#             yield f"event: init\ndata: {to_json(latest)}\n\n"
#         else:
#             yield ": keep-alive\n\n"

#         while True:
#             if await request.is_disconnected():
#                 break

#             doc = await coll.find_one({}, sort=[("_id", -1)])
#             if doc and doc.get("_id") != last_id:
#                 doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
#                 last_id = doc.get("_id")
#                 yield f"data: {to_json(doc)}\n\n"
#             else:
#                 yield ": keep-alive\n\n"

#             await asyncio.sleep(1)

#     return StreamingResponse(event_generator(), headers=headers)

@app.get("/MDB/{station_id}")
async def mdb(request: Request, station_id: str, current: UserClaims = Depends(get_current_user)):
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    coll = get_mdb_collection_for(station_id)

    async def event_generator():
        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
        latest = await coll.find_one({}, sort=[("_id", -1)])
        if latest:
            latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
            yield f"event: init\ndata: {to_json(latest)}\n\n"
        else:
            yield ": keep-alive\n\n"

        # ‡πÄ‡∏õ‡∏¥‡∏î Change Stream ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö real-time
        pipeline = [
            {"$match": {"operationType": "insert"}}  # ‡∏ü‡∏±‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ insert
        ]
        
        try:
            async with coll.watch(pipeline) as stream:
                async for change in stream:
                    if await request.is_disconnected():
                        break
                    
                    doc = change["fullDocument"]
                    doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                    yield f"data: {to_json(doc)}\n\n"
                    
        except Exception as e:
            print(f"Change stream error: {e}")
            # Fallback ‡πÄ‡∏õ‡πá‡∏ô polling ‡∏ñ‡πâ‡∏≤ Change Stream ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
            last_id = latest.get("_id") if latest else None
            while True:
                if await request.is_disconnected():
                    break
                    
                doc = await coll.find_one({}, sort=[("_id", -1)])
                if doc and doc.get("_id") != last_id:
                    doc["timestamp"] = doc.get("timestamp")
                    last_id = doc.get("_id")
                    yield f"data: {to_json(doc)}\n\n"
                else:
                    yield ": keep-alive\n\n"
                    
                await asyncio.sleep(1)

    return StreamingResponse(event_generator(), headers=headers)

@app.get("/MDB/{station_id}/peak-power")
async def mdb_peak_power(station_id: str, current_user: UserClaims = Depends(get_current_user)):
    """
    ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ PL1N, PL2N, PL3N, PL123N ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (peak) ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• database ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤ database ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢ ‡πÜ ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 150000
    """
    coll = get_mdb_collection_for(station_id)
    
    # Pipeline aggregation ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ max ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    pipeline = [
        {
            "$match": {
                "$and": [
                    {
                        "$expr": {
                            "$lte": [
                                {
                                    "$convert": {
                                        "input": "$PL1N",
                                        "to": "double",
                                        "onError": None,
                                        "onNull": 150001
                                    }
                                },
                                150000
                            ]
                        }
                    },
                    {
                        "$expr": {
                            "$lte": [
                                {
                                    "$convert": {
                                        "input": "$PL2N",
                                        "to": "double",
                                        "onError": None,
                                        "onNull": 150001
                                    }
                                },
                                150000
                            ]
                        }
                    },
                    {
                        "$expr": {
                            "$lte": [
                                {
                                    "$convert": {
                                        "input": "$PL3N",
                                        "to": "double",
                                        "onError": None,
                                        "onNull": 150001
                                    }
                                },
                                150000
                            ]
                        }
                    },
                    {
                        "$expr": {
                            "$lte": [
                                {
                                    "$convert": {
                                        "input": "$PL123N",
                                        "to": "double",
                                        "onError": None,
                                        "onNull": 150001
                                    }
                                },
                                150000
                            ]
                        }
                    }
                ]
            }
        },
        {
            "$group": {
                "_id": None,
                "PL1N_peak": {
                    "$max": {
                        "$convert": {
                            "input": "$PL1N",
                            "to": "double",
                            "onError": None,
                            "onNull": None
                        }
                    }
                },
                "PL2N_peak": {
                    "$max": {
                        "$convert": {
                            "input": "$PL2N",
                            "to": "double",
                            "onError": None,
                            "onNull": None
                        }
                    }
                },
                "PL3N_peak": {
                    "$max": {
                        "$convert": {
                            "input": "$PL3N",
                            "to": "double",
                            "onError": None,
                            "onNull": None
                        }
                    }
                },
                "PL123N_peak": {
                    "$max": {
                        "$convert": {
                            "input": "$PL123N",
                            "to": "double",
                            "onError": None,
                            "onNull": None
                        }
                    }
                },
            }
        },
        {
            "$project": {
                "_id": 0,
                "PL1N_peak": {"$round": ["$PL1N_peak", 2]},
                "PL2N_peak": {"$round": ["$PL2N_peak", 2]},
                "PL3N_peak": {"$round": ["$PL3N_peak", 2]},
                "PL123N_peak": {"$round": ["$PL123N_peak", 2]},
            }
        }
    ]
    
    result = await coll.aggregate(pipeline).to_list(length=1)
    
    if result:
        return result[0]
    else:
        return {"PL1N_peak": None, "PL2N_peak": None, "PL3N_peak": None, "PL123N_peak": None}

async def _resolve_user_id_by_chargebox(chargebox_id: Optional[str]) -> Optional[str]:
    if not chargebox_id:
        return None
    doc = await stations_coll_async.find_one(
        {"chargeBoxID": chargebox_id},
        projection={"user_id": 1}
    )
    if not doc:
        return None
    return str(doc.get("user_id")) if doc.get("user_id") is not None else None

async def _resolve_user_email_by_user_id(user_id: Optional[str]) -> Optional[str]:
    if not user_id:
        return None

    queries = []
    if ObjectId.is_valid(user_id):
        queries.append({"_id": ObjectId(user_id)})
    queries.append({"_id": user_id})  # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô string

    for q in queries:
        doc = await users_coll_async.find_one(q, projection={"email": 1})
        if doc:
            return doc.get("email")
    return None

async def _send_email_async(to_email: str, subject: str, body: str) -> None:
    msg = EmailMessage()
    msg["From"] = SENDER_EMAIL
    msg["To"] = to_email
    msg["Subject"] = subject
    msg.set_content(body)

    # STARTTLS (port 587). ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ SMTPS (465) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô use_tls=True ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á starttls
    await aiosmtplib.send(
        msg,
        hostname=SMTP_HOST,
        port=SMTP_PORT,
        start_tls=True,
        username=SMTP_USER,
        password=SMTP_PASS,
        timeout=30,
    )

async def send_error_email_once(to_email: str | None, chargebox_id: str | None, error_text: str | None, doc_id) -> bool:
    """
    ‡∏™‡πà‡∏á‡πÄ‡∏°‡∏•‡πÅ‡∏à‡πâ‡∏á error ‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ï‡πà‡∏≠‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ error (_id)
    - ‡πÉ‡∏ä‡πâ collection iMPS.errorEmailLog ‡πÄ‡∏Å‡πá‡∏ö _id ‡πÄ‡∏õ‡πá‡∏ô unique key
    - ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏õ‡πá‡∏ô sent
    - ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏•‡∏ö‡∏•‡πá‡∏≠‡∏Å‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏ô‡πâ‡∏≤
    """
    if not to_email or not error_text:
        return False

    key = str(doc_id)  # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ObjectId/str
    now_th = datetime.now(th_tz)

    # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£ insert ‡∏•‡πá‡∏≠‡∏Å (pending) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß -> ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á
    try:
        await email_log_coll.insert_one({
            "_id": key,                    # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ unique key ‡πÄ‡∏õ‡πá‡∏ô doc_id ‡∏Ç‡∏≠‡∏á error
            "status": "pending",
            "to": to_email,
            "chargeBoxID": chargebox_id,
            "createdAt": now_th,
        })
    except DuplicateKeyError:
        return False  # ‡πÄ‡∏Ñ‡∏¢‡∏™‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß

    # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á subject/body ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡∏≠‡∏µ‡πÄ‡∏°‡∏•
    subject = f"[IMPS Error] {chargebox_id or '-'}"
    body = (
        f"‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ,\n\n"
        f"‡∏°‡∏µ Error ‡∏à‡∏≤‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ/‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå: {chargebox_id or '-'}\n"
        f"‡πÄ‡∏ß‡∏•‡∏≤ (TH): {now_th:%Y-%m-%d %H:%M:%S}\n\n"
        f"‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:\n{error_text}\n\n"
        f"-- ‡∏£‡∏∞‡∏ö‡∏ö iMPS"
    )
    try:
        await _send_email_async(to_email, subject, body)
        await email_log_coll.update_one({"_id": key}, {"$set": {"status": "sent", "sentAt": datetime.now(th_tz)}})
        return True
    except Exception:
        # ‡∏ñ‡πâ‡∏≤‡∏™‡πà‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡∏•‡∏ö‡∏•‡πá‡∏≠‡∏Å pending ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏™‡πà‡∏á‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        await email_log_coll.delete_one({"_id": key})
        raise



@app.get("/error/{station_id}")
async def error_stream(request: Request, station_id: str, current: UserClaims = Depends(get_current_user)):
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    coll = get_errorCode_collection_for(station_id)

    async def event_generator():
        last_id = None

        # ----- init -----
        latest = await coll.find_one({}, sort=[("_id", -1)])
        if latest and ("error" in latest):
            last_id = latest.get("_id")

            chargebox_id = latest.get("Chargebox_ID")
            user_id = await _resolve_user_id_by_chargebox(chargebox_id)
            email = await _resolve_user_email_by_user_id(user_id)

            try:
                await send_error_email_once(email, chargebox_id, latest.get("error"), last_id)
            except Exception as e:
                # ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡∏Å‡∏™‡∏ï‡∏£‡∏µ‡∏°: log ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡∏ï‡πà‡∏≠
                print(f"[email] init send failed for {last_id}: {e}")

            payload = {
                "Chargebox_ID": chargebox_id,
                "user_id": user_id,
                "email": email,
                "error": latest.get("error"),
            }
            # yield f"event: init\ndata: {to_json(payload)}\n\n"
        else:
            yield ": keep-alive\n\n"

        # ----- updates -----
        while True:
            if await request.is_disconnected():
                break

            doc = await coll.find_one({}, sort=[("_id", -1)])
            if doc and doc.get("_id") != last_id and ("error" in doc):
                last_id = doc.get("_id")

                chargebox_id = doc.get("Chargebox_ID")
                user_id = await _resolve_user_id_by_chargebox(chargebox_id)
                email = await _resolve_user_email_by_user_id(user_id)
                
                try:
                    await send_error_email_once(email, chargebox_id, doc.get("error"), last_id)
                except Exception as e:
                    print(f"[email] update send failed for {last_id}: {e}")

                payload = {
                    "Chargebox_ID": chargebox_id,
                    "user_id": user_id,
                    "email": email,
                    "error": doc.get("error"),
                }
                # yield f"data: {to_json(payload)}\n\n"
            else:
                yield ": keep-alive\n\n"

            await asyncio.sleep(60)

    return StreamingResponse(event_generator(), headers=headers)

def parse_iso_dt(s: str) -> datetime:
    try:
        return datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        raise HTTPException(status_code=400, detail=f"Bad datetime: {s}")


def _to_utc_dt(iso_str: str) -> datetime:
    # ‡∏£‡∏±‡∏ö ISO ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢ Z ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πá‡πÑ‡∏î‡πâ ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô aware UTC
    s = iso_str
    if s.endswith("Z"):
        s = s.replace("Z", "+00:00")
    dt = datetime.fromisoformat(s)  # ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á aware/naive
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    else:
        dt = dt.astimezone(timezone.utc)
    return dt

def to_float(x, default=0.0):
    try:
        if x is None:
            return default
        if isinstance(x, (int, float)):
            return float(x)
        if isinstance(x, Decimal128):
            return float(x.to_decimal())
        s = str(x).strip().replace(",", ".")
        return float(s)
    except Exception:
        return default

async def change_stream_generator(station_id: str):
    coll = get_mdb_collection_for(station_id)
    async with coll.watch() as cs:
        async for change in cs:
            doc = change.get("fullDocument")
            if not doc:
                continue
            payload = {
                "t": _ensure_utc_iso(doc.get("timestamp")),
                "L1": doc.get("VL1N"),
                "L2": doc.get("VL2N"),
                "L3": doc.get("VL3N"),
                "I1": doc.get("I1"),
                "I2": doc.get("I2"),
                "I3": doc.get("I3"),
                "W1": doc.get("PL1N"),
                "W2": doc.get("PL2N"),
                "W3": doc.get("PL3N"),
            }
            yield f"data: {json.dumps(payload)}\n\n"



def floor_bin(dt: datetime, step_sec: int) -> datetime:
    epoch_ms = int(dt.timestamp() * 1000)
    bin_ms = epoch_ms - (epoch_ms % (step_sec * 1000))
    return datetime.fromtimestamp(bin_ms / 1000, tz=timezone.utc)

# def to_json(doc):
#     doc = dict(doc)
#     doc["_id"] = str(doc["_id"])
#     return json.dumps(doc, default=str)

################ Users
# @app.get("/all-users/")
# def all_users():
#     # ‡πÄ‡∏≠‡∏≤‡∏ó‡∏∏‡∏Å‡∏ü‡∏¥‡∏•‡∏î‡πå ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô password ‡πÅ‡∏•‡∏∞ refreshTokens
#     cursor = users_collection.find({}, {"password": 0, "refreshTokens": 0})
#     docs = list(cursor)

#     # ‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡∏™‡πà‡∏á _id ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢ ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á ObjectId -> str
#     for d in docs:
#         if "_id" in d:
#             d["_id"] = str(d["_id"])

#     return {"users": docs}

@app.get("/all-users/")
def all_users(current: UserClaims = Depends(get_current_user)):
    # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ admin (‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° owner ‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢)
    if current.role != "admin":
        raise HTTPException(status_code=403, detail="forbidden")

    cursor = users_collection.find({}, {"password": 0, "refreshTokens": 0})
    docs = list(cursor)
    for d in docs:
        if "_id" in d:
            d["_id"] = str(d["_id"])
    return {"users": docs}

class addUsers(BaseModel):
    username:str
    email:str
    password:str
    tel:str
    company_name:str
    station_id:Optional[Union[str, int, List[Union[str, int]]]] = None
    role:str 

class UserOut(BaseModel):
    id: str
    username: str
    email: EmailStr
    role: str
    company: str
    station_id: List[str] = Field(default_factory=list)
    tel: str

@app.post("/add_users/", response_model=UserOut, status_code=201)
def insert_users(body: addUsers, current: UserClaims = Depends(get_current_user)):
    if current.role != "admin":
        raise HTTPException(status_code=403, detail="forbidden")

    email = body.email.lower()
    hashed = bcrypt.hashpw(body.password.encode("utf-8"), bcrypt.gensalt()).decode("utf-8")

    station_ids: List[str] = []
    if body.station_id is not None and body.station_id != "":
        if isinstance(body.station_id, list):
            station_ids = [str(x) for x in body.station_id if str(x).strip() != ""]
        else:
            station_ids = [str(body.station_id)]

    doc = {
        "username": body.username.strip(),
        "email": email,
        "password": hashed,
        "role": body.role,
        "company": (body.company_name or "").strip() or None,
        "tel": (body.tel or "").strip() or None,
        "station_id": station_ids,
        "refreshTokens": [],
        "createdAt": datetime.now(timezone.utc),
    }

    try:
        res = users_collection.insert_one(doc)
    except DuplicateKeyError:
        raise HTTPException(status_code=409, detail="Email already exists")

    return {
        "id": str(res.inserted_id),
        "username": doc["username"],
        "email": doc["email"],
        "role": doc["role"],
        "company": doc.get("company"),
        "station_id": doc["station_id"],
        "tel": doc.get("tel"),
        "createdAt": doc["createdAt"],
    }

@app.delete("/delete_users/{user_id}", status_code=204)
def delete_user(user_id: str, current: UserClaims = Depends(get_current_user)):
    # (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å) ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞ admin/owner
    if current.role not in ("admin", "owner"):
        raise HTTPException(status_code=403, detail="Forbidden")

    try:
        oid = ObjectId(user_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Invalid user id")

    res = users_collection.delete_one({"_id": oid})
    if res.deleted_count == 0:
        raise HTTPException(status_code=404, detail="User not found")

    # 204 No Content
    return Response(status_code=204)

class UserUpdate(BaseModel):
    username: str | None = None
    email: EmailStr | None = None
    tel : str | None = None      # ‡πÉ‡∏ä‡πâ "phone" ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ
    company: str | None = None
    role: str | None = None       # admin ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏î‡πâ
    is_active: bool | None = None # admin ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏î‡πâ
    password: str | None = None   # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏Æ‡∏ä‡πÄ‡∏™‡∏°‡∏≠‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ñ‡πà‡∏≤
    station_id: Optional[List[str]] = None  # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö technician

def hash_password(raw: str) -> str:
    return bcrypt.hashpw(raw.encode("utf-8"), bcrypt.gensalt()).decode("utf-8")

# ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï
ALLOW_FIELDS_ADMIN_USER = {"username", "email", "tel", "company", "role", "is_active", "password", "station_id"}
ALLOW_FIELDS_SELF_USER  = {"username", "email", "tel", "company", "password"}


@app.patch("/user_update/{id}", response_model=UserOut)
def update_user(id: str, body: UserUpdate, current: UserClaims = Depends(get_current_user)):
    oid = to_object_id_or_400(id)

    doc = users_collection.find_one({"_id": oid})
    if not doc:
        raise HTTPException(status_code=404, detail="user not found")

    # ‚îÄ‚îÄ Permission: admin ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î, owner ‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á, ‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏´‡πâ‡∏≤‡∏°
    # if current.role == "admin":
    #     pass  # ‡∏ú‡πà‡∏≤‡∏ô
    # elif current.role == "owner":
    #     if current.user_id != str(oid):
    #         raise HTTPException(status_code=403, detail="forbidden")
    # else:
    #     # ‡∏Å‡∏±‡∏ô‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏≠‡∏∑‡πà‡∏ô ‡πÜ (‡πÄ‡∏ä‡πà‡∏ô user) ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
    #     raise HTTPException(status_code=403, detail="forbidden")

    # ‚îÄ‚îÄ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° incoming fields
    incoming = {
        k: (v.strip() if isinstance(v, str) else v)
        for k, v in body.model_dump(exclude_none=True).items()
    }
    if not incoming:
        raise HTTPException(status_code=400, detail="no fields to update")

    # ‚îÄ‚îÄ ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ï‡∏≤‡∏°‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó
    # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏™‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå settings:
    ALLOW_FIELDS_ADMIN_USER = {"username","email","password","role","company","tel","is_active","station_id"}
    ALLOW_FIELDS_SELF_OWNER = {"username","email","password","tel"}  # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ
    if current.role == "admin":
        allowed = ALLOW_FIELDS_ADMIN_USER
    else:  # owner
        allowed = ALLOW_FIELDS_SELF_OWNER

    payload = {k: v for k, v in incoming.items() if k in allowed}
    if not payload:
        raise HTTPException(status_code=400, detail="no permitted fields to update")

    # ‚îÄ‚îÄ ‡πÅ‡∏Æ‡∏ä‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if "password" in payload:
        payload["password"] = hash_password(payload["password"])

    # ‚îÄ‚îÄ validate station_id (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô list of strings)
    if "station_id" in payload:
        if payload["station_id"] is None:
            payload["station_id"] = []
        elif not isinstance(payload["station_id"], list):
            raise HTTPException(status_code=400, detail="station_id must be a list of strings")
        else:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô string
            if not all(isinstance(s, str) for s in payload["station_id"]):
                raise HTTPException(status_code=400, detail="station_id must contain only strings")

    # ‚îÄ‚îÄ validate is_active (admin ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ñ‡∏∂‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
    if "is_active" in payload and not isinstance(payload["is_active"], bool):
        raise HTTPException(status_code=400, detail="is_active must be boolean")

    now = datetime.now(timezone.utc)
    payload["updatedAt"] = now

    try:
        users_collection.update_one({"_id": oid}, {"$set": payload})
    except DuplicateKeyError:
        raise HTTPException(status_code=409, detail="duplicate email or username")

    newdoc = users_collection.find_one({"_id": oid}) or {}
    created_at = newdoc.get("createdAt") or now
    if "createdAt" not in newdoc:
        users_collection.update_one({"_id": oid}, {"$set": {"createdAt": created_at}})

    return {
        "id": str(newdoc["_id"]),
        "username": newdoc.get("username", ""),
        "email": newdoc.get("email", ""),
        "role": newdoc.get("role", ""),
        "company": (newdoc.get("company") or ""),
        "station_id": list(newdoc.get("station_id") or []),
        "tel": (newdoc.get("tel") or ""),
        "createdAt": created_at,
        "updatedAt": newdoc.get("updatedAt", now),
    }

# --------------------------------------- station ---------------------------
def parse_iso_utc(s: str) -> Optional[datetime]:
    try:
        # "2025-09-29T16:19:54.659Z"
        return datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        return None

def latest_onoff(station_id: str) -> Dict[str, Any]:
    """
    ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å stationsOnOff/<station_id>
    ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á doc:
      { payload: { value: 0/1, timestamp: "ISO-UTC" }, ... }
    """
    coll = charger_onoff.get_collection(station_id)
    doc = coll.find_one(
        sort=[("payload.timestamp", -1), ("_id", -1)]
    )
    if not doc:
        return {"status": None, "statusAt": None}

    payload = doc.get("payload", {})
    val = payload.get("value", None)
    ts = payload.get("timestamp", None)

    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô bool ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô: 1/true => True, 0/false => False, ‡∏≠‡∏∑‡πà‡∏ô‡πÜ -> None
    if isinstance(val, (int, bool)):
        status = bool(val)
    else:
        try:
            status = bool(int(val))
        except Exception:
            status = None

    status_at = parse_iso_utc(ts) if isinstance(ts, str) else None
    return {"status": status, "statusAt": status_at}

# @app.get("/all-stations/")
# def all_stations(current: UserClaims = Depends(get_current_user)):
#     # 1) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç match ‡∏ï‡∏≤‡∏° role
#     if current.role == "admin":
#         match_query = {}
#     else:
#         if not current.user_id:
#             raise HTTPException(status_code=401, detail="Missing uid in token")
#         # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Å‡∏£‡∏ì‡∏µ‡πÄ‡∏Å‡πá‡∏ö user_id ‡πÄ‡∏õ‡πá‡∏ô string ‡∏´‡∏£‡∏∑‡∏≠ ObjectId
#         conds = [{"user_id": current.user_id}]
#         try:
#             conds.append({"user_id": ObjectId(current.user_id)})
#         except Exception:
#             pass
#         match_query = {"$or": conds}

#     pipeline = [
#         {"$match": match_query},

#         # 2) ‡πÅ‡∏õ‡∏•‡∏á user_id -> ObjectId ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô string (‡πÄ‡∏û‡∏∑‡πà‡∏≠ lookup)
#         {"$addFields": {
#             "user_obj_id": {
#                 "$cond": [
#                     {"$eq": [{"$type": "$user_id"}, "string"]},
#                     {"$toObjectId": "$user_id"},
#                     "$user_id"  # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô ObjectId ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡∏¥‡∏°
#                 ]
#             }
#         }},

#         # 3) ‡∏î‡∏∂‡∏á username (‡πÅ‡∏•‡∏∞‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏≠‡∏∑‡πà‡∏ô‡πÜ‡∏à‡∏≤‡∏Å users) ‡∏î‡πâ‡∏ß‡∏¢ $lookup
#         {"$lookup": {
#             "from": "users",              # ‡∏ä‡∏∑‡πà‡∏≠ collection ‡∏Ç‡∏≠‡∏á user
#             "localField": "user_obj_id",  # _id ‡πÉ‡∏ô users ‡πÄ‡∏õ‡πá‡∏ô ObjectId
#             "foreignField": "_id",
#             "as": "owner"
#         }},
#         {"$addFields": {
#             "username": {"$arrayElemAt": ["$owner.username", 0]},
#             # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô email/phone/company
#             # "owner_email": {"$arrayElemAt": ["$owner.email", 0]},
#         }},

#         # 4) ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á array owner ‡∏Å‡∏±‡∏ö‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ
#         {"$project": {"owner": 0, "user_obj_id": 0}},
#     ]

#     docs = list(station_collection.aggregate(pipeline))

#     # ‚òÖ ‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå‡∏ï‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ
#     for d in docs:
#         sid = d.get("station_id")
#         try:
#             last = latest_onoff(str(sid))
#         except Exception:
#             last = {"status": None, "statusAt": None}
#         d["status"] = last["status"]          # true/false/None
#         d["statusAt"] = last["statusAt"]      # datetime | None

#     docs = jsonable_encoder(docs, custom_encoder={ObjectId: str})
#     for d in docs:
#         if "_id" in d:
#             d["_id"] = str(d["_id"])
#     return {"stations": docs}

# class addStations(BaseModel):
#     station_id:str
#     station_name:str
#     brand:str
#     model:str
#     SN:str
#     WO:str 
#     PLCFirmware:str 
#     PIFirmware:str 
#     RTFirmware:str
#     chargeBoxID: str 
#     user_id: Optional[str] = None  
#     owner: Optional[str] = None
#     is_active:Optional[bool] = None


# class StationOut(BaseModel):
#     id: str
#     station_id:str
#     station_name:str
#     brand:str
#     model:str
#     SN:str
#     WO:str 
#     PLCFirmware:str 
#     PIFirmware:str 
#     RTFirmware:str 
#     chargeBoxID:str
#     user_id: str 
#     username: Optional[str] = None
#     is_active:  Optional[bool] = None
#     images: Optional[dict] = None   # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
#     createdAt: Optional[datetime] = None
#     class Config:
#         json_encoders = {
#             datetime: lambda v: v.astimezone(ZoneInfo("Asia/Bangkok")).isoformat()
#         }


# @app.post("/add_stations/", response_model=StationOut, status_code=201)
# def insert_stations(
#     body: addStations,
#     current: UserClaims = Depends(get_current_user)
# ):
#     # 1) ‡∏ï‡∏±‡∏î/‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î string fields
#     station_id   = body.station_id.strip()
#     station_name = body.station_name.strip()
#     brand        = body.brand.strip()
#     model        = body.model.strip()
#     SN           = body.SN.strip()
#     WO           = body.WO.strip()
#     PLCFirmware           = body.PLCFirmware.strip()
#     PIFirmware           = body.PIFirmware.strip()
#     RTFirmware           = body.RTFirmware.strip()
#     chargeBoxID           = body.chargeBoxID.strip()

#     if current.role == "admin":
#         owner_oid = None
#         if body.user_id:
#             owner_oid = to_object_id_or_400(body.user_id)
#         elif body.owner:
#             u = users_collection.find_one({"username": body.owner.strip()}, {"_id": 1})
#             if not u:
#                 raise HTTPException(status_code=400, detail="invalid owner username")
#             owner_oid = u["_id"]
#         else:
#             if not current.user_id:
#                 raise HTTPException(status_code=401, detail="Missing uid in token")
#             owner_oid = to_object_id_or_400(current.user_id)
#     else:
#         if not current.user_id:
#             raise HTTPException(status_code=401, detail="Missing uid in token")
#         owner_oid = to_object_id_or_400(current.user_id)

#     # 3) is_active ‡πÄ‡∏õ‡πá‡∏ô boolean ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
#     is_active = True if body.is_active is None else bool(body.is_active)


#     doc: Dict[str, Any] = {
#         "station_id": station_id,
#         "station_name": station_name,
#         "brand": brand,
#         "model": model,
#         "SN": SN,
#         "WO": WO,
#         "PLCFirmware": PLCFirmware,
#         "PIFirmware": PIFirmware,
#         "RTFirmware": RTFirmware,
#         "chargeBoxID": chargeBoxID,
#         "user_id": owner_oid,
#         "is_active": is_active,
#         "images": {},      
#         "createdAt": datetime.now(timezone.utc),
#     }

#     # 5) insert + ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ duplicate key ‡∏Ç‡∏≠‡∏á station_id
#     try:
#         res = station_collection.insert_one(doc)
#     except DuplicateKeyError:
#         raise HTTPException(status_code=409, detail="station_id already exists")

#     # 6) ‡∏´‡∏≤ username ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡πÉ‡∏ô table)
#     owner_doc = users_collection.find_one({"_id": owner_oid}, {"username": 1})
#     owner_username = owner_doc.get("username") if owner_doc else None

 
#     return {
#         "id": str(res.inserted_id),
#         "station_id": doc["station_id"],
#         "station_name": doc["station_name"],
#         "brand": doc["brand"],
#         "model": doc["model"],
#         "SN": doc["SN"],
#         "WO": doc["WO"],
#         "PLCFirmware": doc["PLCFirmware"],
#         "PIFirmware": doc["PIFirmware"],
#         "RTFirmware": doc["RTFirmware"],
#         "chargeBoxID": doc["chargeBoxID"],
#         "user_id": str(doc["user_id"]),
#         "username": owner_username,
#         "is_active": doc["is_active"],
#         "images": doc.get("images", {}),        # ‚¨ÖÔ∏è ‡πÄ‡∏û‡∏¥‡πà‡∏°
#         "createdAt": doc["createdAt"],
        
#     }

# @app.delete("/delete_stations/{id}", status_code=204)
# def delete_station(id: str, current: UserClaims = Depends(get_current_user)):
#     if current.role not in ("admin", "owner"):
#         raise HTTPException(status_code=403, detail="Forbidden")
#     try:
#         oid = ObjectId(id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Invalid id")
#     res = station_collection.delete_one({"_id":  oid})
#     if res.deleted_count == 0:
#         raise HTTPException(status_code=404, detail="Station not found")
#     return Response(status_code=204)

# class StationUpdate(BaseModel):
#     station_name: Optional[str] = None
#     brand: Optional[str] = None
#     model: Optional[str] = None
#     SN: Optional[str] = None
#     WO: Optional[str] = None
#     PLCFirmware: Optional[str] = None
#     PIFirmware: Optional[str] = None
#     RTFirmware: Optional[str] = None
#     chargeBoxID: Optional[str] = None
#     # status: Optional[bool] = None
#     images: Optional[dict] = None
#     is_active: Optional[bool] = None
#     user_id: str | None = None 


# ALLOW_FIELDS_ADMIN = {"station_id", "station_name", "brand", "model", "SN", "WO", "PLCFirmware", "PIFirmware", "RTFirmware", "chargeBoxID", "status","is_active", "user_id","images"}
# # ALLOW_FIELDS_NONADMIN = {"status"}

def to_object_id_or_400(s: str) -> ObjectId:
    try:
        return ObjectId(s)
    except Exception:
        raise HTTPException(status_code=400, detail="invalid user_id")

# # ===== Helpers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ =====
# STATION_IMG_ALLOWED = {"image/jpeg", "image/png", "image/webp"}
# STATION_IMG_MAX_BYTES = 3 * 1024 * 1024  # 3 MB

# def _ensure_dir(p: pathlib.Path):
#     p.mkdir(parents=True, exist_ok=True)

# async def save_station_image(station_id: str, kind: str, up: UploadFile) -> str:
#     """
#     ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå‡∏•‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå /uploads/stations/<station_id>/
#     ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ URL ‡∏ó‡∏µ‡πà‡∏ù‡∏±‡πà‡∏á Frontend ‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ (/uploads/...)
#     """
#     if up.content_type not in STATION_IMG_ALLOWED:
#         raise HTTPException(status_code=415, detail=f"Unsupported file type: {up.content_type}")

#     data = await up.read()
#     if len(data) > STATION_IMG_MAX_BYTES:
#         raise HTTPException(status_code=413, detail="File too large (> 3MB)")

#     # ‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
#     subdir = pathlib.Path(UPLOADS_ROOT) / "stations" / station_id
#     _ensure_dir(subdir)

#     # ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: kind-uuid.ext
#     ext = {
#         "image/jpeg": ".jpg",
#         "image/png": ".png",
#         "image/webp": ".webp",
#     }.get(up.content_type, "")
#     fname = f"{kind}-{uuid.uuid4().hex}{ext}"
#     dest  = subdir / fname

#     with open(dest, "wb") as f:
#         f.write(data)

#     # URL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏≠‡∏≤‡πÑ‡∏õ‡πÅ‡∏™‡∏î‡∏á
#     url = f"/uploads/stations/{station_id}/{fname}"
#     return url

# @app.patch("/update_stations/{id}", response_model=StationOut)
# def update_station(
#     id: str,
#     body: StationUpdate,
#     current: UserClaims = Depends(get_current_user)
# ):
#     # ‡∏ï‡∏£‡∏ß‡∏à id ‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ
#     try:
#         oid = ObjectId(id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="invalid id")

#     # ‡∏´‡∏≤ station
#     st = station_collection.find_one({"_id": oid})
#     if not st:
#         raise HTTPException(status_code=404, detail="station not found")

#     # ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå: non-admin ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô owner ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
#     if current.role != "admin":
#         st_owner = st.get("user_id")  # ‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô ObjectId
#         st_owner_str = str(st_owner) if st_owner is not None else None
#         if not current.user_id or current.user_id != st_owner_str:
#             raise HTTPException(status_code=403, detail="forbidden")

#     # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤
#     incoming: Dict[str, Any] = {
#         k: (v.strip() if isinstance(v, str) else v)
#         for k, v in body.model_dump(exclude_none=True).items()
#     }
#     if not incoming:
#         raise HTTPException(status_code=400, detail="no fields to update")

#     # ‡∏ó‡∏≥ allowlist + map owner (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ admin)
#     if current.role == "admin":
#         payload = {k: v for k, v in incoming.items() if k in ALLOW_FIELDS_ADMIN}

#         # ‡∏ñ‡πâ‡∏≤ admin ‡∏™‡πà‡∏á user_id ‡∏°‡∏≤ ‚Üí ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô ObjectId ‡πÅ‡∏•‡∏∞ validate
#         if "user_id" in payload:
#             user_id_raw = payload["user_id"]

#             # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏™‡∏≠‡∏á‡πÅ‡∏ö‡∏ö: ‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô id (24hex) ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡πà‡∏á‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô username
#             udoc = None
#             if isinstance(user_id_raw, str) and len(user_id_raw) == 24:
#                 # ‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ObjectId string
#                 udoc = users_collection.find_one({"_id": to_object_id_or_400(user_id_raw)})
#             else:
#                 # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô‡∏™‡πà‡∏á username ‡∏°‡∏≤ (‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ‡πÅ‡∏ï‡πà‡∏Å‡∏±‡∏ô‡πÑ‡∏ß‡πâ)
#                 udoc = users_collection.find_one({"username": user_id_raw})

#             if not udoc:
#                 raise HTTPException(status_code=400, detail="invalid user_id")

#             # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô ObjectId ‡πÉ‡∏ô DB
#             payload["user_id"] = udoc["_id"]
    

#     if "is_active" in payload and not isinstance(payload["is_active"], bool):
#         raise HTTPException(status_code=400, detail="is_active must be boolean")

#     # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á update
#     update_doc: Dict[str, Any] = {"$set": payload}

#     # ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‚Äú‡∏•‡∏ö‚Äù ‡∏ü‡∏¥‡∏•‡∏î‡πå username ‡πÄ‡∏î‡∏¥‡∏°‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å stations (‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞ user_id)
#     # ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ (‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡πÉ‡∏™‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏•‡∏≠‡∏î):
#     update_doc["$unset"] = {"username": ""}

#     # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
#     res = station_collection.update_one({"_id": oid}, update_doc)
#     if res.matched_count == 0:
#         raise HTTPException(status_code=404, detail="station not found")

#     # ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏∑‡∏ô
#     doc = station_collection.find_one({"_id": oid})
#     created_at = doc.get("createdAt")
#     if created_at is None:
#         created_at = datetime.now(timezone.utc)   # üëà ‡∏Å‡∏±‡∏ô‡∏Ñ‡πà‡∏≤ None
#     return {
#         "id": str(doc["_id"]),
#         "station_id": doc.get("station_id", ""),
#         "station_name": doc.get("station_name", ""),
#         "brand": doc.get("brand", ""),
#         "model": doc.get("model", ""),
#         "SN": doc.get("SN", ""),
#         "WO": doc.get("WO", ""),
#         "PLCFirmware": doc.get("PLCFirmware", ""),
#         "PIFirmware": doc.get("PIFirmware", ""),
#         "RTFirmware": doc.get("RTFirmware", ""),
#         "chargeBoxID": doc.get("chargeBoxID", ""),
#         "createdAt": created_at,  
#         # ‡∏™‡πà‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô string ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ù‡∏±‡πà‡∏á client ‡πÉ‡∏ä‡πâ‡∏á‡πà‡∏≤‡∏¢
#         "user_id": str(doc["user_id"]) if doc.get("user_id") else "",
#         "username": doc.get("username"),
#         "is_active": bool(doc.get("is_active", False)),
#         "images": doc.get("images", {}),       # ‚úÖ ‡πÉ‡∏™‡πà‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
#         "updatedAt": datetime.now(timezone.utc)
#     }

# @app.post("/stations/{station_id}/upload-images")
# async def upload_station_images(
#     station_id: str,
#     station: Optional[UploadFile] = File(None),
#     mdb: Optional[UploadFile]     = File(None),
#     charger: Optional[UploadFile] = File(None),
#     device: Optional[UploadFile]  = File(None),
#     current: UserClaims = Depends(get_current_user),
# ):
#     # ‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ
#     doc = station_collection.find_one({"station_id": station_id})
#     if not doc:
#         raise HTTPException(status_code=404, detail="station not found")

#     # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå: admin ‡∏ú‡πà‡∏≤‡∏ô / owner ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
#     owner_str = str(doc.get("user_id")) if doc.get("user_id") else None
#     if current.role != "admin" and current.user_id != owner_str:
#         raise HTTPException(status_code=403, detail="forbidden")

#     updated: dict[str, str] = {}
#     for kind, up in {"station": station, "mdb": mdb, "charger": charger, "device": device}.items():
#         if up is None:
#             continue
#         url = await save_station_image(station_id, kind, up)
#         updated[kind] = url

#     if not updated:
#         return {"updated": False, "images": doc.get("images", {})}

#     images = doc.get("images", {})
#     images.update(updated)

#     station_collection.update_one(
#         {"_id": doc["_id"]},
#         {"$set": {"images": images, "updatedAt": datetime.now(timezone.utc)}}
#     )

#     return {"updated": True, "images": images}

# @app.get("/owners")
# async def get_owners():
#     cursor = users_collection.find({"role": "owner"}, {"_id": 1, "username": 1})
#     owners = [{"user_id": str(u["_id"]), "username": u["username"]} for u in cursor]

#     if not owners:
#         raise HTTPException(status_code=404, detail="owners not found")

#     return {"owners": owners}

# stationOnOff = client1["stationsOnOff"]
# class StationIdsIn(BaseModel):
#     station_ids: List[str]

# def _latest_onoff_bool(sid: str) -> bool:
#     coll = stationOnOff.get_collection(str(sid))
#     doc = coll.find_one(sort=[("payload.timestamp", -1), ("_id", -1)])  # ‚Üê ‡πÄ‡∏≠‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏£‡∏¥‡∏á ‡πÜ
#     if not doc:
#         return False
#     payload = doc.get("payload", {})
#     val = payload.get("value", 0)
#     # map ‡πÄ‡∏õ‡πá‡∏ô bool ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î
#     if isinstance(val, bool):
#         return val
#     try:
#         return bool(int(val))
#     except Exception:
#         return False

# @app.post("/station-onoff/bulk")
# def get_station_onoff_bulk(body: StationIdsIn):
#     out: Dict[str, bool] = {}
#     for sid in body.station_ids:
#         try:
#             out[sid] = _latest_onoff_bool(sid)
#         except Exception:
#             out[sid] = False
#     return {"status": out}

@app.get("/charger-onoff/{station_id}")
def station_onoff_latest(station_id: str, current: UserClaims = Depends(get_current_user)):
    # if current.role != "admin" and station_id not in set(current.station_ids):
        # raise HTTPException(status_code=403, detail="Forbidden station_id")

    data = latest_onoff(str(station_id))
    status_at_iso = (
        data["statusAt"].astimezone(ZoneInfo("Asia/Bangkok")).isoformat()
        if data["statusAt"] else None
    )
    return {"station_id": station_id, "status": data["status"], "statusAt": status_at_iso}

def parse_iso_any_tz(s: str) -> datetime | None:
    if not isinstance(s, str):
        return None
    try:
        return datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        try:
            return datetime.fromisoformat(s + "+00:00")
        except Exception:
            return None
# ------------------------------------ new station
# ----- Charger Models -----
class ChargerCreate(BaseModel):
    chargeBoxID: str
    chargerNo: Optional[int] = None  # Auto-generated if not provided
    brand: str
    model: str
    SN: str
    WO: str
    power: Optional[str] = ""
    PLCFirmware: str
    PIFirmware: str
    RTFirmware: str
    commissioningDate: Optional[str] = None
    warrantyYears: Optional[int] = 1
    numberOfCables: Optional[int] = 1
    is_active: Optional[bool] = True  # NEW: Charger active status

class ChargerUpdate(BaseModel):
    chargeBoxID: Optional[str] = None
    chargerNo: Optional[int] = None
    brand: Optional[str] = None
    model: Optional[str] = None
    SN: Optional[str] = None
    WO: Optional[str] = None
    power: Optional[str] = None
    PLCFirmware: Optional[str] = None
    PIFirmware: Optional[str] = None
    RTFirmware: Optional[str] = None
    commissioningDate: Optional[str] = None
    warrantyYears: Optional[int] = None
    numberOfCables: Optional[int] = None
    is_active: Optional[bool] = None  # NEW: Charger active status

class ChargerOut(BaseModel):
    id: str
    station_id: str
    chargeBoxID: str
    chargerNo: Optional[int] = None
    brand: str
    model: str
    SN: str
    WO: str
    power: Optional[str] = ""
    PLCFirmware: str
    PIFirmware: str
    RTFirmware: str
    commissioningDate: Optional[str] = None
    warrantyYears: Optional[int] = 1
    numberOfCables: Optional[int] = 1
    is_active: Optional[bool] = True  # NEW: Charger active status
    status: Optional[bool] = None
    images: Optional[dict] = None
    createdAt: Optional[datetime] = None

# ----- Station Models -----
class StationCreate(BaseModel):
    station_id: str
    station_name: str
    user_id: Optional[str] = None
    owner: Optional[str] = None  # username (fallback)
    is_active: Optional[bool] = True

class StationUpdate(BaseModel):
    station_name: Optional[str] = None
    is_active: Optional[bool] = None
    user_id: Optional[str] = None

class StationOut(BaseModel):
    id: str
    station_id: str
    station_name: str
    user_id: str
    username: Optional[str] = None
    is_active: bool
    status: Optional[bool] = None
    stationImage: Optional[str] = None
    chargers: List[ChargerOut] = []
    createdAt: Optional[datetime] = None

    class Config:
        json_encoders = {
            datetime: lambda v: v.astimezone(ZoneInfo("Asia/Bangkok")).isoformat()
        }

# ----- Combined Create (Station + Chargers) -----
class StationWithChargersCreate(BaseModel):
    """For creating Station with Chargers in one request"""
    station: StationCreate
    chargers: List[ChargerCreate]

class StationWithChargersOut(BaseModel):
    station: StationOut
    chargers: List[ChargerOut]

# ============================================================
# Helper Functions
# ============================================================

def to_object_id(s: str) -> ObjectId:
    """Convert string to ObjectId or raise 400"""
    try:
        return ObjectId(s)
    except (InvalidId, Exception):
        raise HTTPException(status_code=400, detail="Invalid ID format")


def get_username_by_user_id(user_id: ObjectId) -> Optional[str]:
    """Get username from user_id"""
    user = users_collection.find_one({"_id": user_id}, {"username": 1})
    return user.get("username") if user else None


def get_charger_status(station_id: str, chargeBoxID: str) -> bool:
    """Get latest Charger status from stationOnOff"""
    try:
        coll = charger_onoff.get_collection(str(station_id))
        doc = coll.find_one(
            {"payload.chargeBoxID": chargeBoxID},
            sort=[("payload.timestamp", -1), ("_id", -1)]
        )
        if not doc:
            return False
        val = doc.get("payload", {}).get("value", 0)
        return bool(int(val)) if not isinstance(val, bool) else val
    except Exception:
        return False

def get_station_status(station_id: str) -> bool:
    """Get latest Station status"""
    try:
        coll = charger_onoff.get_collection(str(station_id))
        doc = coll.find_one(sort=[("payload.timestamp", -1), ("_id", -1)])
        if not doc:
            return False
        val = doc.get("payload", {}).get("value", 0)
        return bool(int(val)) if not isinstance(val, bool) else val
    except Exception:
        return False


def get_next_charger_no(station_id: str) -> int:
    """Get next chargerNo for a station"""
    max_charger = charger_collection.find_one(
        {"station_id": station_id},
        sort=[("chargerNo", -1)]
    )
    if max_charger and max_charger.get("chargerNo"):
        return max_charger["chargerNo"] + 1
    return 1


def format_charger(doc: dict, include_status: bool = True) -> ChargerOut:
    """Convert charger document to ChargerOut"""
    charger_id = str(doc["_id"])
    station_id = doc.get("station_id", "")
    
    status = None
    if include_status:
        status = get_charger_status(station_id, doc.get("chargeBoxID", ""))
    
    return ChargerOut(
        id=charger_id,
        station_id=station_id,
        chargeBoxID=doc.get("chargeBoxID", ""),
        chargerNo=doc.get("chargerNo"),
        brand=doc.get("brand", ""),
        model=doc.get("model", ""),
        SN=doc.get("SN", ""),
        WO=doc.get("WO", ""),
        power=doc.get("power", ""),
        PLCFirmware=doc.get("PLCFirmware", ""),
        PIFirmware=doc.get("PIFirmware", ""),
        RTFirmware=doc.get("RTFirmware", ""),
        commissioningDate=doc.get("commissioningDate"),
        warrantyYears=doc.get("warrantyYears", 1),
        numberOfCables=doc.get("numberOfCables", 1),
        is_active=doc.get("is_active", True),
        status=status,
        images=doc.get("images", {}),
        createdAt=doc.get("createdAt"),
    )

def format_station_with_chargers(station_doc: dict, charger_docs: List[dict]) -> StationOut:
    """Convert station document + chargers to StationOut"""
    station_id = station_doc.get("station_id", "")
    user_id = station_doc.get("user_id")
    user_id_str = str(user_id) if user_id else ""
    
    # Get username
    username = None
    if user_id:
        username = get_username_by_user_id(user_id if isinstance(user_id, ObjectId) else to_object_id(user_id))
    
    # Get status
    status = get_station_status(station_id)
    
    # Format chargers
    chargers = [format_charger(c) for c in charger_docs]
    
    return StationOut(
        id=str(station_doc["_id"]),
        station_id=station_id,
        station_name=station_doc.get("station_name", ""),
        user_id=user_id_str,
        username=username,
        is_active=bool(station_doc.get("is_active", False)),
        status=status,
        stationImage=station_doc.get("images", {}).get("station"),
        chargers=chargers,
        createdAt=station_doc.get("createdAt"),
    )


# ============================================================
# Image Upload Helpers
# ============================================================

ALLOWED_IMAGE_TYPES = {"image/jpeg", "image/png", "image/webp"}
MAX_IMAGE_BYTES = 3 * 1024 * 1024  # 3 MB


def _ensure_dir(p: pathlib.Path):
    p.mkdir(parents=True, exist_ok=True)


async def save_image(folder: str, item_id: str, kind: str, upload: UploadFile) -> str:
    """
    Save image and return URL
    folder: "stations" or "chargers"
    """
    if upload.content_type not in ALLOWED_IMAGE_TYPES:
        raise HTTPException(status_code=415, detail=f"Unsupported file type: {upload.content_type}")
    
    data = await upload.read()
    if len(data) > MAX_IMAGE_BYTES:
        raise HTTPException(status_code=413, detail="File too large (> 3MB)")
    
    subdir = pathlib.Path(UPLOADS_ROOT) / folder / item_id
    _ensure_dir(subdir)
    
    ext = {
        "image/jpeg": ".jpg",
        "image/png": ".png",
        "image/webp": ".webp",
    }.get(upload.content_type, "")
    
    fname = f"{kind}-{uuid.uuid4().hex}{ext}"
    dest = subdir / fname
    
    with open(dest, "wb") as f:
        f.write(data)
    
    return f"/uploads/{folder}/{item_id}/{fname}"


# ---------------------------------------------------------
# GET /all-stations/ - Get all Stations with Chargers
# ---------------------------------------------------------
@app.get("/all-stations/")
def get_all_stations(
    # current: UserClaims = Depends(get_current_user)
):
    """Get all Stations with Chargers (nested)"""
    
    # TODO: Add filter by role
    # if current.role == "admin":
    #     match_query = {}
    # else:
    #     match_query = {"user_id": to_object_id(current.user_id)}
    
    match_query = {}
    
    # Get stations
    stations_cursor = station_collection.find(match_query)
    
    result = []
    for station_doc in stations_cursor:
        station_id = station_doc.get("station_id")
        
        # Get chargers for this station (sorted by chargerNo)
        chargers_cursor = charger_collection.find({"station_id": station_id}).sort("chargerNo", 1)
        charger_docs = list(chargers_cursor)
        
        # Format and add to result
        station_out = format_station_with_chargers(station_doc, charger_docs)
        result.append(station_out.dict())
    
    return {"stations": result}


# ---------------------------------------------------------
# POST /add_stations/ - Create Station with Chargers
# ---------------------------------------------------------
@app.post("/add_stations/", status_code=201)
def create_station_with_chargers(
    body: StationWithChargersCreate,
    # current: UserClaims = Depends(get_current_user)
):
    """Create new Station with Chargers"""
    
    station_data = body.station
    chargers_data = body.chargers
    
    # Validate station_id
    station_id = station_data.station_id.strip()
    if not station_id:
        raise HTTPException(status_code=400, detail="station_id is required")
    
    # Check duplicate station_id
    if station_collection.find_one({"station_id": station_id}):
        raise HTTPException(status_code=409, detail="station_id already exists")
    
    # Resolve owner
    owner_oid = None
    if station_data.user_id:
        owner_oid = to_object_id(station_data.user_id)
    elif station_data.owner:
        user = users_collection.find_one({"username": station_data.owner.strip()}, {"_id": 1})
        if not user:
            raise HTTPException(status_code=400, detail="Invalid owner username")
        owner_oid = user["_id"]
    # else:
    #     owner_oid = to_object_id(current.user_id)
    
    now = datetime.now(timezone.utc)
    
    # 1) Insert Station
    station_doc = {
        "station_id": station_id,
        "station_name": station_data.station_name.strip(),
        "user_id": owner_oid,
        "is_active": station_data.is_active if station_data.is_active is not None else True,
        "images": {},
        "createdAt": now,
    }
    
    try:
        station_result = station_collection.insert_one(station_doc)
    except DuplicateKeyError:
        raise HTTPException(status_code=409, detail="station_id already exists")
    
    # 2) Insert Chargers
    created_chargers = []
    for idx, charger in enumerate(chargers_data):
        # Use provided chargerNo or auto-generate
        charger_no = charger.chargerNo if charger.chargerNo else idx + 1
        
        charger_doc = {
            "station_id": station_id,
            "chargeBoxID": charger.chargeBoxID.strip(),
            "chargerNo": charger_no,
            "brand": charger.brand.strip(),
            "model": charger.model.strip(),
            "SN": charger.SN.strip(),
            "WO": charger.WO.strip(),
            "power": charger.power.strip() if charger.power else "",
            "PLCFirmware": charger.PLCFirmware.strip(),
            "PIFirmware": charger.PIFirmware.strip(),
            "RTFirmware": charger.RTFirmware.strip(),
            "commissioningDate": charger.commissioningDate,
            "warrantyYears": charger.warrantyYears if charger.warrantyYears else 1,
            "numberOfCables": charger.numberOfCables if charger.numberOfCables else 1,
            "is_active": charger.is_active if charger.is_active is not None else True,
            "images": {},
            "createdAt": now,
        }
        charger_result = charger_collection.insert_one(charger_doc)
        charger_doc["_id"] = charger_result.inserted_id
        created_chargers.append(charger_doc)
    
    # 3) Format response
    station_doc["_id"] = station_result.inserted_id
    station_out = format_station_with_chargers(station_doc, created_chargers)
    
    return {
        "id": str(station_result.inserted_id),
        "station": station_out.dict(),
        "chargers": [format_charger(c, include_status=False).dict() for c in created_chargers],
    }

# ---------------------------------------------------------
# PATCH /update_stations/{id} - Update Station
# ---------------------------------------------------------
@app.patch("/update_stations/{id}")
def update_station(
    id: str,
    body: StationUpdate,
    # current: UserClaims = Depends(get_current_user)
):
    """Update Station data (not including Chargers)"""
    
    oid = to_object_id(id)
    station = station_collection.find_one({"_id": oid})
    if not station:
        raise HTTPException(status_code=404, detail="Station not found")
    
    # TODO: Check permission
    # if current.role != "admin" and str(station.get("user_id")) != current.user_id:
    #     raise HTTPException(status_code=403, detail="Forbidden")
    
    update_data = {}
    if body.station_name is not None:
        update_data["station_name"] = body.station_name.strip()
    if body.is_active is not None:
        update_data["is_active"] = body.is_active
    if body.user_id is not None:
        update_data["user_id"] = to_object_id(body.user_id)
    
    if not update_data:
        raise HTTPException(status_code=400, detail="No fields to update")
    
    update_data["updatedAt"] = datetime.now(timezone.utc)
    
    station_collection.update_one({"_id": oid}, {"$set": update_data})
    
    # Return updated station
    updated = station_collection.find_one({"_id": oid})
    chargers = list(charger_collection.find({"station_id": updated["station_id"]}).sort("chargerNo", 1))
    
    return format_station_with_chargers(updated, chargers).dict()


# ---------------------------------------------------------
# DELETE /delete_stations/{id} - Delete Station and all Chargers
# ---------------------------------------------------------
@app.delete("/delete_stations/{id}", status_code=204)
def delete_station(
    id: str,
    # current: UserClaims = Depends(get_current_user)
):
    """Delete Station and all related Chargers"""
    
    oid = to_object_id(id)
    station = station_collection.find_one({"_id": oid})
    if not station:
        raise HTTPException(status_code=404, detail="Station not found")
    
    # TODO: Check permission
    
    station_id = station["station_id"]
    
    # Delete all chargers of this station
    charger_collection.delete_many({"station_id": station_id})
    
    # Delete station
    station_collection.delete_one({"_id": oid})
    
    return Response(status_code=204)


# ---------------------------------------------------------
# POST /add_charger/{station_id} - Add Charger to Station
# ---------------------------------------------------------
@app.post("/add_charger/{station_id}", status_code=201)
def add_charger_to_station(
    station_id: str,
    body: ChargerCreate,
    # current: UserClaims = Depends(get_current_user)
):
    """Add new Charger to existing Station"""
    
    # Check if station exists
    station = station_collection.find_one({"station_id": station_id})
    if not station:
        raise HTTPException(status_code=404, detail="Station not found")
    
    # TODO: Check permission
    
    # Get next chargerNo if not provided
    charger_no = body.chargerNo if body.chargerNo else get_next_charger_no(station_id)
    
    charger_doc = {
        "station_id": station_id,
        "chargeBoxID": body.chargeBoxID.strip(),
        "chargerNo": charger_no,
        "brand": body.brand.strip(),
        "model": body.model.strip(),
        "SN": body.SN.strip(),
        "WO": body.WO.strip(),
        "power": body.power.strip() if body.power else "",
        "PLCFirmware": body.PLCFirmware.strip(),
        "PIFirmware": body.PIFirmware.strip(),
        "RTFirmware": body.RTFirmware.strip(),
        "commissioningDate": body.commissioningDate,
        "warrantyYears": body.warrantyYears if body.warrantyYears else 1,
        "numberOfCables": body.numberOfCables if body.numberOfCables else 1,
        "is_active": body.is_active if body.is_active is not None else True,
        "images": {},
        "createdAt": datetime.now(timezone.utc),
    }
    
    result = charger_collection.insert_one(charger_doc)
    charger_doc["_id"] = result.inserted_id
    
    return format_charger(charger_doc, include_status=False).dict()


# ---------------------------------------------------------
# PATCH /update_charger/{id} - Update Charger
# ---------------------------------------------------------
@app.patch("/update_charger/{id}")
def update_charger(
    id: str,
    body: ChargerUpdate,
    # current: UserClaims = Depends(get_current_user)
):
    """Update Charger data"""
    
    oid = to_object_id(id)
    charger = charger_collection.find_one({"_id": oid})
    if not charger:
        raise HTTPException(status_code=404, detail="Charger not found")
    
    # TODO: Check permission via station owner
    # station = stations_collection.find_one({"station_id": charger["station_id"]})
    # if current.role != "admin" and str(station.get("user_id")) != current.user_id:
    #     raise HTTPException(status_code=403, detail="Forbidden")
    
    update_data = {}
    
    # String fields
    for field in ["chargeBoxID", "brand", "model", "SN", "WO", "power", "PLCFirmware", "PIFirmware", "RTFirmware", "commissioningDate"]:
        value = getattr(body, field, None)
        if value is not None:
            update_data[field] = value.strip() if isinstance(value, str) else value
    
    # Integer fields
    for field in ["chargerNo", "warrantyYears", "numberOfCables"]:
        value = getattr(body, field, None)
        if value is not None:
            update_data[field] = value
    
    # Boolean fields
    if body.is_active is not None:
        update_data["is_active"] = body.is_active
    
    if not update_data:
        raise HTTPException(status_code=400, detail="No fields to update")
    
    update_data["updatedAt"] = datetime.now(timezone.utc)
    
    charger_collection.update_one({"_id": oid}, {"$set": update_data})
    
    updated = charger_collection.find_one({"_id": oid})
    return format_charger(updated).dict()


# ---------------------------------------------------------
# DELETE /delete_charger/{id} - Delete Charger
# ---------------------------------------------------------
@app.delete("/delete_charger/{id}", status_code=204)
def delete_charger(
    id: str,
    # current: UserClaims = Depends(get_current_user)
):
    """Delete Charger"""
    
    oid = to_object_id(id)
    charger = charger_collection.find_one({"_id": oid})
    if not charger:
        raise HTTPException(status_code=404, detail="Charger not found")
    
    # TODO: Check permission
    
    charger_collection.delete_one({"_id": oid})
    return Response(status_code=204)


# ---------------------------------------------------------
# POST /stations/{station_id}/upload-image - Upload Station image
# ---------------------------------------------------------
@app.post("/stations/{station_id}/upload-image")
async def upload_station_image(
    station_id: str,
    station: Optional[UploadFile] = File(None),
    # current: UserClaims = Depends(get_current_user)
):
    """Upload image for Station"""
    
    doc = station_collection.find_one({"station_id": station_id})
    if not doc:
        raise HTTPException(status_code=404, detail="Station not found")
    
    # TODO: Check permission
    
    if not station:
        return {"updated": False, "images": doc.get("images", {})}
    
    url = await save_image("stations", station_id, "station", station)
    
    images = doc.get("images", {})
    images["station"] = url
    
    station_collection.update_one(
        {"_id": doc["_id"]},
        {"$set": {"images": images, "updatedAt": datetime.now(timezone.utc)}}
    )
    
    return {"updated": True, "images": images}


# ---------------------------------------------------------
# POST /chargers/{charger_id}/upload-images - Upload Charger images
# ---------------------------------------------------------
@app.post("/chargers/{charger_id}/upload-images")
async def upload_charger_images(
    charger_id: str,
    mdb: Optional[UploadFile] = File(None),
    charger: Optional[UploadFile] = File(None),
    device: Optional[UploadFile] = File(None),
    # current: UserClaims = Depends(get_current_user)
):
    """Upload images for Charger (mdb, charger, device)"""
    
    oid = to_object_id(charger_id)
    doc = charger_collection.find_one({"_id": oid})
    if not doc:
        raise HTTPException(status_code=404, detail="Charger not found")
    
    # TODO: Check permission
    
    updated_images = {}
    for kind, upload in {"mdb": mdb, "charger": charger, "device": device}.items():
        if upload is None:
            continue
        url = await save_image("chargers", charger_id, kind, upload)
        updated_images[kind] = url
    
    if not updated_images:
        return {"updated": False, "images": doc.get("images", {})}
    
    images = doc.get("images", {})
    images.update(updated_images)
    
    charger_collection.update_one(
        {"_id": oid},
        {"$set": {"images": images, "updatedAt": datetime.now(timezone.utc)}}
    )
    
    return {"updated": True, "images": images}


# ---------------------------------------------------------
# GET /station/{station_id} - Get single Station with Chargers
# ---------------------------------------------------------
@app.get("/station/{station_id}")
def get_station(
    station_id: str,
    # current: UserClaims = Depends(get_current_user)
):
    """Get single Station with Chargers"""
    
    station = station_collection.find_one({"station_id": station_id})
    if not station:
        raise HTTPException(status_code=404, detail="Station not found")
    
    chargers = list(charger_collection.find({"station_id": station_id}).sort("chargerNo", 1))
    
    return format_station_with_chargers(station, chargers).dict()


# ---------------------------------------------------------
# GET /chargers/{station_id} - Get Chargers of Station
# ---------------------------------------------------------
@app.get("/chargers/{station_id}")
def get_chargers_by_station(
    station_id: str,
    # current: UserClaims = Depends(get_current_user)
):
    """Get all Chargers of a Station"""
    
    chargers = list(charger_collection.find({"station_id": station_id}).sort("chargerNo", 1))
    return {"chargers": [format_charger(c).dict() for c in chargers]}


# -------------------------------------------------- PMReport (charger)   
#     
# def get_pmreport_collection_for(station_id: str):
#     _validate_station_id(station_id)
#     coll = PMReportDB.get_collection(str(station_id))
#     return coll

# def _compute_next_pm_date_str(pm_date_str: str | None) -> str | None:
#     if not pm_date_str:
#         return None
#     # pm_date ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô "YYYY-MM-DD"
#     try:
#         d = datetime.fromisoformat(pm_date_str).date()  # date object
#     except ValueError:
#         return None
#     next_d = d + relativedelta(months=+6)              # ‚Üê ‡∏ï‡∏£‡∏á 6 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô
#     return next_d.isoformat()     

# # --- helper: ‡πÄ‡∏≠‡∏≤ pm_date ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å PMReportDB/<station_id> ---
# async def _latest_pm_date_from_pmreport(station_id: str) -> dict | None:
#     if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
#         raise HTTPException(status_code=400, detail="Bad station_id")
#     coll = PMReportDB.get_collection(str(station_id))

#     pipeline = [
#         {"$addFields": {
#             "_ts": {
#                 "$ifNull": [
#                     {
#                         "$cond": [
#                             {"$eq": [{"$type": "$timestamp"}, "string"]},
#                             {"$dateFromString": {
#                                 "dateString": "$timestamp",
#                                 "timezone": "UTC",
#                                 "onError": None,
#                                 "onNull": None
#                             }},
#                             "$timestamp"
#                         ]
#                     },
#                     {"$toDate": "$_id"}
#                 ]
#             }
#         }},
#         {"$sort": {"_ts": -1, "_id": -1}},
#         {"$limit": 1},
#         {"$project": {"_id": 1, "pm_date": 1, "timestamp": 1}}
#     ]

#     cursor = coll.aggregate(pipeline)
#     docs = await cursor.to_list(length=1)
#     return docs[0] if docs else None

# async def _pmreport_latest_core(station_id: str, current: UserClaims):
#     # --- auth & validate ---
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
#     if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
#         raise HTTPException(status_code=400, detail="Bad station_id")

#     # 1) ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å stations
#     st = station_collection.find_one(
#         {"station_id": station_id},
#         {"_id": 1, "PIFirmware": 1, "PLCFirmware": 1, "RTFirmware": 1, "timestamp": 1, "updatedAt": 1}
#     )
#     if not st:
#         raise HTTPException(status_code=404, detail="Station not found")

#     pi_fw  = st.get("PIFirmware")
#     plc_fw = st.get("PLCFirmware")
#     rt_fw  = st.get("RTFirmware")

#     # 2) ‡∏î‡∏∂‡∏á pm_date ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å PMReportDB
#     pm_latest = await _latest_pm_date_from_pmreport(station_id)
#     pm_date = pm_latest.get("pm_date") if pm_latest else None

#     # ‡πÄ‡∏ß‡∏•‡∏≤: ‡πÉ‡∏ä‡πâ timestamp ‡∏à‡∏≤‡∏Å pm report ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô fallback ‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ
#     ts_raw = (pm_latest.get("timestamp") if pm_latest else None) or st.get("timestamp") or st.get("updatedAt")

#     ts_dt = (parse_iso_any_tz(ts_raw) if isinstance(ts_raw, str)
#              else (ts_raw if isinstance(ts_raw, datetime) else None))
#     ts_utc = ts_dt.astimezone(ZoneInfo("UTC")).isoformat() if ts_dt else None
#     ts_th  = ts_dt.astimezone(ZoneInfo("Asia/Bangkok")).isoformat() if ts_dt else None

#     pm_next_date = _compute_next_pm_date_str(pm_date)

#     return {
#         "_id": str(st["_id"]),
#         "pi_firmware": pi_fw,
#         "plc_firmware": plc_fw,
#         "rt_firmware": rt_fw,
#         "pm_date": pm_date,              # ‚Üê ‡∏°‡∏≤‡∏à‡∏≤‡∏Å PMReportDB
#         "pm_next_date": pm_next_date, 
#         "timestamp": ts_raw,             # pmreport.timestamp ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
#         "timestamp_utc": ts_utc,
#         "timestamp_th": ts_th,
#         "source": "stations + PMReportDB",  # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠ debug
#     }

# @app.get("/pmreport/get")
# async def pmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
#     coll = get_pmreport_collection_for(station_id)
#     doc = await coll.find_one({"_id": ObjectId(report_id)})
#     if not doc:
#         raise HTTPException(status_code=404, detail="not found")

#     doc["_id"] = str(doc["_id"])
#     return doc

# @app.get("/pmreport/list")
# async def pmreport_list(
#     station_id: str = Query(...),
#     page: int = Query(1, ge=1),
#     pageSize: int = Query(20, ge=1, le=100),
# ):
#     coll = get_pmreport_collection_for(station_id)
#     skip = (page - 1) * pageSize

#     cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side" : 1, "createdAt": 1}).sort(
#         [("createdAt", -1), ("_id", -1)]
#     ).skip(skip).limit(pageSize)
#     items_raw = await cursor.to_list(length=pageSize)
#     total = await coll.count_documents({})

#     # --- ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å PMReportURL ‡πÇ‡∏î‡∏¢ map ‡∏î‡πâ‡∏ß‡∏¢ pm_date (string) ---
#     pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
#     urls_coll = get_pmurl_coll_upload(station_id)
#     url_by_day: dict[str, str] = {}

#     if pm_dates:
#         ucur = urls_coll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
#         url_docs = await ucur.to_list(length=10_000)
#         for u in url_docs:
#             day = u.get("pm_date")
#             first_url = (u.get("urls") or [None])[0]
#             if day and first_url and day not in url_by_day:
#                 url_by_day[day] = first_url

#     items = [{
#         "id": str(it["_id"]),
#         "issue_id": it.get("issue_id"),
#         "doc_name": it.get("doc_name"),
#         "pm_date": it.get("pm_date"),
#         "inspector": it.get("inspector"),
#         "side" : it.get("side"),
#         "createdAt": _ensure_utc_iso(it.get("createdAt")),
#         "file_url": url_by_day.get(it.get("pm_date") or "", ""),
#     } for it in items_raw]

#     pm_date_arr = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
#     return {"items": items, "pm_date": pm_date_arr, "page": page, "pageSize": pageSize, "total": total}

# # ‡πÄ‡∏î‡∏¥‡∏° (path param) ‚Üí ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å helper
# @app.get("/pmreport/latest/{station_id}")
# async def pmreport_latest(station_id: str, current: UserClaims = Depends(get_current_user)):
#     return await _pmreport_latest_core(station_id, current)

# # ‡πÉ‡∏´‡∏°‡πà (query param) ‚Üí ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö /pmreport/latest/?station_id=...
# @app.get("/pmreport/latest/")
# async def pmreport_latest_q(
#     station_id: str = Query(..., description="‡πÄ‡∏ä‡πà‡∏ô Klongluang3"),
#     current: UserClaims = Depends(get_current_user),
# ):
#     return await _pmreport_latest_core(station_id, current)

# class PMMeasureRow(BaseModel):
#     value: str = ""
#     unit: str = "V"

# class PMMeasures(BaseModel):
#     m16: Dict[str, PMMeasureRow] = Field(default_factory=dict)  # L1-L2, L2-L3, ...
#     cp: PMMeasureRow = PMMeasureRow()

# class PMRowPF(BaseModel):
#     pf: Optional[Literal["PASS","FAIL","NA",""]] = ""
#     remark: Optional[str] = ""

# class PMSubmitIn(BaseModel):
#     side: Literal["pre", "post"]
#     station_id: str
#     job: dict
#     measures_pre: dict
#     rows_pre: Optional[dict[str, Any]] = None
#     pm_date:str
#     issue_id: Optional[str] = None
#     doc_name: Optional[str] = None 
#     inspector: Optional[str] = None 

# async def _latest_issue_id_anywhere(
#     station_id: str,
#     pm_type: str,
#     d: date,
#     source: Literal["charger", "mdb", "ccb", "cbbox", "station"] = "charger",
# ) -> str | None:
#     """
#     source = "pm"    -> ‡πÉ‡∏ä‡πâ get_pmreport_collection_for + get_pmurl_coll_upload
#     source = "mdbpm" -> ‡πÉ‡∏ä‡πâ get_mdbpmreport_collection_for + get_mdbpmurl_coll_upload
#     """
#     if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
#         raise HTTPException(status_code=400, detail="Bad station_id")

#     yymm = f"{d.year % 100:02d}{d.month:02d}"
#     prefix = f"PM-{pm_type}-{yymm}-"

#     if source == "charger":
#         rep_coll = get_pmreport_collection_for(station_id)
#         url_coll = get_pmurl_coll_upload(station_id)
#     elif source == "mdb": 
#         rep_coll = get_mdbpmreport_collection_for(station_id)
#         url_coll = get_mdbpmurl_coll_upload(station_id)
#     elif source == "ccb": 
#         rep_coll = get_ccbpmreport_collection_for(station_id)
#         url_coll = get_ccbpmurl_coll_upload(station_id)
#     elif source == "cbbox": 
#         rep_coll = get_cbboxpmreport_collection_for(station_id)
#         url_coll = get_cbboxpmurl_coll_upload(station_id)
#     elif source == "station": 
#         rep_coll = get_stationpmreport_collection_for(station_id)
#         url_coll = get_stationpmurl_coll_upload(station_id)

#     pipeline = [
#         {"$match": {"issue_id": {"$regex": f"^{prefix}\\d+$"}}},
#         {"$project": {"issue_id": 1}},
#     ]

#     rep_docs = await rep_coll.aggregate(pipeline).to_list(length=1000)
#     url_docs = await url_coll.aggregate(pipeline).to_list(length=1000)

#     best = None
#     best_n = 0

#     for ddoc in rep_docs + url_docs:
#         s = ddoc.get("issue_id") or ""
#         m = re.search(r"(\d+)$", s)
#         if not m:
#             continue
#         n = int(m.group(1))
#         if n > best_n:
#             best_n = n
#             best = s

#     return best

# async def _next_issue_id(db, station_id: str, pm_type: str, d, pad: int = 2) -> str:
#     yymm = f"{d.year % 100:02d}{d.month:02d}"
#     seq = await db.pm_sequences.find_one_and_update(
#         {"station_id": station_id, "pm_type": pm_type, "yymm": yymm},
#         {"$inc": {"n": 1}, "$setOnInsert": {"createdAt": datetime.now(timezone.utc)}},
#         upsert=True,
#         return_document=ReturnDocument.AFTER,
#     )
#     return f"PM-{pm_type}-{yymm}-{int(seq['n']):0{pad}d}"

# @app.get("/pmreport/preview-issueid")
# async def pmreport_preview_issueid(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     """
#     ‡∏î‡∏π issue_id ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (PM-CG-YYMM-XX) ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á
#     ‡πÉ‡∏ä‡πâ‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡πÑ‡∏õ‡πÇ‡∏ä‡∏ß‡πå‡∏ö‡∏ô‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÄ‡∏â‡∏¢ ‡πÜ
#     """
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     pm_type = "CG"

#     latest = await _latest_issue_id_anywhere(station_id, pm_type, d)

#     yymm = f"{d.year % 100:02d}{d.month:02d}"
#     prefix = f"PM-{pm_type}-{yymm}-"

#     if not latest:
#         next_issue = f"{prefix}01"
#     else:
#         m = re.search(r"(\d+)$", latest)
#         cur = int(m.group(1)) if m else 0
#         next_issue = f"{prefix}{cur+1:02d}"

#     return {"issue_id": next_issue}

# async def _latest_doc_name_from_pmreport(
#     station_id: str,
#     pm_date: str,
#     source: Literal["charger", "mdb", "ccb", "cbbox", "station"] = "charger",
# ) -> dict | None:
#     """
#     ‡∏î‡∏∂‡∏á doc_name ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å PMReportDB/<station_id> ‡∏´‡∏£‡∏∑‡∏≠ MDBPMReportDB/<station_id>
#     ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö pm_date

#     source = "pm"    -> ‡πÉ‡∏ä‡πâ PMReportDB
#     source = "mdbpm" -> ‡πÉ‡∏ä‡πâ MDBPMReportDB
#     """
#     if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
#         raise HTTPException(status_code=400, detail="Bad station_id")
    
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#         year = d.year
#     except ValueError:
#         return None
    
#     # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å DB ‡∏ï‡∏≤‡∏° source
#     if source == "charger":
#         coll = PMReportDB.get_collection(str(station_id))
#     elif source == "mdb": 
#         coll = MDBPMReportDB.get_collection(str(station_id))
#     elif source == "ccb": 
#         coll = CCBPMReportDB.get_collection(str(station_id))
#     elif source == "cbbox": 
#         coll = CBBOXPMReportDB.get_collection(str(station_id))
#     elif source == "station": 
#         coll = stationPMReportDB.get_collection(str(station_id))
    
#     pipeline = [
#         {
#             "$match": {
#                 "doc_name": {"$regex": f"^{station_id}_\\d+/{year}$"}
#             }
#         },
#         {"$sort": {"_id": -1}},
#         {"$limit": 1},
#         {"$project": {"_id": 1, "doc_name": 1}}
#     ]
    
#     cursor = coll.aggregate(pipeline)
#     docs = await cursor.to_list(length=1)
#     return docs[0] if docs else None

# async def _latest_doc_name_anywhere(
#     station_id: str,
#     year: int,
#     source: Literal["charger", "mdb", "ccb", "cbbox", "station"] = "charger",
# ) -> str | None:
#     pattern = f"^{station_id}_\\d+/{year}$"

#     # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å collection ‡∏ï‡∏≤‡∏° source
#     if source == "charger":
#         rep_coll = get_pmreport_collection_for(station_id)
#         url_coll = get_pmurl_coll_upload(station_id)
#     elif source == "mdb": 
#         rep_coll = get_mdbpmreport_collection_for(station_id)
#         url_coll = get_mdbpmurl_coll_upload(station_id)
#     elif source == "ccb": 
#         rep_coll = get_ccbpmreport_collection_for(station_id)
#         url_coll = get_ccbpmurl_coll_upload(station_id)
#     elif source == "cbbox": 
#         rep_coll = get_cbboxpmreport_collection_for(station_id)
#         url_coll = get_cbboxpmurl_coll_upload(station_id)
#     elif source == "station": 
#         rep_coll = get_stationpmreport_collection_for(station_id)
#         url_coll = get_stationpmurl_coll_upload(station_id)

#     pipeline = [
#         {"$match": {"doc_name": {"$regex": pattern}}},
#         {"$project": {"doc_name": 1}},
#     ]

#     rep_docs = await rep_coll.aggregate(pipeline).to_list(length=1000)
#     url_docs = await url_coll.aggregate(pipeline).to_list(length=1000)

#     best_seq = 0
#     best_name = None

#     for d in rep_docs + url_docs:
#         name = d.get("doc_name") or ""
#         m = re.search(r"_(\d+)/\d{4}$", name)
#         if not m:
#             continue
#         seq = int(m.group(1))
#         if seq > best_seq:
#             best_seq = seq
#             best_name = name

#     return best_name

# @app.get("/pmreport/latest-docname")
# async def pmreport_latest_docname(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     """
#     ‡∏î‡∏∂‡∏á doc_name ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏õ‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö pm_date)
#     ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏•‡∏Ç‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà frontend
#     """
#     # auth ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    
#     latest = await _latest_doc_name_from_pmreport(station_id, pm_date)
    
#     return {
#         "doc_name": latest.get("doc_name") if latest else None,
#         "station_id": station_id,
#         "pm_date": pm_date
#     }

# async def _next_year_seq(
#     db,
#     station_id: str,
#     d: date,
#     kind: Literal["pm", "cm"] = "pm",
#     pm_type: str | None = None,
# ) -> int:
#     """
#     ‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏õ‡∏µ ‡∏ï‡πà‡∏≠ station_id + (pm_type) + year

#     PM: ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° station_id + pm_type + year
#         ‡πÄ‡∏ä‡πà‡∏ô Klongluang3 + CG + 2025 ‚Üí 1, 2, 3, ...

#     CM: ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° station_id + year ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
#         ‡πÄ‡∏ä‡πà‡∏ô Klongluang3 + 2025 ‚Üí 1, 2, 3, ...
#     """
#     year = d.year
#     seq = await db.pm_year_sequences.find_one_and_update(
#         {"station_id": station_id, "pm_type": pm_type, "year": year},
#         {"$inc": {"n": 1}, "$setOnInsert": {"createdAt": datetime.now(timezone.utc)}},
#         upsert=True,
#         return_document=ReturnDocument.AFTER,
#     )
#     return int(seq["n"])

# @app.get("/pmreport/preview-docname")
# async def preview_docname(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     year = d.year

#     latest = await _latest_doc_name_anywhere(station_id, year)

#     if not latest:
#         next_doc = f"{station_id}_1/{year}"
#     else:
#         import re
#         m = re.search(r"_(\d+)/\d{4}$", latest)
#         current_num = int(m.group(1)) if m else 0
#         next_doc = f"{station_id}_{current_num + 1}/{year}"

#     return {"doc_name": next_doc}

# @app.post("/pmreport/pre/submit")
# async def pmreport_submit(body: PMSubmitIn, current: UserClaims = Depends(get_current_user)):
#     station_id = body.station_id.strip()
#     coll = get_pmreport_collection_for(station_id)
#     db = coll.database

#     pm_type = str(body.job.get("pm_type") or "CG").upper()
#     body.job["pm_type"] = pm_type

#     url_coll = get_pmurl_coll_upload(station_id)

#     try:
#         d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     client_issue = body.issue_id
#     issue_id: str | None = None    

#     if client_issue:
#         yymm = f"{d.year % 100:02d}{d.month:02d}"
#         prefix = f"PM-{pm_type}-{yymm}-"
#         valid_fmt = client_issue.startswith(prefix)
#         rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
#         url_exists = await url_coll.find_one({"issue_id": client_issue})
#         unique = not (rep_exists or url_exists)

#         if valid_fmt and unique:
#             issue_id = client_issue
    
#     if not issue_id:
#         while True:
#             candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
#             rep_exists = await coll.find_one({"issue_id": candidate})
#             url_exists = await url_coll.find_one({"issue_id": candidate})
#             if not rep_exists and not url_exists:
#                 issue_id = candidate
#                 break

#     client_docName = body.doc_name
#     doc_name = None
#     if client_docName:
#         year = f"{d.year}"
#         prefix = f"{station_id}_"
#         valid_fmt = client_docName.startswith(prefix)

#         url_coll = get_pmurl_coll_upload(station_id)
#         rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
#         url_exists = await url_coll.find_one({"doc_name": client_docName})
#         unique = not (rep_exists or url_exists)

#         if valid_fmt and unique:
#             doc_name = client_docName
 
#     if not doc_name:
#         year_seq = await _next_year_seq(db, station_id, pm_type, d)
#         year = d.year
#         doc_name = f"{station_id}_{year_seq}/{year}"

#     inspector = body.inspector
#     doc = {
#         "station_id": station_id,
#         "doc_name": doc_name,
#         "issue_id": issue_id,
#         "job": body.job,
#         "rows_pre": body.rows_pre or {},      # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏° - ‡πÄ‡∏Å‡πá‡∏ö pf ‡πÅ‡∏•‡∏∞ remark ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ Pre
#         "measures_pre": body.measures_pre,
#         "pm_date": body.pm_date,
#         "inspector": inspector,
#         "photos_pre": {},
#         "status": "draft",
#         "side": body.side,
#         "timestamp": datetime.now(timezone.utc),
#     }
#     res = await coll.insert_one(doc)
#     return {
#         "ok": True,
#         "report_id": str(res.inserted_id),
#         "issue_id": issue_id,
#         "doc_name": doc_name,
#     }

# class PMPostIn(BaseModel):
#     report_id: str | None = None      # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°
#     station_id: str
#     # issue_id: str | None = None
#     # job: dict
#     rows: dict
#     measures: dict
#     summary: str
#     # pm_date: str
#     # doc_name: str | None = None
#     summaryCheck: str | None = None
#     dust_filter: Dict[str, bool] | None = None
#     side: Literal["post", "after"]

# @app.post("/pmreport/submit")
# async def pmreport_submit(
#     body: PMPostIn,
#     current: UserClaims = Depends(get_current_user)
# ):
#     station_id = body.station_id.strip()
#     coll = get_pmreport_collection_for(station_id)
#     db = coll.database
#     url_coll = get_pmurl_coll_upload(station_id)

#     # pm_type = str(body.job.get("pm_type") or "CG").upper()
#     # body.job["pm_type"] = pm_type

#     # try:
#     #     d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
#     # except ValueError:
#     #     raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     # ---------- ‡∏Å‡∏£‡∏ì‡∏µ 1: ‡∏°‡∏µ report_id ‚Üí UPDATE doc ‡πÄ‡∏î‡∏¥‡∏° (pre+post ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß) ----------
#     if body.report_id:
#         try:
#             oid = ObjectId(body.report_id)
#         except InvalidId:
#             raise HTTPException(status_code=400, detail="invalid report_id")

#         existing = await coll.find_one({"_id": oid, "station_id": station_id})
#         if not existing:
#             raise HTTPException(status_code=404, detail="Report not found")

#         # reuse ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° ‡πÑ‡∏°‡πà gen ‡πÉ‡∏´‡∏°‡πà
#         # issue_id = existing.get("issue_id")
#         # doc_name = existing.get("doc_name")
#         # inspector = body.inspector or existing.get("inspector") or current.username

#         update_fields = {
#             # "job": body.job,
#             "rows": body.rows,
#             "measures": body.measures,          # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á PM
#             "summary": body.summary,
#             "summaryCheck": body.summaryCheck,
#             # "pm_date": body.pm_date,
#             # "inspector": inspector,
#             "dust_filter": body.dust_filter,
#             # "doc_name": doc_name,
#             "side": "post",                     # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ù‡∏±‡πà‡∏á post ‡πÅ‡∏•‡πâ‡∏ß
#             "timestamp_post": datetime.now(timezone.utc),
#         }

#         await coll.update_one({"_id": oid}, {"$set": update_fields})

#         return {
#             "ok": True,
#             "report_id": body.report_id,
#             # "issue_id": issue_id,
#             # "doc_name": doc_name,
#         }

#     doc = {
#         "station_id": station_id,
#         # "issue_id": issue_id,
#         # "job": body.job,
#         "rows": body.rows,
#         "measures": body.measures,
#         "summary": body.summary,
#         "summaryCheck": body.summaryCheck,
#         # "pm_date": body.pm_date,
#         # "inspector": inspector,
#         "dust_filter": body.dust_filter,
#         # "doc_name": doc_name,
#         "photos": {},
#         "status": "draft",
#         "side": "post",
#         "timestamp": datetime.now(timezone.utc),
#     }
#     res = await coll.insert_one(doc)
#     return {
#         "ok": True,
#         "report_id": str(res.inserted_id),
#         # "issue_id": issue_id,
#         # "doc_name": doc_name,
#     }

# # ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå
# UPLOADS_ROOT = os.getenv("UPLOADS_ROOT", "./uploads")
# os.makedirs(UPLOADS_ROOT, exist_ok=True)

# # ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏∑‡∏ô‡πÉ‡∏´‡πâ Frontend ‡∏ú‡πà‡∏≤‡∏ô /uploads/...
# app.mount("/uploads", StaticFiles(directory=UPLOADS_ROOT, html=False), name="uploads")

# ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif"}
# MAX_FILE_MB = 10

# def _safe_name(name: str) -> str:
#     # ‡∏Å‡∏±‡∏ô path traversal ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏õ‡∏•‡∏Å ‡πÜ
#     base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
#     return base[:120] or secrets.token_hex(4)

# def _ext(fname: str) -> str:
#     return (fname.rsplit(".",1)[-1].lower() if "." in fname else "")

# @app.post("/pmreport/{report_id}/pre/photos")
# async def pmreport_upload_photos(
#     report_id: str,
#     station_id: str = Form(...),
#     group: str = Form(...),                   # ‡πÄ‡∏ä‡πà‡∏ô "g1", "g3_0", "g10_1"
#     files: list[UploadFile] = File(...),
# ):
#     # ‡πÅ‡∏Å‡πâ regex ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö g1, g3_0, g10_1
#     if not re.fullmatch(r"g\d+(_\d+)?", group):
#         raise HTTPException(status_code=400, detail="Bad group key")

#     coll = get_pmreport_collection_for(station_id)
#     from bson import ObjectId
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")


#     # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô station ‡∏ô‡∏µ‡πâ
#     doc = await coll.find_one({"_id": oid}, {"_id":1, "station_id":1})
#     if not doc:
#         raise HTTPException(status_code=404, detail="Report not found")
#     if doc.get("station_id") != station_id:
#         raise HTTPException(status_code=400, detail="station_id mismatch")

#     # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "pm" / station_id / report_id / "pre" / group
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     saved = []
#     total = 0
#     for f in files:
#         ext = _ext(f.filename or "")
#         if ext not in ALLOWED_EXTS:
#             raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

#         data = await f.read()
#         total += len(data)
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         # URL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏ô Frontend
#         url_path = f"/uploads/pm/{station_id}/{report_id}/pre/{group}/{fname}"
#         saved.append({
#             "filename": fname,
#             "size": len(data),
#             "url": url_path,
#             # "remark": remark or "",
#             "uploadedAt": datetime.now(timezone.utc)
#         })

#     # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PMReport: push ‡∏•‡∏á photos.<group>
#     await coll.update_one(
#         {"_id": oid},
#         {"$push": {f"photos_pre.{group}": {"$each": saved}}}
#     )

#     return {"ok": True, "count": len(saved), "group": group, "files": saved}


# @app.post("/pmreport/{report_id}/post/photos")
# async def pmreport_upload_photos(
#     report_id: str,
#     station_id: str = Form(...),
#     group: str = Form(...),                   # ‡πÄ‡∏ä‡πà‡∏ô "g1", "g3_0", "g10_1", "g11_0"
#     files: list[UploadFile] = File(...),
#     remark: str | None = Form(None),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
#     if not re.fullmatch(r"g\d+(_\d+)?", group):
#         raise HTTPException(status_code=400, detail="Bad group key")

#     coll = get_pmreport_collection_for(station_id)
#     from bson import ObjectId
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô station ‡∏ô‡∏µ‡πâ
#     doc = await coll.find_one({"_id": oid}, {"_id":1, "station_id":1})
#     if not doc:
#         raise HTTPException(status_code=404, detail="Report not found")
#     if doc.get("station_id") != station_id:
#         raise HTTPException(status_code=400, detail="station_id mismatch")

#     # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "pm"  / station_id / report_id / "post" /  group
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     saved = []
#     total = 0
#     for f in files:
#         ext = _ext(f.filename or "")
#         if ext not in ALLOWED_EXTS:
#             raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

#         data = await f.read()
#         total += len(data)
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         # URL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏ô Frontend
#         url_path = f"/uploads/pm/{station_id}/{report_id}/post/{group}/{fname}"
#         saved.append({
#             "filename": fname,
#             "size": len(data),
#             "url": url_path,
#             "remark": remark or "",
#             "uploadedAt": datetime.now(timezone.utc)
#         })

#     # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PMReport: push ‡∏•‡∏á photos.<group>
#     await coll.update_one(
#         {"_id": oid},
#         {"$push": {f"photos.{group}": {"$each": saved}}}
#     )

#     return {"ok": True, "count": len(saved), "group": group, "files": saved}

# @app.post("/pmreport/{report_id}/finalize")
# async def pmreport_finalize(
#     report_id: str,
#     station_id: str = Form(...),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")

#     coll = get_pmreport_collection_for(station_id)
    
#     from bson import ObjectId
#     oid = ObjectId(report_id)
#     res = await coll.update_one(
#         {"_id": oid},
#         {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc)}}
#     )
#     if res.matched_count == 0:
#         raise HTTPException(status_code=404, detail="Report not found")
#     return {"ok": True}

# def parse_report_date_to_utc(s: str) -> datetime:
#     # 'YYYY-MM-DD' => ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô‡∏ß‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô UTC
#     if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
#         tz_th = ZoneInfo("Asia/Bangkok")
#         dt_th = datetime.fromisoformat(s + "T00:00:00").replace(tzinfo=tz_th)
#         return dt_th.astimezone(timezone.utc)
#     # ISO ‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢ Z ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏≠‡∏≠‡∏ü‡πÄ‡∏ã‡πá‡∏ï
#     if s.endswith("Z"):
#         return datetime.fromisoformat(s.replace("Z", "+00:00")).astimezone(timezone.utc)
#     if re.search(r"[+\-]\d{2}:\d{2}$", s):
#         return datetime.fromisoformat(s).astimezone(timezone.utc)
#     # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô ‚Üí ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢
#     return datetime.fromisoformat(s + "+07:00").astimezone(timezone.utc)

# def get_pmurl_coll_upload(station_id: str):
#     _validate_station_id(station_id)
#     coll = PMUrlDB.get_collection(str(station_id))
#     # # ‡πÄ‡∏Å‡πá‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ö‡∏ö Date ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏ß‡πâ query ‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
#     # try:
#     #     coll.create_index([("reportDate", 1)])
#     #     coll.create_index([("createdAt", -1), ("_id", -1)])
#     # except Exception:
#     #     pass
#     return coll

# # --- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö PDF ---
# ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif","pdf"}  # <<-- ‡πÄ‡∏û‡∏¥‡πà‡∏° pdf
# MAX_FILE_MB = 20  # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏∂‡πâ‡∏ô

# def _safe_name(name: str) -> str:
#     base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
#     return base[:120] or secrets.token_hex(4)

# def normalize_pm_date(s: str) -> str:
#     """
#     ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á:
#       - 'YYYY-MM-DD'           -> ‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏î‡∏¥‡∏°
#       - ISO (‡∏°‡∏µ Z/offset ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ) -> ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏∑‡∏ô date().isoformat()
#     ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô 'YYYY-MM-DD' (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏ß‡∏•‡∏≤)
#     """
#     if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
#         return s
#     # ‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤
#     if s.endswith("Z") or re.search(r"[+\-]\d{2}:\d{2}$", s):
#         dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
#     else:
#         # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ -> ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢
#         dt = datetime.fromisoformat(s).replace(tzinfo=th_tz)
#     return dt.astimezone(th_tz).date().isoformat()

# @app.post("/pmurl/upload-files", status_code=201)
# async def pmurl_upload_files(
#     station_id: str = Form(...),
#     reportDate: str = Form(...),                 # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO
#     files: list[UploadFile] = File(...),
#     issue_id: Optional[str] = Form(None),
#     doc_name: Optional[str] = Form(None),
#     inspector: Optional[str] = Form(None),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # auth (‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏Å‡πá‡πÄ‡∏≠‡∏≤ comment ‡∏≠‡∏≠‡∏Å)
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")

#     # ‡∏ï‡∏£‡∏ß‡∏à/‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô
#     coll = get_pmurl_coll_upload(station_id)

#     # ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å form ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô 'YYYY-MM-DD' (‡πÑ‡∏ó‡∏¢) ‡∏î‡πâ‡∏ß‡∏¢ helper ‡πÄ‡∏î‡∏¥‡∏°
#     pm_date = normalize_pm_date(reportDate)

#     # parse ‡πÄ‡∏õ‡πá‡∏ô date object ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏´‡∏≤ seq
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

#     # ‡∏ä‡∏ô‡∏¥‡∏î PM (‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ fix ‡πÄ‡∏õ‡πá‡∏ô CG)
#     pm_type = "CG"

#     # -------------------------
#     # 1) ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏Å issue_id
#     # -------------------------
#     rep_coll = get_pmreport_collection_for(station_id)
#     final_issue_id: str | None = None
#     client_issue = (issue_id or "").strip()

#     if client_issue:
#         yymm = f"{d.year % 100:02d}{d.month:02d}"
#         prefix = f"PM-{pm_type}-{yymm}-"
#         valid_fmt = client_issue.startswith(prefix)

#         url_exists = await coll.find_one({"issue_id": client_issue})
#         rep_exists = await rep_coll.find_one({"issue_id": client_issue})
#         unique = not (url_exists or rep_exists)

#         if valid_fmt and unique:
#             final_issue_id = client_issue

#     if not final_issue_id:
#         while True:
#             candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
#             url_exists = await coll.find_one({"issue_id": candidate})
#             rep_exists = await rep_coll.find_one({"issue_id": candidate})
#             if not url_exists and not rep_exists:
#                 final_issue_id = candidate
#                 break

#     year_seq: int | None = None

#     # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° reuse year_seq ‡∏à‡∏≤‡∏Å PMReportDB ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (issue_id ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
#     rep = await get_pmreport_collection_for(station_id).find_one(
#         {"issue_id": final_issue_id},
#         {"year_seq": 1, "pm_date": 1},
#     )
#     if rep and rep.get("year_seq") is not None:
#         year_seq = int(rep["year_seq"])

#     # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ year_seq ‡∏à‡∏≤‡∏Å PMReport ‚Üí ‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å pm_year_sequences
#     if year_seq is None:
#         year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

#     year = d.year
#     final_doc_name: str | None = None

#     if doc_name:
#         candidate = doc_name.strip()

#         ok_format = candidate.startswith(f"{station_id}_")

#         rep_coll = get_pmreport_collection_for(station_id)
#         rep_exists = await rep_coll.find_one({"doc_name": candidate})
#         url_exists = await coll.find_one({"doc_name": candidate})
#         unique = not (rep_exists or url_exists)

#         if ok_format and unique:
#             final_doc_name = candidate

#     if not final_doc_name:
#         final_doc_name = f"{station_id}_{year_seq}/{year}"

#     doc_name = final_doc_name
#     # -------------------------
#     # 3) ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå PDF ‡∏•‡∏á‡∏î‡∏¥‡∏™‡∏Å‡πå
#     # -------------------------
#     subdir = pm_date  # ‡πÉ‡∏ä‡πâ YYYY-MM-DD ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏¢‡πà‡∏≠‡∏¢
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "pmurl" / station_id / subdir
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     urls: list[str] = []
#     metas: list[dict] = []
#     total_size = 0

#     for f in files:
#         # ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô PDF ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
#         ext = (f.filename.rsplit(".", 1)[-1].lower() if f.filename and "." in f.filename else "")
#         if ext not in ALLOWED_EXTS or ext != "pdf":
#             raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

#         data = await f.read()
#         total_size += len(data)
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(
#                 status_code=413,
#                 detail=f"File too large (> {MAX_FILE_MB} MB)"
#             )

#         safe = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
#         dest = dest_dir / safe
#         with open(dest, "wb") as out:
#             out.write(data)

#         url = f"/uploads/pmurl/{station_id}/{subdir}/{safe}"
#         urls.append(url)
#         metas.append({"name": f.filename, "size": len(data)})

#     inspector_clean = (inspector or "").strip() or None
#     # -------------------------
#     # 4) ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Mongo
#     # -------------------------
#     now = datetime.now(timezone.utc)
#     doc = {
#         "station": station_id,
#         "pm_date": pm_date,          # 'YYYY-MM-DD'
#         "issue_id": final_issue_id,  # PM-CG-YYMM-XX
#         "inspector": inspector_clean,
#         "year": year,                # 2025
#         "year_seq": year_seq,        # 1, 2, 3, ...
#         "doc_name": doc_name,            # Klongluang3_CG_1/2025
#         "urls": urls,
#         "meta": {"files": metas},
#         "source": "upload-files",
#         "createdAt": now,
#         "updatedAt": now,
#     }
#     res = await coll.insert_one(doc)

#     return {
#         "ok": True,
#         "inserted_id": str(res.inserted_id),
#         "count": len(urls),
#         "urls": urls,
#         "issue_id": final_issue_id,
#         "year_seq": year_seq,
#         "doc_name": doc_name,
#         "inspector": inspector_clean,
#     }

# @app.get("/pmurl/list")
# async def pmurl_list(
#     station_id: str = Query(...),
#     page: int = Query(1, ge=1),
#     pageSize: int = Query(20, ge=1, le=100),
# ):
#     """
#     ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå PM (PDF) ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡∏ï‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ ‡∏à‡∏≤‡∏Å PMUrlDB/<station_id>
#     - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö pm_date (string 'YYYY-MM-DD') ‡πÅ‡∏•‡∏∞ reportDate (Date/ISO)
#     - ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡πÄ‡∏Å‡πà‡∏≤ (createdAt desc, _id desc)
#     - ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô /pmreport/list (‡∏°‡∏µ file_url ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
#     """
#     coll = get_pmurl_coll_upload(station_id)
#     skip = (page - 1) * pageSize

#     # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
#     cursor = coll.find(
#         {},
#         {"_id": 1, "issue_id": 1, "doc_name": 1,"inspector":1, "pm_date": 1, "reportDate": 1, "urls": 1, "createdAt": 1}
#     ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

#     items_raw = await cursor.to_list(length=pageSize)
#     total = await coll.count_documents({})

#     def _pm_date_from(doc: dict) -> str | None:
#         """
#         ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ string 'YYYY-MM-DD'
#         - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ pm_date (string) ‚Üí ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
#         - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ reportDate (datetime/string) ‚Üí ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß .date().isoformat()
#         """
#         # ‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô pm_date (string)
#         s = doc.get("pm_date")
#         if isinstance(s, str) and re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
#             return s

#         # ‡∏£‡∏∏‡πà‡∏ô‡πÄ‡∏Å‡πà‡∏≤: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô reportDate (Date/ISO)
#         rd = doc.get("reportDate")
#         if isinstance(rd, datetime):
#             return rd.astimezone(th_tz).date().isoformat()
#         if isinstance(rd, str):
#             try:
#                 dt = datetime.fromisoformat(rd.replace("Z", "+00:00"))
#             except Exception:
#                 # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢
#                 try:
#                     dt = datetime.fromisoformat(rd).replace(tzinfo=th_tz)
#                 except Exception:
#                     return None
#             return dt.astimezone(th_tz).date().isoformat()

#         return None

#     items = []
#     pm_date_arr = []

#     for it in items_raw:
#         pm_date_str = _pm_date_from(it)
#         if pm_date_str:
#             pm_date_arr.append(pm_date_str)

#         urls = it.get("urls") or []
#         first_url = urls[0] if urls else ""

#         items.append({
#             "id": str(it["_id"]),
#             "pm_date": pm_date_str, 
#             "inspector": it.get(("inspector")), 
#             "doc_name": it.get("doc_name"),
#             "issue_id": it.get("issue_id"),                       # 'YYYY-MM-DD' | None
#             "createdAt": _ensure_utc_iso(it.get("createdAt")),
#             "file_url": first_url,                          # ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å (‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î)
#             "urls": urls,                                   # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå‡∏≠‡∏¢‡∏≤‡∏Å‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
#         })

#     return {
#         "items": items,
#         "pm_date": [d for d in pm_date_arr if d],          # ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô /pmreport/list
#         "page": page,
#         "pageSize": pageSize,
#         "total": total,
#     }


# -----------------------------------------------------------------------------------
# new PM Report (Charger)
# -----------------------------------------------------------------------------------
# ============================================================
# PM Report API - ‡πÉ‡∏ä‡πâ SN (Charger Serial Number) ‡πÅ‡∏ó‡∏ô station_id
# ============================================================

def _validate_sn(sn: str):
    """Validate SN format"""
    if not sn or not re.fullmatch(r"[A-Za-z0-9_\-]+", str(sn)):
        raise HTTPException(status_code=400, detail="Bad SN format")

def get_pmreport_collection_for(sn: str):
    """Get PM Report collection by Charger SN"""
    _validate_sn(sn)
    coll = PMReportDB.get_collection(str(sn))
    return coll

def get_pmurl_coll_upload(sn: str):
    """Get PM URL collection by Charger SN"""
    _validate_sn(sn)
    coll = PMUrlDB.get_collection(str(sn))
    return coll

async def _get_charger_by_sn(sn: str) -> dict:
    """Get charger document by SN, raise 404 if not found"""
    charger = charger_collection.find_one({"SN": sn})
    if not charger:
        raise HTTPException(status_code=404, detail=f"Charger with SN '{sn}' not found")
    return charger

def _compute_next_pm_date_str(pm_date_str: str | None) -> str | None:
    if not pm_date_str:
        return None
    try:
        d = datetime.fromisoformat(pm_date_str).date()
    except ValueError:
        return None
    next_d = d + relativedelta(months=+6)
    return next_d.isoformat()     

# --- helper: ‡πÄ‡∏≠‡∏≤ pm_date ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å PMReportDB/<sn> ---
async def _latest_pm_date_from_pmreport(sn: str) -> dict | None:
    _validate_sn(sn)
    coll = PMReportDB.get_collection(str(sn))

    pipeline = [
        {"$addFields": {
            "_ts": {
                "$ifNull": [
                    {
                        "$cond": [
                            {"$eq": [{"$type": "$timestamp"}, "string"]},
                            {"$dateFromString": {
                                "dateString": "$timestamp",
                                "timezone": "UTC",
                                "onError": None,
                                "onNull": None
                            }},
                            "$timestamp"
                        ]
                    },
                    {"$toDate": "$_id"}
                ]
            }
        }},
        {"$sort": {"_ts": -1, "_id": -1}},
        {"$limit": 1},
        {"$project": {"_id": 1, "pm_date": 1, "timestamp": 1}}
    ]

    cursor = coll.aggregate(pipeline)
    docs = await cursor.to_list(length=1)
    return docs[0] if docs else None

async def _pmreport_latest_core(sn: str, current: UserClaims):
    """Get latest PM report info for a charger by SN"""
    _validate_sn(sn)
    
    # 1) ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Charger ‡∏à‡∏≤‡∏Å SN
    charger = await _get_charger_by_sn(sn)
    
    pi_fw  = charger.get("PIFirmware")
    plc_fw = charger.get("PLCFirmware")
    rt_fw  = charger.get("RTFirmware")
    station_id = charger.get("station_id")

    # 2) ‡∏î‡∏∂‡∏á pm_date ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å PMReportDB
    pm_latest = await _latest_pm_date_from_pmreport(sn)
    pm_date = pm_latest.get("pm_date") if pm_latest else None

    # ‡πÄ‡∏ß‡∏•‡∏≤: ‡πÉ‡∏ä‡πâ timestamp ‡∏à‡∏≤‡∏Å pm report ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô fallback ‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á charger
    ts_raw = (pm_latest.get("timestamp") if pm_latest else None) or charger.get("createdAt")

    ts_dt = (parse_iso_any_tz(ts_raw) if isinstance(ts_raw, str)
             else (ts_raw if isinstance(ts_raw, datetime) else None))
    ts_utc = ts_dt.astimezone(ZoneInfo("UTC")).isoformat() if ts_dt else None
    ts_th  = ts_dt.astimezone(ZoneInfo("Asia/Bangkok")).isoformat() if ts_dt else None

    pm_next_date = _compute_next_pm_date_str(pm_date)

    return {
        "_id": str(charger["_id"]),
        "sn": sn,
        "station_id": station_id,
        "chargeBoxID": charger.get("chargeBoxID"),
        "pi_firmware": pi_fw,
        "plc_firmware": plc_fw,
        "rt_firmware": rt_fw,
        "pm_date": pm_date,
        "pm_next_date": pm_next_date, 
        "timestamp": ts_raw,
        "timestamp_utc": ts_utc,
        "timestamp_th": ts_th,
        "source": "chargers + PMReportDB",
    }

@app.get("/pmreport/get")
async def pmreport_get(sn: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    """Get a specific PM report by SN and report_id"""
    coll = get_pmreport_collection_for(sn)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    return doc

@app.get("/pmreport/list")
async def pmreport_list(
    sn: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    """List PM reports for a charger by SN"""
    coll = get_pmreport_collection_for(sn)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector": 1, "side": 1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)
    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # --- ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å PMReportURL ‡πÇ‡∏î‡∏¢ map ‡∏î‡πâ‡∏ß‡∏¢ pm_date (string) ---
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    urls_coll = get_pmurl_coll_upload(sn)
    url_by_day: dict[str, str] = {}

    if pm_dates:
        ucur = urls_coll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "pm_date": it.get("pm_date"),
        "inspector": it.get("inspector"),
        "side": it.get("side"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    pm_date_arr = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    return {"items": items, "pm_date": pm_date_arr, "page": page, "pageSize": pageSize, "total": total}

# ‡πÄ‡∏î‡∏¥‡∏° (path param)
@app.get("/pmreport/latest/{sn}")
async def pmreport_latest(sn: str, current: UserClaims = Depends(get_current_user)):
    """Get latest PM info for a charger by SN (path param)"""
    return await _pmreport_latest_core(sn, current)

# ‡πÉ‡∏´‡∏°‡πà (query param)
@app.get("/pmreport/latest/")
async def pmreport_latest_q(
    sn: str = Query(..., description="Charger Serial Number"),
    current: UserClaims = Depends(get_current_user),
):
    """Get latest PM info for a charger by SN (query param)"""
    return await _pmreport_latest_core(sn, current)

class PMMeasureRow(BaseModel):
    value: str = ""
    unit: str = "V"

class PMMeasures(BaseModel):
    m16: Dict[str, PMMeasureRow] = Field(default_factory=dict)
    cp: PMMeasureRow = PMMeasureRow()

class PMRowPF(BaseModel):
    pf: Optional[Literal["PASS","FAIL","NA",""]] = ""
    remark: Optional[str] = ""

class PMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    sn: str  # Changed from station_id to sn
    job: dict
    measures_pre: dict
    rows_pre: Optional[dict[str, Any]] = None
    pm_date: str
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None 
    inspector: Optional[str] = None 

async def _latest_issue_id_anywhere(
    identifier: str,  # sn ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö charger, station_id ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏∑‡πà‡∏ô‡πÜ
    pm_type: str,
    d: date,
    source: Literal["charger", "mdb", "ccb", "cbbox", "station"] = "charger",
) -> str | None:
    """Find latest issue_id from both report and URL collections"""
    if source == "charger":
        _validate_sn(identifier)
    else:
        _validate_station_id(identifier)

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if source == "charger":
        rep_coll = get_pmreport_collection_for(identifier)          # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
        url_coll = get_pmurl_coll_upload(identifier)                # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
    elif source == "mdb": 
        rep_coll = get_mdbpmreport_collection_for(identifier)       # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
        url_coll = get_mdbpmurl_coll_upload(identifier)             # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
    elif source == "ccb": 
        rep_coll = get_ccbpmreport_collection_for(identifier)       # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
        url_coll = get_ccbpmurl_coll_upload(identifier)             # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
    elif source == "cbbox": 
        rep_coll = get_cbboxpmreport_collection_for(identifier)     # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
        url_coll = get_cbboxpmurl_coll_upload(identifier)           # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
    elif source == "station": 
        rep_coll = get_stationpmreport_collection_for(identifier)   # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
        url_coll = get_stationpmurl_coll_upload(identifier)         # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn

    pipeline = [
        {"$match": {"issue_id": {"$regex": f"^{prefix}\\d+$"}}},
        {"$project": {"issue_id": 1}},
    ]

    rep_docs = await rep_coll.aggregate(pipeline).to_list(length=1000)
    url_docs = await url_coll.aggregate(pipeline).to_list(length=1000)

    best = None
    best_n = 0

    for ddoc in rep_docs + url_docs:
        s = ddoc.get("issue_id") or ""
        m = re.search(r"(\d+)$", s)
        if not m:
            continue
        n = int(m.group(1))
        if n > best_n:
            best_n = n
            best = s

    return best

async def _next_issue_id(db, sn: str, pm_type: str, d, pad: int = 2) -> str:
    """Generate next issue_id for a charger"""
    yymm = f"{d.year % 100:02d}{d.month:02d}"
    seq = await db.pm_sequences.find_one_and_update(
        {"sn": sn, "pm_type": pm_type, "yymm": yymm},
        {"$inc": {"n": 1}, "$setOnInsert": {"createdAt": datetime.now(timezone.utc)}},
        upsert=True,
        return_document=ReturnDocument.AFTER,
    )
    return f"PM-{pm_type}-{yymm}-{int(seq['n']):0{pad}d}"

@app.get("/pmreport/preview-issueid")
async def pmreport_preview_issueid(
    sn: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """Preview next issue_id without actually generating it"""
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "CG"

    latest = await _latest_issue_id_anywhere(sn, pm_type, d)

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

async def _latest_doc_name_from_pmreport(
    identifier: str,
    pm_date: str,
    source: Literal["charger", "mdb", "ccb", "cbbox", "station"] = "charger",
) -> dict | None:
    """Get latest doc_name from PMReportDB for the same year"""
    if source == "charger":
        _validate_sn(identifier)
    else:
        _validate_station_id(identifier)
    
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
        year = d.year
    except ValueError:
        return None
    
    if source == "charger":
        coll = PMReportDB.get_collection(str(identifier))           # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
    elif source == "mdb": 
        coll = MDBPMReportDB.get_collection(str(identifier))        # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
    elif source == "ccb": 
        coll = CCBPMReportDB.get_collection(str(identifier))        # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
    elif source == "cbbox": 
        coll = CBBOXPMReportDB.get_collection(str(identifier))      # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
    elif source == "station": 
        coll = stationPMReportDB.get_collection(str(identifier))    # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
    
    pipeline = [
        {
            "$match": {
                "doc_name": {"$regex": f"^{identifier}_\\d+/{year}$"}  # ‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å sn
            }
        },
        {"$sort": {"_id": -1}},
        {"$limit": 1},
        {"$project": {"_id": 1, "doc_name": 1}}
    ]
    
    cursor = coll.aggregate(pipeline)
    docs = await cursor.to_list(length=1)
    return docs[0] if docs else None

async def _latest_doc_name_anywhere(
    sn: str,
    year: int,
    source: Literal["charger", "mdb", "ccb", "cbbox", "station"] = "charger",
) -> str | None:
    """Find latest doc_name from both report and URL collections"""
    pattern = f"^{sn}_\\d+/{year}$"

    if source == "charger":
        rep_coll = get_pmreport_collection_for(sn)
        url_coll = get_pmurl_coll_upload(sn)
    elif source == "mdb": 
        rep_coll = get_mdbpmreport_collection_for(sn)
        url_coll = get_mdbpmurl_coll_upload(sn)
    elif source == "ccb": 
        rep_coll = get_ccbpmreport_collection_for(sn)
        url_coll = get_ccbpmurl_coll_upload(sn)
    elif source == "cbbox": 
        rep_coll = get_cbboxpmreport_collection_for(sn)
        url_coll = get_cbboxpmurl_coll_upload(sn)
    elif source == "station": 
        rep_coll = get_stationpmreport_collection_for(sn)
        url_coll = get_stationpmurl_coll_upload(sn)

    pipeline = [
        {"$match": {"doc_name": {"$regex": pattern}}},
        {"$project": {"doc_name": 1}},
    ]

    rep_docs = await rep_coll.aggregate(pipeline).to_list(length=1000)
    url_docs = await url_coll.aggregate(pipeline).to_list(length=1000)

    best_seq = 0
    best_name = None

    for d in rep_docs + url_docs:
        name = d.get("doc_name") or ""
        m = re.search(r"_(\d+)/\d{4}$", name)
        if not m:
            continue
        seq = int(m.group(1))
        if seq > best_seq:
            best_seq = seq
            best_name = name

    return best_name

@app.get("/pmreport/latest-docname")
async def pmreport_latest_docname(
    sn: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """Get latest doc_name for calculating next number at frontend"""
    latest = await _latest_doc_name_from_pmreport(sn, pm_date)
    
    return {
        "doc_name": latest.get("doc_name") if latest else None,
        "sn": sn,
        "pm_date": pm_date
    }

async def _next_year_seq(
    db,
    sn: str,
    pm_type: str,
    d: date,
) -> int:
    """Generate next year sequence number for a charger"""
    year = d.year
    seq = await db.pm_year_sequences.find_one_and_update(
        {"sn": sn, "pm_type": pm_type, "year": year},
        {"$inc": {"n": 1}, "$setOnInsert": {"createdAt": datetime.now(timezone.utc)}},
        upsert=True,
        return_document=ReturnDocument.AFTER,
    )
    return int(seq["n"])

@app.get("/pmreport/preview-docname")
async def preview_docname(
    sn: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """Preview next doc_name without actually generating it"""
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(sn, year)

    if not latest:
        next_doc = f"{sn}_1/{year}"
    else:
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{sn}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

@app.post("/pmreport/pre/submit")
async def pmreport_pre_submit(body: PMSubmitIn, current: UserClaims = Depends(get_current_user)):
    """Submit Pre-PM report for a charger"""
    sn = body.sn.strip()
    
    # Validate charger exists
    charger = await _get_charger_by_sn(sn)
    station_id = charger.get("station_id")
    
    coll = get_pmreport_collection_for(sn)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "CG").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_pmurl_coll_upload(sn)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    client_issue = body.issue_id
    issue_id: str | None = None    

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)
        rep_exists = await coll.find_one({"sn": sn, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            issue_id = client_issue
    
    if not issue_id:
        while True:
            candidate = await _next_issue_id(db, sn, pm_type, d, pad=2)
            rep_exists = await coll.find_one({"issue_id": candidate})
            url_exists = await url_coll.find_one({"issue_id": candidate})
            if not rep_exists and not url_exists:
                issue_id = candidate
                break

    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{sn}_"
        valid_fmt = client_docName.startswith(prefix)

        rep_exists = await coll.find_one({"sn": sn, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, sn, pm_type, d)
        year = d.year
        doc_name = f"{sn}_{year_seq}/{year}"

    inspector = body.inspector
    doc = {
        "sn": sn,
        "station_id": station_id,  # Keep reference to station
        "chargeBoxID": charger.get("chargeBoxID"),
        "doc_name": doc_name,
        "issue_id": issue_id,
        "job": body.job,
        "rows_pre": body.rows_pre or {},
        "measures_pre": body.measures_pre,
        "pm_date": body.pm_date,
        "inspector": inspector,
        "photos_pre": {},
        "status": "draft",
        "side": body.side,
        "timestamp": datetime.now(timezone.utc),
    }
    res = await coll.insert_one(doc)
    return {
        "ok": True,
        "report_id": str(res.inserted_id),
        "issue_id": issue_id,
        "doc_name": doc_name,
    }

class PMPostIn(BaseModel):
    report_id: str | None = None
    sn: str  # Changed from station_id to sn
    rows: dict
    measures: dict
    summary: str
    summaryCheck: str | None = None
    dust_filter: Dict[str, bool] | None = None
    side: Literal["post", "after"]

@app.post("/pmreport/submit")
async def pmreport_post_submit(
    body: PMPostIn,
    current: UserClaims = Depends(get_current_user)
):
    """Submit Post-PM report for a charger"""
    sn = body.sn.strip()
    coll = get_pmreport_collection_for(sn)
    db = coll.database
    url_coll = get_pmurl_coll_upload(sn)

    # ---------- ‡∏Å‡∏£‡∏ì‡∏µ 1: ‡∏°‡∏µ report_id ‚Üí UPDATE doc ‡πÄ‡∏î‡∏¥‡∏° ----------
    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "sn": sn})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        update_fields = {
            "rows": body.rows,
            "measures": body.measures,
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            "dust_filter": body.dust_filter,
            "side": "post",
            "timestamp_post": datetime.now(timezone.utc),
        }

        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
        }

    # ---------- ‡∏Å‡∏£‡∏ì‡∏µ 2: ‡πÑ‡∏°‡πà‡∏°‡∏µ report_id ‚Üí INSERT ‡πÉ‡∏´‡∏°‡πà ----------
    # Validate charger exists
    charger = await _get_charger_by_sn(sn)
    
    doc = {
        "sn": sn,
        "station_id": charger.get("station_id"),
        "chargeBoxID": charger.get("chargeBoxID"),
        "rows": body.rows,
        "measures": body.measures,
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        "dust_filter": body.dust_filter,
        "photos": {},
        "status": "draft",
        "side": "post",
        "timestamp": datetime.now(timezone.utc),
    }
    res = await coll.insert_one(doc)
    return {
        "ok": True,
        "report_id": str(res.inserted_id),
    }

# ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå
UPLOADS_ROOT = os.getenv("UPLOADS_ROOT", "./uploads")
os.makedirs(UPLOADS_ROOT, exist_ok=True)

app.mount("/uploads", StaticFiles(directory=UPLOADS_ROOT, html=False), name="uploads")

ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif","pdf"}
MAX_FILE_MB = 20

def _safe_name(name: str) -> str:
    base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
    return base[:120] or secrets.token_hex(4)

def _ext(fname: str) -> str:
    return (fname.rsplit(".",1)[-1].lower() if "." in fname else "")

@app.post("/pmreport/{report_id}/pre/photos")
async def pmreport_upload_pre_photos(
    report_id: str,
    sn: str = Form(...),
    group: str = Form(...),
    files: list[UploadFile] = File(...),
):
    """Upload Pre-PM photos for a charger report"""
    if not re.fullmatch(r"g\d+(_\d+)?", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_pmreport_collection_for(sn)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "sn": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("sn") != sn:
        raise HTTPException(status_code=400, detail="sn mismatch")

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á - ‡πÉ‡∏ä‡πâ sn ‡πÅ‡∏ó‡∏ô station_id
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "pm" / sn / report_id / "pre" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = _ext(f.filename or "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/pm/{sn}/{report_id}/pre/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos_pre.{group}": {"$each": saved}}}
    )

    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/pmreport/{report_id}/post/photos")
async def pmreport_upload_post_photos(
    report_id: str,
    sn: str = Form(...),
    group: str = Form(...),
    files: list[UploadFile] = File(...),
    remark: str | None = Form(None),
):
    """Upload Post-PM photos for a charger report"""
    if not re.fullmatch(r"g\d+(_\d+)?", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_pmreport_collection_for(sn)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "sn": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("sn") != sn:
        raise HTTPException(status_code=400, detail="sn mismatch")

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á - ‡πÉ‡∏ä‡πâ sn ‡πÅ‡∏ó‡∏ô station_id
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "pm" / sn / report_id / "post" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = _ext(f.filename or "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/pm/{sn}/{report_id}/post/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos.{group}": {"$each": saved}}}
    )

    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/pmreport/{report_id}/finalize")
async def pmreport_finalize(
    report_id: str,
    sn: str = Form(...),
):
    """Finalize a PM report"""
    coll = get_pmreport_collection_for(sn)
    
    oid = ObjectId(report_id)
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

def parse_report_date_to_utc(s: str) -> datetime:
    if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
        tz_th = ZoneInfo("Asia/Bangkok")
        dt_th = datetime.fromisoformat(s + "T00:00:00").replace(tzinfo=tz_th)
        return dt_th.astimezone(timezone.utc)
    if s.endswith("Z"):
        return datetime.fromisoformat(s.replace("Z", "+00:00")).astimezone(timezone.utc)
    if re.search(r"[+\-]\d{2}:\d{2}$", s):
        return datetime.fromisoformat(s).astimezone(timezone.utc)
    return datetime.fromisoformat(s + "+07:00").astimezone(timezone.utc)

def normalize_pm_date(s: str) -> str:
    if re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
        return s
    if s.endswith("Z") or re.search(r"[+\-]\d{2}:\d{2}$", s):
        dt = datetime.fromisoformat(s.replace("Z", "+00:00"))
    else:
        dt = datetime.fromisoformat(s).replace(tzinfo=th_tz)
    return dt.astimezone(th_tz).date().isoformat()

@app.post("/pmurl/upload-files", status_code=201)
async def pmurl_upload_files(
    sn: str = Form(...),
    reportDate: str = Form(...),
    files: list[UploadFile] = File(...),
    issue_id: Optional[str] = Form(None),
    doc_name: Optional[str] = Form(None),
    inspector: Optional[str] = Form(None),
):
    """Upload PM PDF files for a charger"""
    # Validate charger exists
    charger = await _get_charger_by_sn(sn)
    station_id = charger.get("station_id")
    
    coll = get_pmurl_coll_upload(sn)
    pm_date = normalize_pm_date(reportDate)

    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

    pm_type = "CG"

    # -------------------------
    # 1) ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÄ‡∏•‡∏∑‡∏≠‡∏Å issue_id
    # -------------------------
    rep_coll = get_pmreport_collection_for(sn)
    final_issue_id: str | None = None
    client_issue = (issue_id or "").strip()

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        url_exists = await coll.find_one({"issue_id": client_issue})
        rep_exists = await rep_coll.find_one({"issue_id": client_issue})
        unique = not (url_exists or rep_exists)

        if valid_fmt and unique:
            final_issue_id = client_issue

    if not final_issue_id:
        while True:
            candidate = await _next_issue_id(coll.database, sn, pm_type, d, pad=2)
            url_exists = await coll.find_one({"issue_id": candidate})
            rep_exists = await rep_coll.find_one({"issue_id": candidate})
            if not url_exists and not rep_exists:
                final_issue_id = candidate
                break

    year_seq: int | None = None

    rep = await get_pmreport_collection_for(sn).find_one(
        {"issue_id": final_issue_id},
        {"year_seq": 1, "pm_date": 1},
    )
    if rep and rep.get("year_seq") is not None:
        year_seq = int(rep["year_seq"])

    if year_seq is None:
        year_seq = await _next_year_seq(coll.database, sn, pm_type, d)

    year = d.year
    final_doc_name: str | None = None

    if doc_name:
        candidate = doc_name.strip()
        ok_format = candidate.startswith(f"{sn}_")

        rep_exists = await rep_coll.find_one({"doc_name": candidate})
        url_exists = await coll.find_one({"doc_name": candidate})
        unique = not (rep_exists or url_exists)

        if ok_format and unique:
            final_doc_name = candidate

    if not final_doc_name:
        final_doc_name = f"{sn}_{year_seq}/{year}"

    doc_name = final_doc_name

    # -------------------------
    # 3) ‡πÄ‡∏ã‡∏ü‡πÑ‡∏ü‡∏•‡πå PDF ‡∏•‡∏á‡∏î‡∏¥‡∏™‡∏Å‡πå
    # -------------------------
    subdir = pm_date
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "pmurl" / sn / subdir
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls: list[str] = []
    metas: list[dict] = []
    total_size = 0

    for f in files:
        ext = (f.filename.rsplit(".", 1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS or ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        total_size += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(
                status_code=413,
                detail=f"File too large (> {MAX_FILE_MB} MB)"
            )

        safe = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        dest = dest_dir / safe
        with open(dest, "wb") as out:
            out.write(data)

        url = f"/uploads/pmurl/{sn}/{subdir}/{safe}"
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    inspector_clean = (inspector or "").strip() or None

    # -------------------------
    # 4) ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á Mongo
    # -------------------------
    now = datetime.now(timezone.utc)
    doc = {
        "sn": sn,
        "station_id": station_id,
        "chargeBoxID": charger.get("chargeBoxID"),
        "pm_date": pm_date,
        "issue_id": final_issue_id,
        "inspector": inspector_clean,
        "year": year,
        "year_seq": year_seq,
        "doc_name": doc_name,
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    }
    res = await coll.insert_one(doc)

    return {
        "ok": True,
        "inserted_id": str(res.inserted_id),
        "count": len(urls),
        "urls": urls,
        "issue_id": final_issue_id,
        "year_seq": year_seq,
        "doc_name": doc_name,
        "inspector": inspector_clean,
    }

@app.get("/pmurl/list")
async def pmurl_list(
    sn: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    """List PM URL documents for a charger"""
    coll = get_pmurl_coll_upload(sn)
    skip = (page - 1) * pageSize

    cursor = coll.find(
        {},
        {"_id": 1, "issue_id": 1, "doc_name": 1, "inspector": 1, "pm_date": 1, "reportDate": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    def _pm_date_from(doc: dict) -> str | None:
        s = doc.get("pm_date")
        if isinstance(s, str) and re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
            return s

        rd = doc.get("reportDate")
        if isinstance(rd, datetime):
            return rd.astimezone(th_tz).date().isoformat()
        if isinstance(rd, str):
            try:
                dt = datetime.fromisoformat(rd.replace("Z", "+00:00"))
            except Exception:
                try:
                    dt = datetime.fromisoformat(rd).replace(tzinfo=th_tz)
                except Exception:
                    return None
            return dt.astimezone(th_tz).date().isoformat()

        return None

    items = []
    pm_date_arr = []

    for it in items_raw:
        pm_date_str = _pm_date_from(it)
        if pm_date_str:
            pm_date_arr.append(pm_date_str)

        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""

        items.append({
            "id": str(it["_id"]),
            "pm_date": pm_date_str, 
            "inspector": it.get("inspector"), 
            "doc_name": it.get("doc_name"),
            "issue_id": it.get("issue_id"),
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,
            "urls": urls,
        })

    return {
        "items": items,
        "pm_date": [d for d in pm_date_arr if d],
        "page": page,
        "pageSize": pageSize,
        "total": total,
    }

# -------------------------------------------------- PMReportPage (MDB)       
# def get_mdbpmreport_collection_for(station_id: str):
#     _validate_station_id(station_id)
#     return MDBPMReportDB.get_collection(str(station_id))

# def get_mdbpmurl_coll_upload(station_id: str):
#     _validate_station_id(station_id)
#     return MDBPMUrlDB.get_collection(str(station_id))

# class MDBPMSubmitIn(BaseModel):
#     side: Literal["pre", "post"]
#     station_id: str
#     job: Dict[str, Any]         # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô (location/date/inspector ‡∏Ø‡∏•‡∏Ø)
#     measures_pre: Dict[str, Dict[str, Any]] 
#     # rows: Dict[str, Dict[str, Any]]  # {"r1": {"pf": "...", "remark": "..."}, ...}
#     # measures: Dict[str, Dict[str, Any]]  # {"m4": {...}, "m5": {...}, ..., "m8": {...}}
#     # summary: str
#     pm_date: str                # "YYYY-MM-DD"
#     issue_id: Optional[str] = None
#     doc_name: Optional[str] = None
#     # summaryCheck: Optional[Literal["PASS","FAIL","NA"]] = None
#     inspector: Optional[str] = None 
#     # dust_filter: Optional[str] = None

@app.get("/mdbpmreport/preview-issueid")
async def mdbpmreport_preview_issueid(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    ‡∏î‡∏π issue_id ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (PM-CG-YYMM-XX) ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á
    ‡πÉ‡∏ä‡πâ‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡πÑ‡∏õ‡πÇ‡∏ä‡∏ß‡πå‡∏ö‡∏ô‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÄ‡∏â‡∏¢ ‡πÜ
    """
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "MB"

    latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="mdb")

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

# @app.get("/mdbpmreport/latest-docname")
# async def mdbpmreport_latest_docname(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     """
#     ‡∏î‡∏∂‡∏á doc_name ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏õ‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö pm_date)
#     ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏•‡∏Ç‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà frontend
#     """
#     # auth ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    
#     latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="mdb")
    
#     return {
#         "doc_name": latest.get("doc_name") if latest else None,
#         "station_id": station_id,
#         "pm_date": pm_date
#     }

@app.get("/mdbpmreport/preview-docname")
async def preview_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(station_id, year,source="mdb")

    if not latest:
        next_doc = f"{station_id}_1/{year}"
    else:
        import re
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{station_id}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

# @app.post("/mdbpmreport/pre/submit")
# async def mdbpmreport_submit(body: MDBPMSubmitIn, current: UserClaims = Depends(get_current_user)):
#     station_id = body.station_id.strip()
#     coll = get_mdbpmreport_collection_for(station_id)
#     db = coll.database

#     pm_type = str(body.job.get("pm_type") or "MB").upper()
#     body.job["pm_type"] = pm_type

#     url_coll = get_mdbpmurl_coll_upload(station_id)

#     try:
#         d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     client_issue = body.issue_id 
#     issue_id: str | None = None    

#     if client_issue:
#         yymm = f"{d.year % 100:02d}{d.month:02d}"
#         prefix = f"PM-{pm_type}-{yymm}-"
#         valid_fmt = client_issue.startswith(prefix)
#         # unique = not await coll.find_one({"station_id": station_id, "issue_id": client_issue})
#         rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
#         url_exists = await url_coll.find_one({"issue_id": client_issue})
#         unique = not (rep_exists or url_exists)

#         if valid_fmt and unique:
#             issue_id = client_issue

#     if not issue_id:
#         while True:
#             candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
#             rep_exists = await coll.find_one({"issue_id": candidate})
#             url_exists = await url_coll.find_one({"issue_id": candidate})
#             if not rep_exists and not url_exists:
#                 issue_id = candidate
#                 break

#     client_docName = body.doc_name
#     doc_name = None
#     if client_docName:
#         year = f"{d.year}"
#         prefix = f"{station_id}_"
#         valid_fmt = client_docName.startswith(prefix)

#         url_coll = get_mdbpmurl_coll_upload(station_id)
#         rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
#         url_exists = await url_coll.find_one({"doc_name": client_docName})
#         unique = not (rep_exists or url_exists)

#         if valid_fmt and unique:
#             doc_name = client_docName
 
#     if not doc_name:
#         year_seq = await _next_year_seq(db, station_id, pm_type, d)
#         year = d.year
#         doc_name = f"{station_id}_{year_seq}/{year}"

#     inspector = body.inspector
#     doc = {
#         "station_id": station_id,
#         "doc_name": doc_name,
#         "issue_id": issue_id,
#         "job": body.job,
#         # "rows": body.rows,
#         "measures_pre": body.measures_pre,         # m4..m8
#         # "summary": body.summary,
#         # "summaryCheck": body.summaryCheck,
#         "pm_date": body.pm_date,           # string YYYY-MM-DD (‡∏ï‡∏≤‡∏°‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå)
#         "inspector": inspector,
#         # "dust_filter": body.dust_filter,
#         "photos_pre": {},
#         "status": "draft",
#         "side": body.side,
#         # "createdAt": datetime.now(timezone.utc),
#         # "updatedAt": datetime.now(timezone.utc),
#         "timestamp": datetime.now(timezone.utc),

#     }

#     res = await coll.insert_one(doc)
#     return {
#         "ok": True, 
#         "report_id": str(res.inserted_id), 
#         "issue_id": issue_id,
#         "doc_name": doc_name,
#     }

# class MDBPMPostIn(BaseModel):
#     report_id: str | None = None      # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°
#     station_id: str
#     # issue_id: str | None = None
#     # job: dict
#     rows: dict
#     measures: dict
#     summary: str
#     # pm_date: str
#     # doc_name: str | None = None
#     summaryCheck: str | None = None
#     dust_filter: str | None = None
#     side: Literal["post", "after"]

# @app.post("/mdbpmreport/submit")
# async def mdbpmreport_submit(body: MDBPMPostIn, current: UserClaims = Depends(get_current_user)):
#     station_id = body.station_id.strip()
#     coll = get_mdbpmreport_collection_for(station_id)
#     db = coll.database

#     url_coll = get_mdbpmurl_coll_upload(station_id)

#     if body.report_id:
#         try:
#             oid = ObjectId(body.report_id)
#         except InvalidId:
#             raise HTTPException(status_code=400, detail="invalid report_id")

#         existing = await coll.find_one({"_id": oid, "station_id": station_id})
#         if not existing:
#             raise HTTPException(status_code=404, detail="Report not found")

#         update_fields = {
#             # "job": body.job,
#             "rows": body.rows,
#             "measures": body.measures,          # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á PM
#             "summary": body.summary,
#             "summaryCheck": body.summaryCheck,
#             # "pm_date": body.pm_date,
#             # "inspector": inspector,
#             "dust_filter": body.dust_filter,
#             # "doc_name": doc_name,
#             "side": "post",                     # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ù‡∏±‡πà‡∏á post ‡πÅ‡∏•‡πâ‡∏ß
#             "timestamp_post": datetime.now(timezone.utc),
#         }

#         await coll.update_one({"_id": oid}, {"$set": update_fields})

#         return {
#             "ok": True,
#             "report_id": body.report_id,
#             # "issue_id": issue_id,
#             # "doc_name": doc_name,
#         }
    
#     doc = {
#         "station_id": station_id,
#         # "doc_name": doc_name,
#         # "issue_id": issue_id,
#         # "job": body.job,
#         "rows": body.rows,
#         "measures": body.measures,         # m4..m8
#         "summary": body.summary,
#         "summaryCheck": body.summaryCheck,
#         # "pm_date": body.pm_date,           # string YYYY-MM-DD (‡∏ï‡∏≤‡∏°‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå)
#         # "inspector": inspector,
#         "dust_filter": body.dust_filter,
#         "status": "draft",
#         "photos": {},                      # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô /photos
#         # "createdAt": datetime.now(timezone.utc),
#         # "updatedAt": datetime.now(timezone.utc),
#         "side": "post",
#         "timestamp": datetime.now(timezone.utc),
#     }

#     res = await coll.insert_one(doc)
#     return {
#         "ok": True, 
#         "report_id": str(res.inserted_id), 
#         # "issue_id": issue_id,
#         # "doc_name": doc_name,
#     }

# @app.get("/mdbpmreport/get")
# async def mdbpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
#     coll = get_mdbpmreport_collection_for(station_id)
#     doc = await coll.find_one({"_id": ObjectId(report_id)})
#     if not doc:
#         raise HTTPException(status_code=404, detail="not found")

#     doc["_id"] = str(doc["_id"])
#     return doc

# @app.get("/mdbpmreport/list")
# async def mdbpmreport_list(
#     station_id: str = Query(...),
#     page: int = Query(1, ge=1),
#     pageSize: int = Query(20, ge=1, le=100),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")

#     coll = get_mdbpmreport_collection_for(station_id)
#     skip = (page - 1) * pageSize

#     cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1 , "pm_date": 1,"inspector":1,"side":1, "createdAt": 1}).sort(
#         [("createdAt", -1), ("_id", -1)]
#     ).skip(skip).limit(pageSize)

#     items_raw = await cursor.to_list(length=pageSize)
#     total = await coll.count_documents({})

#     # ‡∏ú‡∏π‡∏Å URL PDF ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏à‡∏≤‡∏Å MDBPMUrlDB (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
#     pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
#     url_by_day: Dict[str, str] = {}
#     if pm_dates:
#         ucoll = get_mdbpmurl_coll_upload(station_id)
#         ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
#         url_docs = await ucur.to_list(length=10_000)
#         for u in url_docs:
#             day = u.get("pm_date")
#             first_url = (u.get("urls") or [None])[0]
#             if day and first_url and day not in url_by_day:
#                 url_by_day[day] = first_url

#     items = [{
#         "id": str(it["_id"]),
#         "issue_id": it.get("issue_id"),
#         "doc_name": it.get("doc_name"),
#         "inspector": it.get(("inspector")),
#         "side":it.get("side"), 
#         "pm_date": it.get("pm_date"),
#         "createdAt": _ensure_utc_iso(it.get("createdAt")),
#         "file_url": url_by_day.get(it.get("pm_date") or "", ""),
#     } for it in items_raw]

#     return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# @app.post("/mdbpmreport/{report_id}/pre/photos")
# async def mdbpmreport_upload_photos(
#     report_id: str,
#     station_id: str = Form(...),
#     group: str = Form(...),                   # ‡πÄ‡∏ä‡πà‡∏ô "g1" .. "g10"
#     files: list[UploadFile] = File(...),
#     # remark: str | None = Form(None),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
#     # Accept both formats: "g1", "g2" (simple questions) and "r9_1", "r9_2" (group sub-items)
#     if not re.fullmatch(r"(g\d+|r\d+_\d+)", group):
#         raise HTTPException(status_code=400, detail=f"Bad group key format: {group}")

#     # Map group key to database storage key:
#     # - "g1" -> "g1", "g2" -> "g2", etc. (keep g prefix)
#     # - "r9_1", "r9_2", "r9_3", "r9_4" -> "g9" (all merged into question 9 with g prefix)
#     storage_key = group
#     group_match = re.match(r"r(\d+)_\d+", group)
#     if group_match:  # Convert r9_1 format to g9
#         question_num = group_match.group(1)  # Extract question number (e.g., "9")
#         storage_key = f"g{question_num}"  # Add g prefix for database
#     # else: keep group as-is (e.g., "g1" stays "g1")

#     coll = get_mdbpmreport_collection_for(station_id)
#     from bson import ObjectId
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô station ‡∏ô‡∏µ‡πâ
#     doc = await coll.find_one({"_id": oid}, {"_id":1, "station_id":1})
#     if not doc:
#         raise HTTPException(status_code=404, detail="Report not found")
#     if doc.get("station_id") != station_id:
#         raise HTTPException(status_code=400, detail="station_id mismatch")

#     # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á (use storage_key for consistent path)
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpm" / station_id / report_id  / "pre" / storage_key
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     saved = []
#     total = 0
#     for f in files:
#         ext = _ext(f.filename or "")
#         if ext not in ALLOWED_EXTS:
#             raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

#         data = await f.read()
#         total += len(data)
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         # URL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏ô Frontend (use storage_key for consistent URL)
#         url_path = f"/uploads/mdbpm/{station_id}/{report_id}/pre/{storage_key}/{fname}"
#         saved.append({
#             "filename": fname,
#             "size": len(data),
#             "url": url_path,
#             # "remark": remark or "",
#             "uploadedAt": datetime.now(timezone.utc)
#         })

#     # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PMReport: push ‡∏•‡∏á photos_pre.<storage_key>
#     # Note: storage_key already calculated above
#     await coll.update_one(
#         {"_id": oid},
#         {"$push": {f"photos_pre.{storage_key}": {"$each": saved}}}
#     )

#     return {"ok": True, "count": len(saved), "group": group, "files": saved}



# @app.post("/mdbpmreport/{report_id}/post/photos")
# async def mdbpmreport_upload_photos(
#     report_id: str,
#     station_id: str = Form(...),
#     group: str = Form(...),                   # "g1" .. "g11"
#     files: List[UploadFile] = File(...),
#     remark: Optional[str] = Form(None),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
#     # Accept both formats: "g1", "g2" (simple questions) and "r9_1", "r9_2" (group sub-items)
#     if not re.fullmatch(r"(g\d+|r\d+_\d+)", group):
#         raise HTTPException(status_code=400, detail=f"Bad group key format: {group}")

#     # Map group key to database storage key:
#     # - "g1" -> "g1", "g2" -> "g2", etc. (keep g prefix)
#     # - "r9_1", "r9_2", "r9_3", "r9_4" -> "g9" (all merged into question 9 with g prefix)
#     storage_key = group
#     group_match = re.match(r"r(\d+)_\d+", group)
#     if group_match:  # Convert r9_1 format to g9
#         question_num = group_match.group(1)  # Extract question number (e.g., "9")
#         storage_key = f"g{question_num}"  # Add g prefix for database
#     # else: keep group as-is (e.g., "g1" stays "g1")

#     coll = get_mdbpmreport_collection_for(station_id)
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
#     if not doc:
#         raise HTTPException(status_code=404, detail="Report not found")
#     if doc.get("station_id") != station_id:
#         raise HTTPException(status_code=400, detail="station_id mismatch")

#     # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/mdbpm/{station_id}/{report_id}/{group}/
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpm" / station_id / report_id / "post" / storage_key
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     saved = []
#     for f in files:
#         ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
#         if ext not in ALLOWED_EXTS:
#             raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

#         data = await f.read()
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         url_path = f"/uploads/mdbpm/{station_id}/{report_id}/post/{storage_key}/{fname}"
#         saved.append({
#             "filename": fname,
#             "size": len(data),
#             "url": url_path,
#             "remark": remark or "",
#             "uploadedAt": datetime.now(timezone.utc)
#         })

#     # Note: storage_key mapping already done above (before file operations)
#     await coll.update_one(
#         {"_id": oid},
#         {
#             "$push": {f"photos.{storage_key}": {"$each": saved}},
#             "$set": {"updatedAt": datetime.now(timezone.utc)}
#         }
#     )
#     return {"ok": True, "count": len(saved), "group": group, "files": saved}

# @app.post("/mdbpmreport/{report_id}/finalize")
# async def mdbpmreport_finalize(
#     report_id: str,
#     station_id: str = Form(...),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")

#     coll = get_mdbpmreport_collection_for(station_id)
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     # (‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô) ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô finalize ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
#     res = await coll.update_one(
#         {"_id": oid},
#         {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
#     )
#     if res.matched_count == 0:
#         raise HTTPException(status_code=404, detail="Report not found")
#     return {"ok": True}

# @app.post("/mdbpmurl/upload-files", status_code=201)
# async def mdbpmurl_upload_files(
#     station_id: str = Form(...),
#     reportDate: str = Form(...),            # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO -> ‡∏à‡∏∞ normalize ‡πÄ‡∏õ‡πá‡∏ô YYYY-MM-DD
#     files: List[UploadFile] = File(...),    # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ .pdf
#     issue_id: Optional[str] = Form(None),
#     doc_name: Optional[str] = Form(None),
#     inspector: Optional[str] = Form(None),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     coll = get_mdbpmurl_coll_upload(station_id)
#     pm_date = normalize_pm_date(reportDate)  # ‡∏Ñ‡∏∑‡∏ô YYYY-MM-DD

#     # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpmurl" / station_id / pm_date
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     pm_type = "MB"
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

#     rep_coll = get_mdbpmreport_collection_for(station_id)
#     final_issue_id = None
#     client_issue = (issue_id or "").strip()

#     if client_issue:
#         yymm = f"{d.year % 100:02d}{d.month:02d}"
#         prefix = f"PM-{pm_type}-{yymm}-"
#         valid_fmt = client_issue.startswith(prefix)

#         url_exists = await coll.find_one({"issue_id": client_issue})
#         rep_exists = await rep_coll.find_one({"issue_id": client_issue})
#         unique = not (url_exists or rep_exists)

#         if valid_fmt and unique:
#             final_issue_id = client_issue

#     if not final_issue_id:
#         while True:
#             candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
#             url_exists = await coll.find_one({"issue_id": candidate})
#             rep_exists = await rep_coll.find_one({"issue_id": candidate})
#             if not url_exists and not rep_exists:
#                 final_issue_id = candidate
#                 break

#     year_seq: int | None = None

#     # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° reuse year_seq ‡∏à‡∏≤‡∏Å PMReportDB ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (issue_id ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
#     rep = await get_mdbpmreport_collection_for(station_id).find_one(
#         {"issue_id": final_issue_id},
#         {"year_seq": 1, "pm_date": 1},
#     )
#     if rep and rep.get("year_seq") is not None:
#         year_seq = int(rep["year_seq"])

#     # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ year_seq ‡∏à‡∏≤‡∏Å PMReport ‚Üí ‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å pm_year_sequences
#     if year_seq is None:
#         year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

#     year = d.year
#     final_doc_name: str | None = None

#     if doc_name:
#         candidate = doc_name.strip()

#         ok_format = candidate.startswith(f"{station_id}_")

#         rep_coll = get_mdbpmreport_collection_for(station_id)
#         rep_exists = await rep_coll.find_one({"doc_name": candidate})
#         url_exists = await coll.find_one({"doc_name": candidate})
#         unique = not (rep_exists or url_exists)

#         if ok_format and unique:
#             final_doc_name = candidate

#     if not final_doc_name:
#         final_doc_name = f"{station_id}_{year_seq}/{year}"

#     doc_name = final_doc_name

#     # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpmurl" / station_id / pm_date
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     urls, metas = [], []
#     for f in files:
#         ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
#         if ext != "pdf":
#             raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

#         data = await f.read()
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         url = f"/uploads/mdbpmurl/{station_id}/{pm_date}/{fname}"
#         urls.append(url)
#         metas.append({"name": f.filename, "size": len(data)})

#     inspector_clean = (inspector or "").strip() or None
#     now = datetime.now(timezone.utc)
#     res = await coll.insert_one({
#         "station": station_id,
#         "pm_date": pm_date,
#         "inspector": inspector_clean,
#         "doc_name": doc_name,
#         "issue_id": final_issue_id, 
#         "urls": urls,
#         "meta": {"files": metas},
#         "source": "upload-files",
#         "createdAt": now,
#         "updatedAt": now,
#     })
#     return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean,}

# @app.get("/mdbpmurl/list")
# async def mdbpmurl_list(
#     station_id: str = Query(...),
#     page: int = Query(1, ge=1),
#     pageSize: int = Query(20, ge=1, le=100),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     coll = get_mdbpmurl_coll_upload(station_id)
#     skip = (page - 1) * pageSize

#     cursor = coll.find(
#         {},
#         {"_id": 1, "issue_id": 1, "doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
#     ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

#     items_raw = await cursor.to_list(length=pageSize)
#     total = await coll.count_documents({})

#     items = []
#     for it in items_raw:
#         urls = it.get("urls") or []
#         first_url = urls[0] if urls else ""
#         items.append({
#             "id": str(it["_id"]),
#             "pm_date": it.get("pm_date"),
#             "issue_id": it.get("issue_id"),
#             "inspector": it.get(("inspector")), 
#             "doc_name": it.get("doc_name"),
#             "createdAt": _ensure_utc_iso(it.get("createdAt")),
#             "file_url": first_url,
#             "urls": urls,
#         })

#     return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# ------------------------------------------------------------------
# new PM Report (MDB)
# ------------------------------------------------------------------

# ============================================
# MDB PM Report Backend - Updated for New Structure
# ============================================
# 
# ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏´‡∏•‡∏±‡∏Å:
# 1. ‡∏Ç‡πâ‡∏≠ 4 - Dynamic Breaker Main (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß)
# 2. ‡∏Ç‡πâ‡∏≠ 5 - Breaker Charger ‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô charger ‡∏à‡∏£‡∏¥‡∏á
# 3. ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà 9 ‡∏Ç‡πâ‡∏≠ (‡πÅ‡∏ó‡∏ô 11 ‡∏Ç‡πâ‡∏≠‡πÄ‡∏î‡∏¥‡∏°)
# 4. Photo upload ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö key format ‡πÉ‡∏´‡∏°‡πà
#
# ============================================

def get_mdbpmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    return MDBPMReportDB.get_collection(str(station_id))

def get_mdbpmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    return MDBPMUrlDB.get_collection(str(station_id))

from pydantic import BaseModel
from typing import Dict, Any, Optional, List, Literal
from datetime import datetime, timezone
from fastapi import HTTPException, Query, Form, File, UploadFile, Depends
from bson import ObjectId
from bson.errors import InvalidId
import re
import pathlib
import secrets

# ============================================
# 1. UPDATE: MDBPMSubmitIn (Pre-PM)
# ============================================

class MDBPMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    station_id: str
    job: Dict[str, Any]
    measures_pre: Dict[str, Any]
    rows_pre: Optional[Dict[str, Dict[str, Any]]] = None 
    q4_items: Optional[List[Dict[str, str]]] = None  
    charger_count: Optional[int] = None  
    pm_date: str
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None
    inspector: Optional[str] = None


# ============================================
# 2. UPDATE: MDBPMPostIn (Post-PM)
# ============================================

class MDBPMPostIn(BaseModel):
    report_id: Optional[str] = None
    station_id: str
    rows: Dict[str, Any]
    measures: Dict[str, Any]
    summary: str
    summaryCheck: Optional[str] = None
    dust_filter: Optional[str] = None
    side: Literal["post", "after"]

# ============================================
# 3. UPDATE: Pre-PM Submit Endpoint
# ============================================

@app.post("/mdbpmreport/pre/submit")
async def mdbpmreport_pre_submit(body: MDBPMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_mdbpmreport_collection_for(station_id)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "MB").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_mdbpmurl_coll_upload(station_id)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    # Issue ID logic (unchanged)
    client_issue = body.issue_id 
    issue_id: str | None = None    

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)
        rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            issue_id = client_issue

    if not issue_id:
        while True:
            candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
            rep_exists = await coll.find_one({"issue_id": candidate})
            url_exists = await url_coll.find_one({"issue_id": candidate})
            if not rep_exists and not url_exists:
                issue_id = candidate
                break

    # Doc name logic (unchanged)
    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{station_id}_"
        valid_fmt = client_docName.startswith(prefix)

        rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, station_id, pm_type, d)
        year = d.year
        doc_name = f"{station_id}_{year_seq}/{year}"

    inspector = body.inspector
    
    # === UPDATED DOCUMENT STRUCTURE ===
    doc = {
        "station_id": station_id,
        "doc_name": doc_name,
        "issue_id": issue_id,
        "job": body.job,
        
        # NEW: Store measures_pre with new structure
        # {
        #   "m4": {"r4_1": {"L1-N": {"value": "220", "unit": "V"}, ...}, "r4_2": {...}},
        #   "m5": {"r5_1": {...}, "r5_2": {...}},
        #   "m6": {"L1-N": {...}, ...}
        # }
        "measures_pre": body.measures_pre,
        
        # NEW: Store rows_pre
        "rows_pre": body.rows_pre or {},
        
        # NEW: Store dynamic items configuration
        "q4_items": body.q4_items or [{"key": "r4_1", "label": "4.1) Breaker Main ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà 1"}],
        "charger_count": body.charger_count or 1,
        
        "pm_date": body.pm_date,
        "inspector": inspector,
        "photos_pre": {},
        "status": "draft",
        "side": body.side,
        "timestamp": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id), 
        "issue_id": issue_id,
        "doc_name": doc_name,
    }


# ============================================
# 4. UPDATE: Post-PM Submit Endpoint
# ============================================

@app.post("/mdbpmreport/submit")
async def mdbpmreport_post_submit(body: MDBPMPostIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_mdbpmreport_collection_for(station_id)

    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "station_id": station_id})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        update_fields = {
            "rows": body.rows,
            
            # NEW: measures with new structure
            # {
            #   "m4": {"r4_1": {...}, "r4_2": {...}},
            #   "m5": {"r5_1": {...}, "r5_2": {...}},
            #   "m6": {...}
            # }
            "measures": body.measures,
            
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            "dust_filter": body.dust_filter,
            "side": "post",
            "timestamp_post": datetime.now(timezone.utc),
        }

        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
        }
    
    # Insert new (fallback - normally should update existing)
    doc = {
        "station_id": station_id,
        "rows": body.rows,
        "measures": body.measures,
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        "dust_filter": body.dust_filter,
        "status": "draft",
        "photos": {},
        "side": "post",
        "timestamp": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id), 
    }


# ============================================
# 5. UPDATE: Photo Upload - Support New Key Formats
# ============================================

# NEW: Updated regex pattern to support all key formats
PHOTO_GROUP_PATTERN = r"(g\d+|r\d+_\d+)"

# Key formats:
# - g1, g2, g3     -> Simple questions (Q1, Q2, Q3)
# - r4_1, r4_2     -> Dynamic Breaker Main items
# - r5_1, r5_2     -> Charger Breaker items  
# - g6             -> CCB (Q6)
# - r7_1, r7_2...  -> Trip Test items (Q7)
# - g8, g9         -> Simple questions (Q8, Q9)

@app.post("/mdbpmreport/{report_id}/pre/photos")
async def mdbpmreport_upload_photos_pre(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),
    files: List[UploadFile] = File(...),
):
    # Validate group format
    if not re.fullmatch(PHOTO_GROUP_PATTERN, group):
        raise HTTPException(status_code=400, detail=f"Bad group key format: {group}")

    # === UPDATED: Storage key mapping ===
    # For dynamic items (r4_1, r4_2, r5_1, r5_2, r7_1, etc.), keep the full key
    # For simple questions (g1, g2, etc.), keep as-is
    storage_key = group
    
    # Special handling for Trip Test (Q7) - merge into g7
    if re.match(r"r7_\d+", group):
        storage_key = "g7"

    coll = get_mdbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # Folder path
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpm" / station_id / report_id / "pre" / storage_key
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = _ext(f.filename or "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/mdbpm/{station_id}/{report_id}/pre/{storage_key}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "uploadedAt": datetime.now(timezone.utc)
        })

    # Update document
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos_pre.{storage_key}": {"$each": saved}}}
    )

    return {"ok": True, "count": len(saved), "group": group, "storage_key": storage_key, "files": saved}


@app.post("/mdbpmreport/{report_id}/post/photos")
async def mdbpmreport_upload_photos_post(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),
    files: List[UploadFile] = File(...),
    remark: Optional[str] = Form(None),
):
    # Validate group format
    if not re.fullmatch(PHOTO_GROUP_PATTERN, group):
        raise HTTPException(status_code=400, detail=f"Bad group key format: {group}")

    # Storage key mapping (same as pre)
    # storage_key = group
    # if re.match(r"r7_\d+", group):
    #     storage_key = "g7"
    storage_key = group

    coll = get_mdbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    dest_dir = pathlib.Path(UPLOADS_ROOT) / "mdbpm" / station_id / report_id / "post" / storage_key
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".", 1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/mdbpm/{station_id}/{report_id}/post/{storage_key}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos.{storage_key}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "storage_key": storage_key, "files": saved}


# ============================================
# 6. UPDATE: Get Report - Return New Fields
# ============================================

@app.get("/mdbpmreport/get")
async def mdbpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    coll = get_mdbpmreport_collection_for(station_id)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    
    # Ensure new fields are present (for backward compatibility)
    if "q4_items" not in doc:
        doc["q4_items"] = [{"key": "r4_1", "label": "4.1) Breaker Main ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà 1"}]
    if "charger_count" not in doc:
        doc["charger_count"] = 1
    if "rows_pre" not in doc:
        doc["rows_pre"] = {}
    
    return doc


# ============================================
# 7. UPDATE: List Endpoint - Include New Fields
# ============================================

@app.get("/mdbpmreport/list")
async def mdbpmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_mdbpmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find(
        {}, 
        {
            "_id": 1, 
            "issue_id": 1, 
            "doc_name": 1, 
            "pm_date": 1,
            "inspector": 1,
            "side": 1, 
            "createdAt": 1,
            "charger_count": 1,  # NEW
        }
    ).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # URL mapping (unchanged)
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    url_by_day: Dict[str, str] = {}
    if pm_dates:
        ucoll = get_mdbpmurl_coll_upload(station_id)
        ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "inspector": it.get("inspector"),
        "side": it.get("side"), 
        "pm_date": it.get("pm_date"),
        "charger_count": it.get("charger_count", 1),  # NEW
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    return {
        "items": items, 
        "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], 
        "page": page, 
        "pageSize": pageSize, 
        "total": total
    }


# -------------------------------------------------- PMReportPage (CCB)       
def get_ccbpmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    return CCBPMReportDB.get_collection(str(station_id))

def get_ccbpmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    return CCBPMUrlDB.get_collection(str(station_id))

class CCBPMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    station_id: str
    job: Dict[str, Any]         # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô (location/date/inspector ‡∏Ø‡∏•‡∏Ø)
    # rows: Dict[str, Dict[str, Any]]  # {"r1": {"pf": "...", "remark": "..."}, ...}
    measures_pre : Dict[str, Dict[str, Any]]  # {"m4": {...}, "m5": {...}, ..., "m8": {...}}
    # summary: str
    pm_date: str                # "YYYY-MM-DD"
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None 
    # summaryCheck: Optional[Literal["PASS","FAIL","NA"]] = None
    inspector: Optional[str] = None

@app.get("/ccbpmreport/preview-issueid")
async def ccbpmreport_preview_issueid(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    ‡∏î‡∏π issue_id ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (PM-CG-YYMM-XX) ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á
    ‡πÉ‡∏ä‡πâ‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡πÑ‡∏õ‡πÇ‡∏ä‡∏ß‡πå‡∏ö‡∏ô‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÄ‡∏â‡∏¢ ‡πÜ
    """
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "CC"

    latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="ccb")

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

@app.get("/ccbpmreport/latest-docname")
async def ccbpmreport_latest_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    ‡∏î‡∏∂‡∏á doc_name ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏õ‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö pm_date)
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏•‡∏Ç‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà frontend
    """
    # auth ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    
    latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="ccb")
    
    return {
        "doc_name": latest.get("doc_name") if latest else None,
        "station_id": station_id,
        "pm_date": pm_date
    }

@app.get("/ccbpmreport/preview-docname")
async def preview_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(station_id, year,source="ccb")

    if not latest:
        next_doc = f"{station_id}_1/{year}"
    else:
        import re
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{station_id}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

@app.post("/ccbpmreport/pre/submit")
async def ccbpmreport_submit(body: CCBPMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_ccbpmreport_collection_for(station_id)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "CC").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_ccbpmurl_coll_upload(station_id)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    client_issue = body.issue_id 
    issue_id: str | None = None    

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        # ‚≠ê ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏±‡πâ‡∏á PMReportDB + PMUrlDB
        # url_coll = get_pmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            issue_id = client_issue
    
    # if not issue_id:
    #     issue_id = await _next_issue_id(db, station_id, pm_type, d, pad=2)
    if not issue_id:
        while True:
            candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
            rep_exists = await coll.find_one({"issue_id": candidate})
            url_exists = await url_coll.find_one({"issue_id": candidate})
            if not rep_exists and not url_exists:
                issue_id = candidate
                break
   
    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{station_id}_"
        valid_fmt = client_docName.startswith(prefix)

        url_coll = get_ccbpmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, station_id, pm_type, d)
        year = d.year
        doc_name = f"{station_id}_{year_seq}/{year}"

    inspector = body.inspector
    # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô draft ‡∏Å‡πà‡∏≠‡∏ô
    doc = {
        "station_id": station_id,
        "doc_name": doc_name,
        "issue_id": issue_id,
        "job": body.job,
        # "rows": body.rows,
        "measures_pre": body.measures_pre,         # m4..m8
        # "summary": body.summary,
        # "summaryCheck": body.summaryCheck,
        "pm_date": body.pm_date,           # string YYYY-MM-DD (‡∏ï‡∏≤‡∏°‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå)
        "status": "draft",
        # "photos": {},                      # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô /photos
        "photos_pre": {},
        "inspector": inspector,
        "side": body.side,
        # "createdAt": datetime.now(timezone.utc),
        # "updatedAt": datetime.now(timezone.utc),
        "timestamp": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id), 
        "issue_id": issue_id,
        "doc_name": doc_name,
    }

class CCBPMPostIn(BaseModel):
    report_id: str | None = None      # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°
    station_id: str
    # issue_id: str | None = None
    # job: dict
    rows: dict
    measures: dict
    summary: str
    # pm_date: str
    # doc_name: str | None = None
    summaryCheck: str | None = None
    # dust_filter: str | None = None
    side: Literal["post", "after"]

@app.post("/ccbpmreport/submit")
async def ccbpmreport_submit(body: CCBPMPostIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_ccbpmreport_collection_for(station_id)
    db = coll.database

    url_coll = get_ccbpmurl_coll_upload(station_id)

   
    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "station_id": station_id})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        # reuse ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° ‡πÑ‡∏°‡πà gen ‡πÉ‡∏´‡∏°‡πà
        # issue_id = existing.get("issue_id")
        # doc_name = existing.get("doc_name")
        # inspector = body.inspector or existing.get("inspector") or current.username

        update_fields = {
            # "job": body.job,
            "rows": body.rows,
            "measures": body.measures,          # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á PM
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            # "pm_date": body.pm_date,
            # "inspector": inspector,
            # "dust_filter": body.dust_filter,
            # "doc_name": doc_name,
            "side": "post",                     # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ù‡∏±‡πà‡∏á post ‡πÅ‡∏•‡πâ‡∏ß
            "timestamp_post": datetime.now(timezone.utc),
        }

        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
            # "issue_id": issue_id,
            # "doc_name": doc_name,
        }
    
    doc = {
        "station_id": station_id,
        # "doc_name": doc_name,
        # "issue_id": issue_id,
        # "job": body.job,
        "rows": body.rows,
        "measures": body.measures,         # m4..m8
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        # "pm_date": body.pm_date,           # string YYYY-MM-DD (‡∏ï‡∏≤‡∏°‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå)
        # "inspector": inspector,
        # "dust_filter": body.dust_filter,
        "status": "draft",
        "photos": {},                      # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô /photos
        # "createdAt": datetime.now(timezone.utc),
        # "updatedAt": datetime.now(timezone.utc),
        "side": "post",
        "timestamp": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id), 
        # "issue_id": issue_id,
        # "doc_name": doc_name,
    }

@app.get("/ccbpmreport/get")
async def ccbpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    coll = get_ccbpmreport_collection_for(station_id)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    return doc

@app.get("/ccbpmreport/list")
async def ccbpmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_ccbpmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side":1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # ‡∏ú‡∏π‡∏Å URL PDF ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏à‡∏≤‡∏Å CCBPMUrlDB (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    url_by_day: Dict[str, str] = {}
    if pm_dates:
        ucoll = get_ccbpmurl_coll_upload(station_id)
        ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "pm_date": it.get("pm_date"),
        "side":it.get("side"), 
        "inspector": it.get("inspector"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

@app.post("/ccbpmreport/{report_id}/pre/photos")
async def ccbpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "r1" .. "r10", "r9_0" .. "r9_5"
    files: List[UploadFile] = File(...),
    # remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"r\d+(_\d+)?", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_ccbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/ccbpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "ccbpm" / station_id / report_id / "pre" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/ccbpm/{station_id}/{report_id}/pre/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            # "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos_pre.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}


@app.post("/ccbpmreport/{report_id}/post/photos")
async def ccbpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "r1" .. "r10", "r9_0" .. "r9_5"
    files: List[UploadFile] = File(...),
    remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"r\d+(_\d+)?", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_ccbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/ccbpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "ccbpm" / station_id / report_id / "post" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/ccbpm/{station_id}/{report_id}/post/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/ccbpmreport/{report_id}/finalize")
async def ccbpmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_ccbpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # (‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô) ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô finalize ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/ccbpmurl/upload-files", status_code=201)
async def ccbpmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),            # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO -> ‡∏à‡∏∞ normalize ‡πÄ‡∏õ‡πá‡∏ô YYYY-MM-DD
    files: List[UploadFile] = File(...),    # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ .pdf
    # current: UserClaims = Depends(get_current_user),
    issue_id: Optional[str] = Form(None),
    doc_name: Optional[str] = Form(None),
    inspector: Optional[str] = Form(None),
):
    coll = get_ccbpmurl_coll_upload(station_id)
    pm_date = normalize_pm_date(reportDate)  # ‡∏Ñ‡∏∑‡∏ô YYYY-MM-DD

    pm_type = "CC"
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

    rep_coll = get_ccbpmreport_collection_for(station_id)
    final_issue_id = None
    client_issue = (issue_id or "").strip()

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        url_exists = await coll.find_one({"issue_id": client_issue})
        rep_exists = await rep_coll.find_one({"issue_id": client_issue})
        unique = not (url_exists or rep_exists)

        if valid_fmt and unique:
            final_issue_id = client_issue

    if not final_issue_id:
        while True:
            candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
            url_exists = await coll.find_one({"issue_id": candidate})
            rep_exists = await rep_coll.find_one({"issue_id": candidate})
            if not url_exists and not rep_exists:
                final_issue_id = candidate
                break
        
    year_seq: int | None = None

    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° reuse year_seq ‡∏à‡∏≤‡∏Å PMReportDB ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (issue_id ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
    rep = await get_ccbpmreport_collection_for(station_id).find_one(
        {"issue_id": final_issue_id},
        {"year_seq": 1, "pm_date": 1},
    )
    if rep and rep.get("year_seq") is not None:
        year_seq = int(rep["year_seq"])

    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ year_seq ‡∏à‡∏≤‡∏Å PMReport ‚Üí ‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å pm_year_sequences
    if year_seq is None:
        year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

    year = d.year
    final_doc_name: str | None = None

    if doc_name:
        candidate = doc_name.strip()

        ok_format = candidate.startswith(f"{station_id}_")

        rep_coll = get_ccbpmreport_collection_for(station_id)
        rep_exists = await rep_coll.find_one({"doc_name": candidate})
        url_exists = await coll.find_one({"doc_name": candidate})
        unique = not (rep_exists or url_exists)

        if ok_format and unique:
            final_doc_name = candidate

    if not final_doc_name:
        final_doc_name = f"{station_id}_{year_seq}/{year}"

    doc_name = final_doc_name
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà /uploads/ccbpmurl/<station_id>/<YYYY-MM-DD>/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "ccbpmurl" / station_id / pm_date
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls, metas = [], []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url = f"/uploads/ccbpmurl/{station_id}/{pm_date}/{fname}"
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    inspector_clean = (inspector or "").strip() or None
    now = datetime.now(timezone.utc)
    res = await coll.insert_one({
        "station": station_id,
        "pm_date": pm_date,
        "issue_id": final_issue_id, 
        "inspector": inspector_clean,
        "doc_name": doc_name,
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    })
    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean,}

@app.get("/ccbpmurl/list")
async def ccbpmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_ccbpmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find(
        {},
        {"_id": 1, "issue_id": 1,"doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    items = []
    for it in items_raw:
        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""
        items.append({
            "id": str(it["_id"]),
            "pm_date": it.get("pm_date"),
            "issue_id": it.get("issue_id"),
            "inspector": it.get(("inspector")), 
            "doc_name": it.get("doc_name"),
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,
            "urls": urls,
        })

    return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# -------------------------------------------------- PMReportPage (CB-BOX)       
# def get_cbboxpmreport_collection_for(station_id: str):
#     _validate_station_id(station_id)
#     return CBBOXPMReportDB.get_collection(str(station_id))

# def get_cbboxpmurl_coll_upload(station_id: str):
#     _validate_station_id(station_id)
#     return CBBOXPMUrlDB.get_collection(str(station_id))

# class CBBOXPMSubmitIn(BaseModel):
#     side: Literal["pre", "post"]
#     station_id: str
#     job: Dict[str, Any]         # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô (location/date/inspector ‡∏Ø‡∏•‡∏Ø)
#     # rows: Dict[str, Dict[str, Any]]  # {"r1": {"pf": "...", "remark": "..."}, ...}
#     measures_pre: Dict[str, Dict[str, Any]]  # {"m4": {...}, "m5": {...}, ..., "m8": {...}}
#     # summary: str
#     pm_date: str                # "YYYY-MM-DD"
#     issue_id: Optional[str] = None
#     doc_name: Optional[str] = None 
#     # summaryCheck: Optional[Literal["PASS","FAIL","NA"]] = None
#     inspector: Optional[str] = None
#     dropdownQ1: Optional[str] = None  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° dropdown Q1
#     dropdownQ2: Optional[str] = None  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° dropdown Q2

# @app.get("/cbboxpmreport/preview-issueid")
# async def cbboxpmreport_preview_issueid(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     """
#     ‡∏î‡∏π issue_id ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (PM-CG-YYMM-XX) ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á
#     ‡πÉ‡∏ä‡πâ‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡πÑ‡∏õ‡πÇ‡∏ä‡∏ß‡πå‡∏ö‡∏ô‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÄ‡∏â‡∏¢ ‡πÜ
#     """
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     pm_type = "CB"

#     latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="cbbox")

#     yymm = f"{d.year % 100:02d}{d.month:02d}"
#     prefix = f"PM-{pm_type}-{yymm}-"

#     if not latest:
#         next_issue = f"{prefix}01"
#     else:
#         m = re.search(r"(\d+)$", latest)
#         cur = int(m.group(1)) if m else 0
#         next_issue = f"{prefix}{cur+1:02d}"

#     return {"issue_id": next_issue}

# @app.get("/cbboxpmreport/latest-docname")
# async def cbboxpmreport_latest_docname(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     """
#     ‡∏î‡∏∂‡∏á doc_name ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏õ‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö pm_date)
#     ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏•‡∏Ç‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà frontend
#     """
#     # auth ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    
#     latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="cbbox")
    
#     return {
#         "doc_name": latest.get("doc_name") if latest else None,
#         "station_id": station_id,
#         "pm_date": pm_date
#     }

# @app.get("/cbboxpmreport/preview-docname")
# async def preview_docname(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     year = d.year

#     latest = await _latest_doc_name_anywhere(station_id, year,source="cbbox")

#     if not latest:
#         next_doc = f"{station_id}_1/{year}"
#     else:
#         import re
#         m = re.search(r"_(\d+)/\d{4}$", latest)
#         current_num = int(m.group(1)) if m else 0
#         next_doc = f"{station_id}_{current_num + 1}/{year}"

#     return {"doc_name": next_doc}

# @app.post("/cbboxpmreport/pre/submit")
# async def cbboxpmreport_submit(body: CBBOXPMSubmitIn, current: UserClaims = Depends(get_current_user)):
#     station_id = body.station_id.strip()
#     coll = get_cbboxpmreport_collection_for(station_id)
#     db = coll.database

#     pm_type = str(body.job.get("pm_type") or "CB").upper()
#     body.job["pm_type"] = pm_type

#     url_coll = get_cbboxpmurl_coll_upload(station_id)

#     try:
#         d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     client_issue = body.issue_id 
#     issue_id: str | None = None

#     if client_issue:
#         yymm = f"{d.year % 100:02d}{d.month:02d}"
#         prefix = f"PM-{pm_type}-{yymm}-"
#         valid_fmt = client_issue.startswith(prefix)

#         rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
#         url_exists = await url_coll.find_one({"issue_id": client_issue})
#         unique = not await coll.find_one({"station_id": station_id, "issue_id": client_issue})
#         if valid_fmt and unique:
#             issue_id = client_issue

#     # if not issue_id:
#     #     issue_id = await _next_issue_id(db, station_id, pm_type, d, pad=2)
        
#     if not issue_id:
#         while True:
#             candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
#             rep_exists = await coll.find_one({"issue_id": candidate})
#             url_exists = await url_coll.find_one({"issue_id": candidate})
#             if not rep_exists and not url_exists:
#                 issue_id = candidate
#                 break

#     client_docName = body.doc_name
#     doc_name = None
#     if client_docName:
#         year = f"{d.year}"
#         prefix = f"{station_id}_"
#         valid_fmt = client_docName.startswith(prefix)

#         url_coll = get_cbboxpmurl_coll_upload(station_id)
#         rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
#         url_exists = await url_coll.find_one({"doc_name": client_docName})
#         unique = not (rep_exists or url_exists)

#         if valid_fmt and unique:
#             doc_name = client_docName
 
#     if not doc_name:
#         year_seq = await _next_year_seq(db, station_id, pm_type, d)
#         year = d.year
#         doc_name = f"{station_id}_{year_seq}/{year}"

#     doc = {
#         "station_id": station_id,
#         "doc_name": doc_name,
#         "issue_id": issue_id,
#         "job": body.job,
#         "dropdownQ1" : body.dropdownQ1,  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° dropdown Q1
#         "dropdownQ2" : body.dropdownQ2,  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° dropdown Q2
#         # "rows": body.rows,
#         "measures_pre": body.measures_pre,         # m4..m8
#         # "summary": body.summary,
#         # "summaryCheck": body.summaryCheck,
#         "pm_date": body.pm_date,           # string YYYY-MM-DD (‡∏ï‡∏≤‡∏°‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå)
#         "status": "draft",
#         "photos_pre": {},                      # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô /photos
#         "inspector": body.inspector,
#         "side": body.side,
#         "createdAt": datetime.now(timezone.utc),
#         # "updatedAt": datetime.now(timezone.utc),
#         # "timestamp": datetime.now(timezone.utc),
#     }

#     res = await coll.insert_one(doc)
#     return {
#         "ok": True, 
#         "report_id": str(res.inserted_id),
#         "issue_id": issue_id,
#         "doc_name": doc_name,
#     }
       
# class CBBOXPMPostIn(BaseModel):
#     report_id: str | None = None      # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°
#     station_id: str
#     # issue_id: str | None = None
#     # job: dict
#     rows: dict
#     measures: dict
#     summary: str
#     # pm_date: str
#     # doc_name: str | None = None
#     summaryCheck: str | None = None
#     dropdownQ1: Optional[str] = None  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° dropdown Q1
#     dropdownQ2: Optional[str] = None  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° dropdown Q2
#     # dust_filter: str | None = None
#     side: Literal["post", "after"]

# @app.post("/cbboxpmreport/submit")
# async def cbboxpmreport_submit(body: CBBOXPMPostIn, current: UserClaims = Depends(get_current_user)):
#     station_id = body.station_id.strip()
#     coll = get_cbboxpmreport_collection_for(station_id)
#     db = coll.database

#     url_coll = get_cbboxpmurl_coll_upload(station_id)

#     if body.report_id:
#         try:
#             oid = ObjectId(body.report_id)
#         except InvalidId:
#             raise HTTPException(status_code=400, detail="invalid report_id")

#         existing = await coll.find_one({"_id": oid, "station_id": station_id})
#         if not existing:
#             raise HTTPException(status_code=404, detail="Report not found")

#         update_fields = {
#             # "job": body.job,
#             "rows": body.rows,
#             "measures": body.measures,          # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á PM
#             "summary": body.summary,
#             "summaryCheck": body.summaryCheck,
#             # "pm_date": body.pm_date,
#             # "inspector": inspector,
#             # "dust_filter": body.dust_filter,
#             # "doc_name": doc_name,
#             "dropdownQ1": body.dropdownQ1,  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° dropdown Q1
#             "dropdownQ2": body.dropdownQ2,  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° dropdown Q2
#             "side": "post",                     # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ù‡∏±‡πà‡∏á post ‡πÅ‡∏•‡πâ‡∏ß
#             "updatedAt": datetime.now(timezone.utc),
#         }

#         await coll.update_one({"_id": oid}, {"$set": update_fields})

#         return {
#             "ok": True,
#             "report_id": body.report_id,
#             # "issue_id": issue_id,
#             # "doc_name": doc_name,
#         }

#     doc = {
#         "station_id": station_id,
#         # "doc_name": doc_name,
#         # "issue_id": issue_id,
#         # "job": body.job,
#         "rows": body.rows,
#         # "measures_pre": body.measures_pre,         # m4..m8
#         "summary": body.summary,
#         "summaryCheck": body.summaryCheck,
#         "measures": body.measures, 
#         # "pm_date": body.pm_date,           # string YYYY-MM-DD (‡∏ï‡∏≤‡∏°‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå)
#         "status": "draft",
#         "photos": {},                      # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô /photos
#         # "inspector": body.inspector,
#         "side": body.side,
#         # "createdAt": datetime.now(timezone.utc),
#         "updatedAt": datetime.now(timezone.utc),
#         # "timestamp": datetime.now(timezone.utc),
#     }

#     res = await coll.insert_one(doc)
#     return {
#         "ok": True, 
#         "report_id": str(res.inserted_id),
#         # "issue_id": issue_id,
#         # "doc_name": doc_name,
#     }

# @app.get("/cbboxpmreport/get")
# async def cbboxpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
#     coll = get_cbboxpmreport_collection_for(station_id)
#     doc = await coll.find_one({"_id": ObjectId(report_id)})
#     if not doc:
#         raise HTTPException(status_code=404, detail="not found")

#     doc["_id"] = str(doc["_id"])
#     return doc


# @app.get("/cbboxpmreport/list")
# async def cbboxpmreport_list(
#     station_id: str = Query(...),
#     page: int = Query(1, ge=1),
#     pageSize: int = Query(20, ge=1, le=100),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     coll = get_cbboxpmreport_collection_for(station_id)
#     skip = (page - 1) * pageSize

#     cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side":1, "createdAt": 1}).sort(
#         [("createdAt", -1), ("_id", -1)]
#     ).skip(skip).limit(pageSize)

#     items_raw = await cursor.to_list(length=pageSize)
#     total = await coll.count_documents({})

#     # ‡∏ú‡∏π‡∏Å URL PDF ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏à‡∏≤‡∏Å CCBPMUrlDB (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
#     pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
#     url_by_day: Dict[str, str] = {}
#     if pm_dates:
#         ucoll = get_cbboxpmurl_coll_upload(station_id)
#         ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
#         url_docs = await ucur.to_list(length=10_000)
#         for u in url_docs:
#             day = u.get("pm_date")
#             first_url = (u.get("urls") or [None])[0]
#             if day and first_url and day not in url_by_day:
#                 url_by_day[day] = first_url

#     items = [{
#         "id": str(it["_id"]),
#         "issue_id": it.get("issue_id"),
#         "doc_name": it.get("doc_name"),
#         "pm_date": it.get("pm_date"),
#         "inspector": it.get("inspector"),
#         "side":it.get("side"),
#         "createdAt": _ensure_utc_iso(it.get("createdAt")),
#         "file_url": url_by_day.get(it.get("pm_date") or "", ""),
#     } for it in items_raw]

#     return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# @app.post("/cbboxpmreport/{report_id}/pre/photos")
# async def cbboxpmreport_upload_photos(
#     report_id: str,
#     station_id: str = Form(...),
#     group: str = Form(...),                   # "g1" .. "g11"
#     files: List[UploadFile] = File(...),
#     # remark: Optional[str] = Form(None),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
#     if not re.fullmatch(r"g\d+", group):
#         raise HTTPException(status_code=400, detail="Bad group key")

#     coll = get_cbboxpmreport_collection_for(station_id)
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
#     if not doc:
#         raise HTTPException(status_code=404, detail="Report not found")
#     if doc.get("station_id") != station_id:
#         raise HTTPException(status_code=400, detail="station_id mismatch")

#     # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/cbboxpm/{station_id}/{report_id}/{group}/
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "cbboxpm" / station_id / report_id / "pre" / group
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     saved = []
#     for f in files:
#         ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
#         if ext not in ALLOWED_EXTS:
#             raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

#         data = await f.read()
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         url_path = f"/uploads/cbboxpm/{station_id}/{report_id}/pre/{group}/{fname}"
#         saved.append({
#             "filename": fname,
#             "size": len(data),
#             "url": url_path,
#             # "remark": remark or "",
#             "uploadedAt": datetime.now(timezone.utc)
#         })

#     await coll.update_one(
#         {"_id": oid},
#         {
#             "$push": {f"photos_pre.{group}": {"$each": saved}},
#             "$set": {"updatedAt": datetime.now(timezone.utc)}
#         }
#     )
#     return {"ok": True, "count": len(saved), "group": group, "files": saved}


# @app.post("/cbboxpmreport/{report_id}/post/photos")
# async def cbboxpmreport_upload_photos(
#     report_id: str,
#     station_id: str = Form(...),
#     group: str = Form(...),                   # "g1" .. "g11"
#     files: List[UploadFile] = File(...),
#     remark: Optional[str] = Form(None),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
#     if not re.fullmatch(r"g\d+", group):
#         raise HTTPException(status_code=400, detail="Bad group key")

#     coll = get_cbboxpmreport_collection_for(station_id)
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
#     if not doc:
#         raise HTTPException(status_code=404, detail="Report not found")
#     if doc.get("station_id") != station_id:
#         raise HTTPException(status_code=400, detail="station_id mismatch")

#     # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/cbboxpm/{station_id}/{report_id}/{group}/
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "cbboxpm" / station_id / report_id / "post" / group
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     saved = []
#     for f in files:
#         ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
#         if ext not in ALLOWED_EXTS:
#             raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

#         data = await f.read()
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         url_path = f"/uploads/cbboxpm/{station_id}/{report_id}/post/{group}/{fname}"
#         saved.append({
#             "filename": fname,
#             "size": len(data),
#             "url": url_path,
#             "remark": remark or "",
#             "uploadedAt": datetime.now(timezone.utc)
#         })

#     await coll.update_one(
#         {"_id": oid},
#         {
#             "$push": {f"photos.{group}": {"$each": saved}},
#             "$set": {"updatedAt": datetime.now(timezone.utc)}
#         }
#     )
#     return {"ok": True, "count": len(saved), "group": group, "files": saved}

# @app.post("/cbboxpmreport/{report_id}/finalize")
# async def cbboxpmreport_finalize(
#     report_id: str,
#     station_id: str = Form(...),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")

#     coll = get_cbboxpmreport_collection_for(station_id)
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     # (‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô) ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô finalize ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
#     res = await coll.update_one(
#         {"_id": oid},
#         {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
#     )
#     if res.matched_count == 0:
#         raise HTTPException(status_code=404, detail="Report not found")
#     return {"ok": True}

# @app.post("/cbboxpmurl/upload-files", status_code=201)
# async def cbboxpmurl_upload_files(
#     station_id: str = Form(...),
#     reportDate: str = Form(...),            # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO -> ‡∏à‡∏∞ normalize ‡πÄ‡∏õ‡πá‡∏ô YYYY-MM-DD
#     files: List[UploadFile] = File(...),    # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ .pdf
#     # current: UserClaims = Depends(get_current_user),
#     issue_id: Optional[str] = Form(None),
#     doc_name: Optional[str] = Form(None),
#     inspector: Optional[str] = Form(None),
# ):
#     coll = get_cbboxpmurl_coll_upload(station_id)
#     pm_date = normalize_pm_date(reportDate)  # ‡∏Ñ‡∏∑‡∏ô YYYY-MM-DD

#     pm_type = "CB"
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

#     rep_coll = get_cbboxpmreport_collection_for(station_id)
#     final_issue_id = None
#     client_issue = (issue_id or "").strip()

#     if client_issue:
#         yymm = f"{d.year % 100:02d}{d.month:02d}"
#         prefix = f"PM-{pm_type}-{yymm}-"
#         valid_fmt = client_issue.startswith(prefix)

#         # ‡∏ï‡∏£‡∏ß‡∏à uniqueness ‡πÉ‡∏ô‡∏ó‡∏±‡πâ‡∏á 2 ‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô
#         url_exists = await coll.find_one({"issue_id": client_issue})
#         rep_exists = await rep_coll.find_one({"issue_id": client_issue})
#         unique = not (url_exists or rep_exists)

#         if valid_fmt and unique:
#             final_issue_id = client_issue

#     # if not final_issue_id:
#     #     # ‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÅ‡∏ö‡∏ö atomic ‡∏à‡∏≤‡∏Å pm_sequences
#     #     final_issue_id = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
#     if not final_issue_id:
#         while True:
#             candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
#             url_exists = await coll.find_one({"issue_id": candidate})
#             rep_exists = await rep_coll.find_one({"issue_id": candidate})
#             if not url_exists and not rep_exists:
#                 final_issue_id = candidate
#                 break

#     year_seq: int | None = None
#     # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° reuse year_seq ‡∏à‡∏≤‡∏Å PMReportDB ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (issue_id ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
#     rep = await get_cbboxpmreport_collection_for(station_id).find_one(
#         {"issue_id": final_issue_id},
#         {"year_seq": 1, "pm_date": 1},
#     )
#     if rep and rep.get("year_seq") is not None:
#         year_seq = int(rep["year_seq"])

#     # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ year_seq ‡∏à‡∏≤‡∏Å PMReport ‚Üí ‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å pm_year_sequences
#     if year_seq is None:
#         year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

#     year = d.year
#     final_doc_name: str | None = None

#     if doc_name:
#         candidate = doc_name.strip()

#         ok_format = candidate.startswith(f"{station_id}_")

#         rep_coll = get_cbboxpmreport_collection_for(station_id)
#         rep_exists = await rep_coll.find_one({"doc_name": candidate})
#         url_exists = await coll.find_one({"doc_name": candidate})
#         unique = not (rep_exists or url_exists)

#         if ok_format and unique:
#             final_doc_name = candidate

#     if not final_doc_name:
#         final_doc_name = f"{station_id}_{year_seq}/{year}"

#     doc_name = final_doc_name

#     # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "cbboxpmurl" / station_id / pm_date
#     dest_dir.mkdir(parents=True, exist_ok=True)


#     urls, metas = [], []
#     for f in files:
#         ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
#         if ext != "pdf":
#             raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

#         data = await f.read()
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         url = f"/uploads/cbboxpmurl/{station_id}/{pm_date}/{fname}"
#         urls.append(url)
#         metas.append({"name": f.filename, "size": len(data)})
    
#     inspector_clean = (inspector or "").strip() or None
#     now = datetime.now(timezone.utc)
#     res = await coll.insert_one({
#         "station": station_id,
#         "issue_id": final_issue_id, 
#         "inspector": inspector_clean,
#         "doc_name": doc_name,
#         "pm_date": pm_date,
#         "urls": urls,
#         "meta": {"files": metas},
#         "source": "upload-files",
#         "createdAt": now,
#         "updatedAt": now,
#     })
#     return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean}

# @app.get("/cbboxpmurl/list")
# async def cbboxpmurl_list(
#     station_id: str = Query(...),
#     page: int = Query(1, ge=1),
#     pageSize: int = Query(20, ge=1, le=100),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     coll = get_cbboxpmurl_coll_upload(station_id)
#     skip = (page - 1) * pageSize

#     cursor = coll.find(
#         {},
#         {"_id": 1, "issue_id": 1,"doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
#     ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

#     items_raw = await cursor.to_list(length=pageSize)
#     total = await coll.count_documents({})

#     items = []
#     for it in items_raw:
#         urls = it.get("urls") or []
#         first_url = urls[0] if urls else ""
#         items.append({
#             "id": str(it["_id"]),
#             "pm_date": it.get("pm_date"),
#             "issue_id": it.get("issue_id"),
#             "inspector": it.get(("inspector")), 
#             "doc_name": it.get("doc_name"),
#             "createdAt": _ensure_utc_iso(it.get("createdAt")),
#             "file_url": first_url,
#             "urls": urls,
#         })

#     return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# ---------------------------------------------------------------------------
# new cb-box
# ---------------------------------------------------------------------------
def get_cbboxpmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    return CBBOXPMReportDB.get_collection(str(station_id))

def get_cbboxpmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    return CBBOXPMUrlDB.get_collection(str(station_id))

class CBBOXPMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    station_id: str
    job: Dict[str, Any]         # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô (location/date/inspector ‡∏Ø‡∏•‡∏Ø)
    rows_pre: Dict[str, Dict[str, Any]]  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° rows_pre {"r1": {"pf": "...", "remark": "..."}, ...}
    measures_pre: Dict[str, Dict[str, Any]]  # {"m5": {...}}
    pm_date: str                # "YYYY-MM-DD"
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None 
    inspector: Optional[str] = None
    dropdownQ1: Optional[str] = None  # ‚úÖ dropdown Q1
    dropdownQ2: Optional[str] = None  # ‚úÖ dropdown Q2
    comment_pre: Optional[str] = None  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° comment ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Pre mode

@app.get("/cbboxpmreport/preview-issueid")
async def cbboxpmreport_preview_issueid(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    ‡∏î‡∏π issue_id ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (PM-CG-YYMM-XX) ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á
    ‡πÉ‡∏ä‡πâ‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡πÑ‡∏õ‡πÇ‡∏ä‡∏ß‡πå‡∏ö‡∏ô‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÄ‡∏â‡∏¢ ‡πÜ
    """
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "CB"

    latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="cbbox")

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

@app.get("/cbboxpmreport/latest-docname")
async def cbboxpmreport_latest_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    ‡∏î‡∏∂‡∏á doc_name ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏õ‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö pm_date)
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏•‡∏Ç‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà frontend
    """
    latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="cbbox")
    
    return {
        "doc_name": latest.get("doc_name") if latest else None,
        "station_id": station_id,
        "pm_date": pm_date
    }

@app.get("/cbboxpmreport/preview-docname")
async def preview_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(station_id, year,source="cbbox")

    if not latest:
        next_doc = f"{station_id}_1/{year}"
    else:
        import re
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{station_id}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

@app.post("/cbboxpmreport/pre/submit")
async def cbboxpmreport_submit(body: CBBOXPMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_cbboxpmreport_collection_for(station_id)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "CB").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_cbboxpmurl_coll_upload(station_id)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    client_issue = body.issue_id 
    issue_id: str | None = None

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        if valid_fmt and unique:
            issue_id = client_issue

    if not issue_id:
        while True:
            candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
            rep_exists = await coll.find_one({"issue_id": candidate})
            url_exists = await url_coll.find_one({"issue_id": candidate})
            if not rep_exists and not url_exists:
                issue_id = candidate
                break

    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{station_id}_"
        valid_fmt = client_docName.startswith(prefix)

        url_coll = get_cbboxpmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, station_id, pm_type, d)
        year = d.year
        doc_name = f"{station_id}_{year_seq}/{year}"

    doc = {
        "station_id": station_id,
        "doc_name": doc_name,
        "issue_id": issue_id,
        "job": body.job,
        "dropdownQ1": body.dropdownQ1,  # ‚úÖ dropdown Q1
        "dropdownQ2": body.dropdownQ2,  # ‚úÖ dropdown Q2
        "rows_pre": body.rows_pre,      # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° rows_pre
        "measures_pre": body.measures_pre,
        "comment_pre": body.comment_pre,  # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° comment ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Pre mode
        "pm_date": body.pm_date,
        "status": "draft",
        "photos_pre": {},
        "inspector": body.inspector,
        "side": body.side,
        "createdAt": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id),
        "issue_id": issue_id,
        "doc_name": doc_name,
    }
       
class CBBOXPMPostIn(BaseModel):
    report_id: str | None = None
    station_id: str
    rows: dict
    measures: dict
    summary: str
    summaryCheck: str | None = None
    dropdownQ1: Optional[str] = None  # ‚úÖ dropdown Q1
    dropdownQ2: Optional[str] = None  # ‚úÖ dropdown Q2
    side: Literal["post", "after"]

@app.post("/cbboxpmreport/submit")
async def cbboxpmreport_submit(body: CBBOXPMPostIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_cbboxpmreport_collection_for(station_id)
    db = coll.database

    url_coll = get_cbboxpmurl_coll_upload(station_id)

    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "station_id": station_id})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        update_fields = {
            "rows": body.rows,
            "measures": body.measures,
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            "dropdownQ1": body.dropdownQ1,  # ‚úÖ dropdown Q1
            "dropdownQ2": body.dropdownQ2,  # ‚úÖ dropdown Q2
            "side": "post",
            "updatedAt": datetime.now(timezone.utc),
        }

        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
        }

    doc = {
        "station_id": station_id,
        "rows": body.rows,
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        "measures": body.measures, 
        "status": "draft",
        "photos": {},
        "side": body.side,
        "updatedAt": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id),
    }

@app.get("/cbboxpmreport/get")
async def cbboxpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    coll = get_cbboxpmreport_collection_for(station_id)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    return doc


@app.get("/cbboxpmreport/list")
async def cbboxpmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_cbboxpmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side":1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # ‡∏ú‡∏π‡∏Å URL PDF ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏à‡∏≤‡∏Å CCBPMUrlDB (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    url_by_day: Dict[str, str] = {}
    if pm_dates:
        ucoll = get_cbboxpmurl_coll_upload(station_id)
        ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "pm_date": it.get("pm_date"),
        "inspector": it.get("inspector"),
        "side":it.get("side"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

@app.post("/cbboxpmreport/{report_id}/pre/photos")
async def cbboxpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "g1" .. "g11"
    files: List[UploadFile] = File(...),
):
    if not re.fullmatch(r"g\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_cbboxpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/cbboxpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cbboxpm" / station_id / report_id / "pre" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/cbboxpm/{station_id}/{report_id}/pre/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos_pre.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}


@app.post("/cbboxpmreport/{report_id}/post/photos")
async def cbboxpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "g1" .. "g11"
    files: List[UploadFile] = File(...),
    remark: Optional[str] = Form(None),
):
    if not re.fullmatch(r"g\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_cbboxpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/cbboxpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cbboxpm" / station_id / report_id / "post" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/cbboxpm/{station_id}/{report_id}/post/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/cbboxpmreport/{report_id}/finalize")
async def cbboxpmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
):
    coll = get_cbboxpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/cbboxpmurl/upload-files", status_code=201)
async def cbboxpmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),            # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO -> ‡∏à‡∏∞ normalize ‡πÄ‡∏õ‡πá‡∏ô YYYY-MM-DD
    files: List[UploadFile] = File(...),    # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ .pdf
    issue_id: Optional[str] = Form(None),
    doc_name: Optional[str] = Form(None),
    inspector: Optional[str] = Form(None),
):
    coll = get_cbboxpmurl_coll_upload(station_id)
    pm_date = normalize_pm_date(reportDate)  # ‡∏Ñ‡∏∑‡∏ô YYYY-MM-DD

    pm_type = "CB"
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

    rep_coll = get_cbboxpmreport_collection_for(station_id)
    final_issue_id = None
    client_issue = (issue_id or "").strip()

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        # ‡∏ï‡∏£‡∏ß‡∏à uniqueness ‡πÉ‡∏ô‡∏ó‡∏±‡πâ‡∏á 2 ‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô
        url_exists = await coll.find_one({"issue_id": client_issue})
        rep_exists = await rep_coll.find_one({"issue_id": client_issue})
        unique = not (url_exists or rep_exists)

        if valid_fmt and unique:
            final_issue_id = client_issue

    if not final_issue_id:
        while True:
            candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
            url_exists = await coll.find_one({"issue_id": candidate})
            rep_exists = await rep_coll.find_one({"issue_id": candidate})
            if not url_exists and not rep_exists:
                final_issue_id = candidate
                break

    year_seq: int | None = None
    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° reuse year_seq ‡∏à‡∏≤‡∏Å PMReportDB ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (issue_id ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
    rep = await get_cbboxpmreport_collection_for(station_id).find_one(
        {"issue_id": final_issue_id},
        {"year_seq": 1, "pm_date": 1},
    )
    if rep and rep.get("year_seq") is not None:
        year_seq = int(rep["year_seq"])

    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ year_seq ‡∏à‡∏≤‡∏Å PMReport ‚Üí ‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å pm_year_sequences
    if year_seq is None:
        year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

    year = d.year
    final_doc_name: str | None = None

    if doc_name:
        candidate = doc_name.strip()

        ok_format = candidate.startswith(f"{station_id}_")

        rep_coll = get_cbboxpmreport_collection_for(station_id)
        rep_exists = await rep_coll.find_one({"doc_name": candidate})
        url_exists = await coll.find_one({"doc_name": candidate})
        unique = not (rep_exists or url_exists)

        if ok_format and unique:
            final_doc_name = candidate

    if not final_doc_name:
        final_doc_name = f"{station_id}_{year_seq}/{year}"

    doc_name = final_doc_name

    # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cbboxpmurl" / station_id / pm_date
    dest_dir.mkdir(parents=True, exist_ok=True)


    urls, metas = [], []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url = f"/uploads/cbboxpmurl/{station_id}/{pm_date}/{fname}"
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})
    
    inspector_clean = (inspector or "").strip() or None
    now = datetime.now(timezone.utc)
    res = await coll.insert_one({
        "station": station_id,
        "issue_id": final_issue_id, 
        "inspector": inspector_clean,
        "doc_name": doc_name,
        "pm_date": pm_date,
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    })
    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean}

@app.get("/cbboxpmurl/list")
async def cbboxpmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_cbboxpmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find(
        {},
        {"_id": 1, "issue_id": 1,"doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    items = []
    for it in items_raw:
        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""
        items.append({
            "id": str(it["_id"]),
            "pm_date": it.get("pm_date"),
            "issue_id": it.get("issue_id"),
            "inspector": it.get(("inspector")), 
            "doc_name": it.get("doc_name"),
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,
            "urls": urls,
        })

    return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}


# -------------------------------------------------- PMReportPage (station)       
# def get_stationpmreport_collection_for(station_id: str):
#     _validate_station_id(station_id)
#     return stationPMReportDB.get_collection(str(station_id))

# def get_stationpmurl_coll_upload(station_id: str):
#     _validate_station_id(station_id)
#     return stationPMUrlDB.get_collection(str(station_id))

# class stationPMSubmitIn(BaseModel):
#     side: Literal["pre", "post"]
#     station_id: str
#     job: Dict[str, Any]         # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô (location/date/inspector ‡∏Ø‡∏•‡∏Ø)
#     # rows: Dict[str, Dict[str, Any]]  # {"r1": {"pf": "...", "remark": "..."}, ...}
#     # measures: Dict[str, Dict[str, Any]]  # {"m4": {...}, "m5": {...}, ..., "m8": {...}}
#     # summary: str
#     pm_date: str                # "YYYY-MM-DD"
#     issue_id: Optional[str] = None
#     doc_name: Optional[str] = None 
#     # summaryCheck: Optional[Literal["PASS","FAIL","NA"]] = None
#     inspector: Optional[str] = None

# @app.get("/stationpmreport/preview-issueid")
# async def stationpmreport_preview_issueid(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     """
#     ‡∏î‡∏π issue_id ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (PM-CG-YYMM-XX) ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á
#     ‡πÉ‡∏ä‡πâ‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡πÑ‡∏õ‡πÇ‡∏ä‡∏ß‡πå‡∏ö‡∏ô‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÄ‡∏â‡∏¢ ‡πÜ
#     """
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     pm_type = "ST"

#     latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="station")

#     yymm = f"{d.year % 100:02d}{d.month:02d}"
#     prefix = f"PM-{pm_type}-{yymm}-"

#     if not latest:
#         next_issue = f"{prefix}01"
#     else:
#         m = re.search(r"(\d+)$", latest)
#         cur = int(m.group(1)) if m else 0
#         next_issue = f"{prefix}{cur+1:02d}"

#     return {"issue_id": next_issue}

# @app.get("/stationpmreport/latest-docname")
# async def stationpmreport_latest_docname(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     """
#     ‡∏î‡∏∂‡∏á doc_name ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏õ‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö pm_date)
#     ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏•‡∏Ç‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà frontend
#     """
#     latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="station")
    
#     return {
#         "doc_name": latest.get("doc_name") if latest else None,
#         "station_id": station_id,
#         "pm_date": pm_date
#     }

# @app.get("/stationpmreport/preview-docname")
# async def preview_docname(
#     station_id: str = Query(...),
#     pm_date: str = Query(...),
#     current: UserClaims = Depends(get_current_user),
# ):
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     year = d.year

#     latest = await _latest_doc_name_anywhere(station_id, year,source="station")

#     if not latest:
#         next_doc = f"{station_id}_1/{year}"
#     else:
#         import re
#         m = re.search(r"_(\d+)/\d{4}$", latest)
#         current_num = int(m.group(1)) if m else 0
#         next_doc = f"{station_id}_{current_num + 1}/{year}"

#     return {"doc_name": next_doc}

# @app.post("/stationpmreport/pre/submit")
# async def stationpmreport_submit(body: stationPMSubmitIn, current: UserClaims = Depends(get_current_user)):
#     station_id = body.station_id.strip()
#     coll = get_stationpmreport_collection_for(station_id)
#     db = coll.database

#     pm_type = str(body.job.get("pm_type") or "ST").upper()
#     body.job["pm_type"] = pm_type

#     url_coll = get_stationpmurl_coll_upload(station_id)

#     try:
#         d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

#     client_issue = body.issue_id 
#     issue_id: str | None = None
#     if client_issue:
#         yymm = f"{d.year % 100:02d}{d.month:02d}"
#         prefix = f"PM-{pm_type}-{yymm}-"
#         valid_fmt = client_issue.startswith(prefix)
        
#         rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
#         url_exists = await url_coll.find_one({"issue_id": client_issue})
#         unique = not await coll.find_one({"station_id": station_id, "issue_id": client_issue})
#         if valid_fmt and unique:
#             issue_id = client_issue

#     # if not issue_id:
#     #     issue_id = await _next_issue_id(db, station_id, pm_type, d, pad=2)
#     if not issue_id:
#             while True:
#                 candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
#                 rep_exists = await coll.find_one({"issue_id": candidate})
#                 url_exists = await url_coll.find_one({"issue_id": candidate})
#                 if not rep_exists and not url_exists:
#                     issue_id = candidate
#                     break  

#     client_docName = body.doc_name
#     doc_name = None
#     if client_docName:
#         year = f"{d.year}"
#         prefix = f"{station_id}_"
#         valid_fmt = client_docName.startswith(prefix)

#         url_coll = get_stationpmurl_coll_upload(station_id)
#         rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
#         url_exists = await url_coll.find_one({"doc_name": client_docName})
#         unique = not (rep_exists or url_exists)

#         if valid_fmt and unique:
#             doc_name = client_docName
 
#     if not doc_name:
#         year_seq = await _next_year_seq(db, station_id, pm_type, d)
#         year = d.year
#         doc_name = f"{station_id}_{year_seq}/{year}"

#     # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô draft ‡∏Å‡πà‡∏≠‡∏ô
#     doc = {
#         "station_id": station_id,
#         "issue_id": issue_id,
#         "doc_name": doc_name,
#         "job": body.job,
#         # "rows": body.rows,
#         # "measures": body.measures,         # m4..m8
#         # "summary": body.summary,
#         # "summaryCheck": body.summaryCheck,
#         "pm_date": body.pm_date,           # string YYYY-MM-DD (‡∏ï‡∏≤‡∏°‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå)
#         "status": "draft",
#         "photos_pre": {},                      # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô /photos
#         "side": body.side,
#         "inspector": body.inspector,
#         "createdAt": datetime.now(timezone.utc),
#         # "updatedAt": datetime.now(timezone.utc),
#     }

#     res = await coll.insert_one(doc)
#     return {
#         "ok": True, 
#         "report_id": str(res.inserted_id),
#         "issue_id": issue_id,
#         "doc_name": doc_name,
#     }

# class stationPMPostIn(BaseModel):
#     report_id: str | None = None      # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°
#     station_id: str
#     # issue_id: str | None = None
#     # job: dict
#     rows: dict
#     # measures: dict
#     summary: str
#     # pm_date: str
#     # doc_name: str | None = None
#     summaryCheck: str | None = None
#     # dust_filter: str | None = None
#     side: Literal["post", "after"]

# @app.post("/stationpmreport/submit")
# async def stationpmreport_submit(body: stationPMPostIn, current: UserClaims = Depends(get_current_user)):
#     station_id = body.station_id.strip()
#     coll = get_stationpmreport_collection_for(station_id)
#     db = coll.database

#     # pm_type = str(body.job.get("pm_type") or "ST").upper()
#     # body.job["pm_type"] = pm_type

#     url_coll = get_stationpmurl_coll_upload(station_id)

#     if body.report_id:
#         try:
#             oid = ObjectId(body.report_id)
#         except InvalidId:
#             raise HTTPException(status_code=400, detail="invalid report_id")

#         existing = await coll.find_one({"_id": oid, "station_id": station_id})
#         if not existing:
#             raise HTTPException(status_code=404, detail="Report not found")

#         update_fields = {
#             # "job": body.job,
#             "rows": body.rows,
#             # "measures": body.measures,          # ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏´‡∏•‡∏±‡∏á PM
#             "summary": body.summary,
#             "summaryCheck": body.summaryCheck,
#             # "pm_date": body.pm_date,
#             # "inspector": inspector,
#             # "dust_filter": body.dust_filter,
#             # "doc_name": doc_name,
#             "side": "post",                     # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏ù‡∏±‡πà‡∏á post ‡πÅ‡∏•‡πâ‡∏ß
#             "updatedAt": datetime.now(timezone.utc),
#         }
    
#         await coll.update_one({"_id": oid}, {"$set": update_fields})

#         return {
#             "ok": True,
#             "report_id": body.report_id,
#             # "issue_id": issue_id,
#             # "doc_name": doc_name,
#         }
#     # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô draft ‡∏Å‡πà‡∏≠‡∏ô
#     doc = {
#         "station_id": station_id,
#         # "issue_id": issue_id,
#         # "doc_name": doc_name,
#         # "job": body.job,
#         "rows": body.rows,
#         # "measures": body.measures,         # m4..m8
#         "summary": body.summary,
#         "summaryCheck": body.summaryCheck,
#         # "pm_date": body.pm_date,           # string YYYY-MM-DD (‡∏ï‡∏≤‡∏°‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå)
#         "status": "draft",
#         "photos": {},                      # ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡πÉ‡∏ô /photos
#         # "inspector": body.inspector,
#         # "createdAt": datetime.now(timezone.utc),
#         "side": "post",
#         "updatedAt": datetime.now(timezone.utc),
#     }

#     res = await coll.insert_one(doc)
#     return {"ok": True, "report_id": str(res.inserted_id)}

# @app.get("/stationpmreport/get")
# async def stationpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
#     coll = get_stationpmreport_collection_for(station_id)
#     doc = await coll.find_one({"_id": ObjectId(report_id)})
#     if not doc:
#         raise HTTPException(status_code=404, detail="not found")

#     doc["_id"] = str(doc["_id"])
#     return doc

# @app.get("/stationpmreport/list")
# async def ccbpmreport_list(
#     station_id: str = Query(...),
#     page: int = Query(1, ge=1),
#     pageSize: int = Query(20, ge=1, le=100),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     coll = get_stationpmreport_collection_for(station_id)
#     skip = (page - 1) * pageSize

#     cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side":1, "createdAt": 1}).sort(
#         [("createdAt", -1), ("_id", -1)]
#     ).skip(skip).limit(pageSize)

#     items_raw = await cursor.to_list(length=pageSize)
#     total = await coll.count_documents({})

#     # ‡∏ú‡∏π‡∏Å URL PDF ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏à‡∏≤‡∏Å MDBPMUrlDB (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
#     pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
#     url_by_day: Dict[str, str] = {}
#     if pm_dates:
#         ucoll = get_stationpmurl_coll_upload(station_id)
#         ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
#         url_docs = await ucur.to_list(length=10_000)
#         for u in url_docs:
#             day = u.get("pm_date")
#             first_url = (u.get("urls") or [None])[0]
#             if day and first_url and day not in url_by_day:
#                 url_by_day[day] = first_url

#     items = [{
#         "id": str(it["_id"]),
#         "issue_id": it.get("issue_id"),
#         "doc_name": it.get("doc_name"),
#         "pm_date": it.get("pm_date"),
#         "side":it.get("side"),
#         "inspector": it.get("inspector"),
#         "createdAt": _ensure_utc_iso(it.get("createdAt")),
#         "file_url": url_by_day.get(it.get("pm_date") or "", ""),
#     } for it in items_raw]

#     return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# @app.post("/stationpmreport/{report_id}/pre/photos")
# async def stationpmreport_upload_photos(
#     report_id: str,
#     station_id: str = Form(...),
#     group: str = Form(...),                   # "r1" .. "r10"
#     files: List[UploadFile] = File(...),
#     # remark: Optional[str] = Form(None),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
#     if not re.fullmatch(r"r\d+", group):
#         raise HTTPException(status_code=400, detail="Bad group key")

#     coll = get_stationpmreport_collection_for(station_id)
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
#     if not doc:
#         raise HTTPException(status_code=404, detail="Report not found")
#     if doc.get("station_id") != station_id:
#         raise HTTPException(status_code=400, detail="station_id mismatch")

#     # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/mdbpm/{station_id}/{report_id}/{group}/
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "stationpm" / station_id / report_id / "pre" / group
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     saved = []
#     for f in files:
#         ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
#         if ext not in ALLOWED_EXTS:
#             raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

#         data = await f.read()
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         url_path = f"/uploads/stationpm/{station_id}/{report_id}/pre/{group}/{fname}"
#         saved.append({
#             "filename": fname,
#             "size": len(data),
#             "url": url_path,
#             # "remark": remark or "",
#             "uploadedAt": datetime.now(timezone.utc)
#         })

#     await coll.update_one(
#         {"_id": oid},
#         {
#             "$push": {f"photos_pre.{group}": {"$each": saved}},
#             "$set": {"updatedAt": datetime.now(timezone.utc)}
#         }
#     )
#     return {"ok": True, "count": len(saved), "group": group, "files": saved}

# @app.post("/stationpmreport/{report_id}/post/photos")
# async def stationpmreport_upload_photos(
#     report_id: str,
#     station_id: str = Form(...),
#     group: str = Form(...),                   # "r1" .. "r10"
#     files: List[UploadFile] = File(...),
#     remark: Optional[str] = Form(None),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")
#     if not re.fullmatch(r"r\d+", group):
#         raise HTTPException(status_code=400, detail="Bad group key")

#     coll = get_stationpmreport_collection_for(station_id)
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
#     if not doc:
#         raise HTTPException(status_code=404, detail="Report not found")
#     if doc.get("station_id") != station_id:
#         raise HTTPException(status_code=400, detail="station_id mismatch")

#     # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/mdbpm/{station_id}/{report_id}/{group}/
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "stationpm" / station_id / report_id / "post" / group
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     saved = []
#     for f in files:
#         ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
#         if ext not in ALLOWED_EXTS:
#             raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

#         data = await f.read()
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         url_path = f"/uploads/stationpm/{station_id}/{report_id}/post/{group}/{fname}"
#         saved.append({
#             "filename": fname,
#             "size": len(data),
#             "url": url_path,
#             "remark": remark or "",
#             "uploadedAt": datetime.now(timezone.utc)
#         })

#     await coll.update_one(
#         {"_id": oid},
#         {
#             "$push": {f"photos.{group}": {"$each": saved}},
#             "$set": {"updatedAt": datetime.now(timezone.utc)}
#         }
#     )
#     return {"ok": True, "count": len(saved), "group": group, "files": saved}

# @app.post("/stationpmreport/{report_id}/finalize")
# async def stationpmreport_finalize(
#     report_id: str,
#     station_id: str = Form(...),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     # if current.role != "admin" and station_id not in set(current.station_ids):
#     #     raise HTTPException(status_code=403, detail="Forbidden station_id")

#     coll = get_stationpmreport_collection_for(station_id)
#     try:
#         oid = ObjectId(report_id)
#     except Exception:
#         raise HTTPException(status_code=400, detail="Bad report_id")

#     # (‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô) ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô finalize ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
#     res = await coll.update_one(
#         {"_id": oid},
#         {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
#     )
#     if res.matched_count == 0:
#         raise HTTPException(status_code=404, detail="Report not found")
#     return {"ok": True}

# @app.post("/stationpmurl/upload-files", status_code=201)
# async def stationmurl_upload_files(
#     station_id: str = Form(...),
#     reportDate: str = Form(...),            # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO -> ‡∏à‡∏∞ normalize ‡πÄ‡∏õ‡πá‡∏ô YYYY-MM-DD
#     files: List[UploadFile] = File(...),    # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ .pdf
#     # current: UserClaims = Depends(get_current_user),
#     issue_id: Optional[str] = Form(None),
#     doc_name: Optional[str] = Form(None),
#     inspector: Optional[str] = Form(None),
# ):
#     coll = get_stationpmurl_coll_upload(station_id)
#     pm_date = normalize_pm_date(reportDate)  # ‡∏Ñ‡∏∑‡∏ô YYYY-MM-DD

#     pm_type = "ST"
#     try:
#         d = datetime.strptime(pm_date, "%Y-%m-%d").date()
#     except ValueError:
#         raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

#     rep_coll = get_stationpmreport_collection_for(station_id)
#     final_issue_id = None
#     client_issue = (issue_id or "").strip()

#     if client_issue:
#         yymm = f"{d.year % 100:02d}{d.month:02d}"
#         prefix = f"PM-{pm_type}-{yymm}-"
#         valid_fmt = client_issue.startswith(prefix)

#         # ‡∏ï‡∏£‡∏ß‡∏à uniqueness ‡πÉ‡∏ô‡∏ó‡∏±‡πâ‡∏á 2 ‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô
#         url_exists = await coll.find_one({"issue_id": client_issue})
#         rep_exists = await rep_coll.find_one({"issue_id": client_issue})
#         unique = not (url_exists or rep_exists)

#         if valid_fmt and unique:
#             final_issue_id = client_issue

#     if not final_issue_id:
#         while True:
#             candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
#             url_exists = await coll.find_one({"issue_id": candidate})
#             rep_exists = await rep_coll.find_one({"issue_id": candidate})
#             if not url_exists and not rep_exists:
#                 final_issue_id = candidate
#                 break

#     year_seq: int | None = None
#     # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° reuse year_seq ‡∏à‡∏≤‡∏Å PMReportDB ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (issue_id ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
#     rep = await get_stationpmreport_collection_for(station_id).find_one(
#         {"issue_id": final_issue_id},
#         {"year_seq": 1, "pm_date": 1},
#     )
#     if rep and rep.get("year_seq") is not None:
#         year_seq = int(rep["year_seq"])

#     # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ year_seq ‡∏à‡∏≤‡∏Å PMReport ‚Üí ‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å pm_year_sequences
#     if year_seq is None:
#         year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

#     year = d.year
#     final_doc_name: str | None = None

#     if doc_name:
#         candidate = doc_name.strip()

#         ok_format = candidate.startswith(f"{station_id}_")

#         rep_coll = get_stationpmreport_collection_for(station_id)
#         rep_exists = await rep_coll.find_one({"doc_name": candidate})
#         url_exists = await coll.find_one({"doc_name": candidate})
#         unique = not (rep_exists or url_exists)

#         if ok_format and unique:
#             final_doc_name = candidate

#     if not final_doc_name:
#         final_doc_name = f"{station_id}_{year_seq}/{year}"

#     doc_name = final_doc_name

#     # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
#     dest_dir = pathlib.Path(UPLOADS_ROOT) / "stationpmurl" / station_id / pm_date
#     dest_dir.mkdir(parents=True, exist_ok=True)

#     urls, metas = [], []
#     for f in files:
#         ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
#         if ext != "pdf":
#             raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

#         data = await f.read()
#         if len(data) > MAX_FILE_MB * 1024 * 1024:
#             raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

#         fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
#         path = dest_dir / fname
#         with open(path, "wb") as out:
#             out.write(data)

#         url = f"/uploads/stationpmurl/{station_id}/{pm_date}/{fname}"
#         urls.append(url)
#         metas.append({"name": f.filename, "size": len(data)})

#     inspector_clean = (inspector or "").strip() or None
#     now = datetime.now(timezone.utc)
#     res = await coll.insert_one({
#         "station": station_id,
#         "pm_date": pm_date,
#         "issue_id": final_issue_id, 
#         "inspector": inspector_clean,
#         "doc_name": doc_name,
#         "urls": urls,
#         "meta": {"files": metas},
#         "source": "upload-files",
#         "createdAt": now,
#         "updatedAt": now,
#     })
#     return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean}

# @app.get("/stationpmurl/list")
# async def stationpmurl_list(
#     station_id: str = Query(...),
#     page: int = Query(1, ge=1),
#     pageSize: int = Query(20, ge=1, le=100),
#     # current: UserClaims = Depends(get_current_user),
# ):
#     coll = get_stationpmurl_coll_upload(station_id)
#     skip = (page - 1) * pageSize

#     cursor = coll.find(
#         {},
#         {"_id": 1, "issue_id": 1,"doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
#     ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

#     items_raw = await cursor.to_list(length=pageSize)
#     total = await coll.count_documents({})

#     items = []
#     for it in items_raw:
#         urls = it.get("urls") or []
#         first_url = urls[0] if urls else ""
#         items.append({
#             "id": str(it["_id"]),
#             "pm_date": it.get("pm_date"),
#             "issue_id": it.get("issue_id"),
#             "inspector": it.get(("inspector")), 
#             "doc_name": it.get("doc_name"),
#             "createdAt": _ensure_utc_iso(it.get("createdAt")),
#             "file_url": first_url,
#             "urls": urls,
#         })

#     return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

# -------------------------------------------------------------------------------
# new PM Report (Station)
# -------------------------------------------------------------------------------
def get_stationpmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    return stationPMReportDB.get_collection(str(station_id))

def get_stationpmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    return stationPMUrlDB.get_collection(str(station_id))

class stationPMSubmitIn(BaseModel):
    side: Literal["pre", "post"]
    station_id: str
    job: Dict[str, Any]         # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô (location/date/inspector ‡∏Ø‡∏•‡∏Ø)
    rows_pre: Dict[str, Dict[str, Any]]
    pm_date: str                # "YYYY-MM-DD"
    issue_id: Optional[str] = None
    doc_name: Optional[str] = None 
    comment_pre: Optional[str] = None
    inspector: Optional[str] = None

@app.get("/stationpmreport/preview-issueid")
async def stationpmreport_preview_issueid(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    ‡∏î‡∏π issue_id ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (PM-CG-YYMM-XX) ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÄ‡∏•‡∏Ç‡∏à‡∏£‡∏¥‡∏á
    ‡πÉ‡∏ä‡πâ‡∏´‡∏≤‡πÄ‡∏•‡∏Ç‡πÑ‡∏õ‡πÇ‡∏ä‡∏ß‡πå‡∏ö‡∏ô‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÄ‡∏â‡∏¢ ‡πÜ
    """
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    pm_type = "ST"

    latest = await _latest_issue_id_anywhere(station_id, pm_type, d,source="station")

    yymm = f"{d.year % 100:02d}{d.month:02d}"
    prefix = f"PM-{pm_type}-{yymm}-"

    if not latest:
        next_issue = f"{prefix}01"
    else:
        m = re.search(r"(\d+)$", latest)
        cur = int(m.group(1)) if m else 0
        next_issue = f"{prefix}{cur+1:02d}"

    return {"issue_id": next_issue}

@app.get("/stationpmreport/latest-docname")
async def stationpmreport_latest_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """
    ‡∏î‡∏∂‡∏á doc_name ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ (‡∏õ‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö pm_date)
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏•‡∏Ç‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà frontend
    """
    latest = await _latest_doc_name_from_pmreport(station_id, pm_date, source="station")
    
    return {
        "doc_name": latest.get("doc_name") if latest else None,
        "station_id": station_id,
        "pm_date": pm_date
    }

@app.get("/stationpmreport/preview-docname")
async def preview_docname(
    station_id: str = Query(...),
    pm_date: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    year = d.year

    latest = await _latest_doc_name_anywhere(station_id, year,source="station")

    if not latest:
        next_doc = f"{station_id}_1/{year}"
    else:
        import re
        m = re.search(r"_(\d+)/\d{4}$", latest)
        current_num = int(m.group(1)) if m else 0
        next_doc = f"{station_id}_{current_num + 1}/{year}"

    return {"doc_name": next_doc}

@app.post("/stationpmreport/pre/submit")
async def stationpmreport_submit(body: stationPMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    coll = get_stationpmreport_collection_for(station_id)
    db = coll.database

    pm_type = str(body.job.get("pm_type") or "ST").upper()
    body.job["pm_type"] = pm_type

    url_coll = get_stationpmurl_coll_upload(station_id)

    try:
        d = datetime.strptime(body.pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="pm_date must be YYYY-MM-DD")

    # === Issue ID Generation ===
    client_issue = body.issue_id 
    issue_id: str | None = None
    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)
        
        rep_exists = await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        url_exists = await url_coll.find_one({"issue_id": client_issue})
        unique = not await coll.find_one({"station_id": station_id, "issue_id": client_issue})
        if valid_fmt and unique:
            issue_id = client_issue

    if not issue_id:
        while True:
            candidate = await _next_issue_id(db, station_id, pm_type, d, pad=2)
            rep_exists = await coll.find_one({"issue_id": candidate})
            url_exists = await url_coll.find_one({"issue_id": candidate})
            if not rep_exists and not url_exists:
                issue_id = candidate
                break  

    # === Doc Name Generation ===
    client_docName = body.doc_name
    doc_name = None
    if client_docName:
        year = f"{d.year}"
        prefix = f"{station_id}_"
        valid_fmt = client_docName.startswith(prefix)

        url_coll = get_stationpmurl_coll_upload(station_id)
        rep_exists = await coll.find_one({"station_id": station_id, "doc_name": client_docName})
        url_exists = await url_coll.find_one({"doc_name": client_docName})
        unique = not (rep_exists or url_exists)

        if valid_fmt and unique:
            doc_name = client_docName
 
    if not doc_name:
        year_seq = await _next_year_seq(db, station_id, pm_type, d)
        year = d.year
        doc_name = f"{station_id}_{year_seq}/{year}"

    # === Create Document ===
    doc = {
        "station_id": station_id,
        "issue_id": issue_id,
        "doc_name": doc_name,
        "job": body.job,
        "rows_pre": body.rows_pre,        # ‚úÖ NEW: Store Pre mode rows (pf + remark)
        "comment_pre": body.comment_pre,  # ‚úÖ NEW: Store Pre mode comment
        "pm_date": body.pm_date,
        "status": "draft",
        "photos_pre": {},
        "side": body.side,
        "inspector": body.inspector,
        "createdAt": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {
        "ok": True, 
        "report_id": str(res.inserted_id),
        "issue_id": issue_id,
        "doc_name": doc_name,
    }

class stationPMPostIn(BaseModel):
    report_id: str | None = None      # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°
    station_id: str
    rows: dict
    summary: str
    summaryCheck: str | None = None
    side: Literal["post", "after"]

@app.post("/stationpmreport/submit")
async def stationpmreport_post_submit(body: stationPMPostIn, current: UserClaims = Depends(get_current_user)):
    """Post mode submission - updates existing report"""
    station_id = body.station_id.strip()
    coll = get_stationpmreport_collection_for(station_id)
    db = coll.database

    url_coll = get_stationpmurl_coll_upload(station_id)

    if body.report_id:
        try:
            oid = ObjectId(body.report_id)
        except InvalidId:
            raise HTTPException(status_code=400, detail="invalid report_id")

        existing = await coll.find_one({"_id": oid, "station_id": station_id})
        if not existing:
            raise HTTPException(status_code=404, detail="Report not found")

        update_fields = {
            "rows": body.rows,
            "summary": body.summary,
            "summaryCheck": body.summaryCheck,
            "side": "post",
            "updatedAt": datetime.now(timezone.utc),
        }
    
        await coll.update_one({"_id": oid}, {"$set": update_fields})

        return {
            "ok": True,
            "report_id": body.report_id,
        }

    # Fallback: create new document (shouldn't happen normally)
    doc = {
        "station_id": station_id,
        "rows": body.rows,
        "summary": body.summary,
        "summaryCheck": body.summaryCheck,
        "status": "draft",
        "photos": {},
        "side": "post",
        "updatedAt": datetime.now(timezone.utc),
    }

    res = await coll.insert_one(doc)
    return {"ok": True, "report_id": str(res.inserted_id)}

@app.get("/stationpmreport/get")
async def stationpmreport_get(station_id: str, report_id: str, current: UserClaims = Depends(get_current_user)):
    """Get report - returns all fields including rows_pre and comment_pre"""
    coll = get_stationpmreport_collection_for(station_id)
    doc = await coll.find_one({"_id": ObjectId(report_id)})
    if not doc:
        raise HTTPException(status_code=404, detail="not found")

    doc["_id"] = str(doc["_id"])
    return doc

@app.get("/stationpmreport/list")
async def ccbpmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_stationpmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "issue_id": 1, "doc_name": 1, "pm_date": 1, "inspector" : 1,"side":1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # ‡∏ú‡∏π‡∏Å URL PDF ‡∏£‡∏≤‡∏¢‡∏ß‡∏±‡∏ô‡∏à‡∏≤‡∏Å MDBPMUrlDB (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    pm_dates = [it.get("pm_date") for it in items_raw if it.get("pm_date")]
    url_by_day: Dict[str, str] = {}
    if pm_dates:
        ucoll = get_stationpmurl_coll_upload(station_id)
        ucur = ucoll.find({"pm_date": {"$in": pm_dates}}, {"pm_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("pm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "issue_id": it.get("issue_id"),
        "doc_name": it.get("doc_name"),
        "pm_date": it.get("pm_date"),
        "side":it.get("side"),
        "inspector": it.get("inspector"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("pm_date") or "", ""),
    } for it in items_raw]

    return {"items": items, "pm_date": [it.get("pm_date") for it in items_raw if it.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}

@app.post("/stationpmreport/{report_id}/pre/photos")
async def stationpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "r1" .. "r10"
    files: List[UploadFile] = File(...),
    # remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"r\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_stationpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/mdbpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "stationpm" / station_id / report_id / "pre" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/stationpm/{station_id}/{report_id}/pre/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            # "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos_pre.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/stationpmreport/{report_id}/post/photos")
async def stationpmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # "r1" .. "r10"
    files: List[UploadFile] = File(...),
    remark: Optional[str] = Form(None),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"r\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_stationpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: /uploads/mdbpm/{station_id}/{report_id}/{group}/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "stationpm" / station_id / report_id / "post" / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/stationpm/{station_id}/{report_id}/post/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    await coll.update_one(
        {"_id": oid},
        {
            "$push": {f"photos.{group}": {"$each": saved}},
            "$set": {"updatedAt": datetime.now(timezone.utc)}
        }
    )
    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/stationpmreport/{report_id}/finalize")
async def stationpmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    # current: UserClaims = Depends(get_current_user),
):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_stationpmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # (‡∏≠‡∏≠‡∏õ‡∏ä‡∏±‡∏ô) ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô finalize ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc), "updatedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/stationpmurl/upload-files", status_code=201)
async def stationmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),            # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO -> ‡∏à‡∏∞ normalize ‡πÄ‡∏õ‡πá‡∏ô YYYY-MM-DD
    files: List[UploadFile] = File(...),    # ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ .pdf
    # current: UserClaims = Depends(get_current_user),
    issue_id: Optional[str] = Form(None),
    doc_name: Optional[str] = Form(None),
    inspector: Optional[str] = Form(None),
):
    coll = get_stationpmurl_coll_upload(station_id)
    pm_date = normalize_pm_date(reportDate)  # ‡∏Ñ‡∏∑‡∏ô YYYY-MM-DD

    pm_type = "ST"
    try:
        d = datetime.strptime(pm_date, "%Y-%m-%d").date()
    except ValueError:
        raise HTTPException(status_code=400, detail="reportDate/pm_date must be YYYY-MM-DD")

    rep_coll = get_stationpmreport_collection_for(station_id)
    final_issue_id = None
    client_issue = (issue_id or "").strip()

    if client_issue:
        yymm = f"{d.year % 100:02d}{d.month:02d}"
        prefix = f"PM-{pm_type}-{yymm}-"
        valid_fmt = client_issue.startswith(prefix)

        # ‡∏ï‡∏£‡∏ß‡∏à uniqueness ‡πÉ‡∏ô‡∏ó‡∏±‡πâ‡∏á 2 ‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô
        url_exists = await coll.find_one({"issue_id": client_issue})
        rep_exists = await rep_coll.find_one({"issue_id": client_issue})
        unique = not (url_exists or rep_exists)

        if valid_fmt and unique:
            final_issue_id = client_issue

    if not final_issue_id:
        while True:
            candidate = await _next_issue_id(coll.database, station_id, pm_type, d, pad=2)
            url_exists = await coll.find_one({"issue_id": candidate})
            rep_exists = await rep_coll.find_one({"issue_id": candidate})
            if not url_exists and not rep_exists:
                final_issue_id = candidate
                break

    year_seq: int | None = None
    # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° reuse year_seq ‡∏à‡∏≤‡∏Å PMReportDB ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß (issue_id ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
    rep = await get_stationpmreport_collection_for(station_id).find_one(
        {"issue_id": final_issue_id},
        {"year_seq": 1, "pm_date": 1},
    )
    if rep and rep.get("year_seq") is not None:
        year_seq = int(rep["year_seq"])

    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ year_seq ‡∏à‡∏≤‡∏Å PMReport ‚Üí ‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å pm_year_sequences
    if year_seq is None:
        year_seq = await _next_year_seq(coll.database, station_id, pm_type, d)

    year = d.year
    final_doc_name: str | None = None

    if doc_name:
        candidate = doc_name.strip()

        ok_format = candidate.startswith(f"{station_id}_")

        rep_coll = get_stationpmreport_collection_for(station_id)
        rep_exists = await rep_coll.find_one({"doc_name": candidate})
        url_exists = await coll.find_one({"doc_name": candidate})
        unique = not (rep_exists or url_exists)

        if ok_format and unique:
            final_doc_name = candidate

    if not final_doc_name:
        final_doc_name = f"{station_id}_{year_seq}/{year}"

    doc_name = final_doc_name

    # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà /uploads/mdbpmurl/<station_id>/<YYYY-MM-DD>/
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "stationpmurl" / station_id / pm_date
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls, metas = [], []
    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if f.filename and "." in f.filename else "")
        if ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url = f"/uploads/stationpmurl/{station_id}/{pm_date}/{fname}"
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    inspector_clean = (inspector or "").strip() or None
    now = datetime.now(timezone.utc)
    res = await coll.insert_one({
        "station": station_id,
        "pm_date": pm_date,
        "issue_id": final_issue_id, 
        "inspector": inspector_clean,
        "doc_name": doc_name,
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    })
    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls,"issue_id": final_issue_id,"doc_name": doc_name,"inspector": inspector_clean}

@app.get("/stationpmurl/list")
async def stationpmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    # current: UserClaims = Depends(get_current_user),
):
    coll = get_stationpmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find(
        {},
        {"_id": 1, "issue_id": 1,"doc_name": 1,"inspector":1, "pm_date": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    items = []
    for it in items_raw:
        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""
        items.append({
            "id": str(it["_id"]),
            "pm_date": it.get("pm_date"),
            "issue_id": it.get("issue_id"),
            "inspector": it.get(("inspector")), 
            "doc_name": it.get("doc_name"),
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,
            "urls": urls,
        })

    return {"items": items, "pm_date": [i["pm_date"] for i in items if i.get("pm_date")], "page": page, "pageSize": pageSize, "total": total}



#----------------------------------------------------------------------
# CM Report
# ---------------------------------------------------------------------
def get_cmreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    coll = CMReportDB.get_collection(str(station_id))
    return coll

def get_cmurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    coll = CMUrlDB.get_collection(str(station_id))
    return coll

@app.get("/cmreport/list")
async def cmreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_cmreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "cm_date": 1, "status": 1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)
    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # --- ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å PMReportURL ‡πÇ‡∏î‡∏¢ map ‡∏î‡πâ‡∏ß‡∏¢ pm_date (string) ---
    cm_dates = [it.get("cm_date") for it in items_raw if it.get("cm_date")]
    urls_coll = get_cmurl_coll_upload(station_id)
    url_by_day: dict[str, str] = {}

    if cm_dates:
        ucur = urls_coll.find({"cm_date": {"$in": cm_dates}}, {"cm_date": 1, "status": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("cm_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "cm_date": it.get("cm_date"),
        "status": it.get("status"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("cm_date") or "", ""),
    } for it in items_raw]

    cm_date_arr = [it.get("cm_date") for it in items_raw if it.get("cm_date")]
    status_arr = [it.get("status") for it in items_raw if it.get("status")]
    return {"items": items, "cm_date": cm_date_arr, "status": status_arr, "page": page, "pageSize": pageSize, "total": total}

# ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå
UPLOADS_ROOT = os.getenv("UPLOADS_ROOT", "./uploads")
os.makedirs(UPLOADS_ROOT, exist_ok=True)

# ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏∑‡∏ô‡πÉ‡∏´‡πâ Frontend ‡∏ú‡πà‡∏≤‡∏ô /uploads/...
app.mount("/uploads", StaticFiles(directory=UPLOADS_ROOT, html=False), name="uploads")

# ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif"}
# MAX_FILE_MB = 10

def _safe_name(name: str) -> str:
    # ‡∏Å‡∏±‡∏ô path traversal ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏õ‡∏•‡∏Å ‡πÜ
    base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
    return base[:120] or secrets.token_hex(4)

def _ext(fname: str) -> str:
    return (fname.rsplit(".",1)[-1].lower() if "." in fname else "")

@app.post("/cmreport/{report_id}/photos")
async def cmreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    group: str = Form(...),                   # ‡πÄ‡∏ä‡πà‡∏ô "g1" .. "g10"
    files: list[UploadFile] = File(...),
    remark: str | None = Form(None),
    current: UserClaims = Depends(get_current_user),
):
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")
    if not re.fullmatch(r"g\d+", group):
        raise HTTPException(status_code=400, detail="Bad group key")

    coll = get_cmreport_collection_for(station_id)
    from bson import ObjectId
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô station ‡∏ô‡∏µ‡πâ
    doc = await coll.find_one({"_id": oid}, {"_id":1, "station_id":1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cm" / station_id / report_id / group
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    total = 0
    for f in files:
        ext = _ext(f.filename or "")
        if ext not in ALLOWED_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        total += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        # URL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ö‡∏ô Frontend
        url_path = f"/uploads/cm/{station_id}/{report_id}/{group}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc)
        })

    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PMReport: push ‡∏•‡∏á photos.<group>
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos.{group}": {"$each": saved}}}
    )

    return {"ok": True, "count": len(saved), "group": group, "files": saved}

@app.post("/cmreport/{report_id}/finalize")
async def cmreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    current: UserClaims = Depends(get_current_user),
):
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_cmreport_collection_for(station_id)
    from bson import ObjectId
    oid = ObjectId(report_id)
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/cmurl/upload-files", status_code=201)
async def cmurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),                 # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO
    files: list[UploadFile] = File(...),
    status: str = Form(...),  
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # ‡∏ï‡∏£‡∏ß‡∏à/‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô
    coll = get_cmurl_coll_upload(station_id)

    # parse ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô UTC datetime (‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
    cm_date = normalize_pm_date(reportDate)

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á: /uploads/pmurl/<station_id>/<YYYY-MM-DD>/
    # subdir = report_dt_utc.astimezone(th_tz).date().isoformat()
    subdir = cm_date
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "cmurl" / station_id / subdir
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls = []
    metas = []
    total_size = 0

    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if "." in f.filename else "")
        if ext not in ALLOWED_EXTS or ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        total_size += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        safe = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        dest = dest_dir / safe
        with open(dest, "wb") as out:
            out.write(data)

        url = f"/uploads/cmurl/{station_id}/{subdir}/{safe}"   # ‚Üê ‡∏à‡∏∞‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å StaticFiles ‡∏ó‡∏µ‡πà mount ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    now = datetime.now(timezone.utc)

    doc = {
        "station": station_id,
        "cm_date": cm_date,
        "status": (status or "").strip(), 
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    }
    res = await coll.insert_one(doc)

    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls}

@app.get("/cmurl/list")
async def cmurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
    status: str | None = Query(None),
):
    coll = get_cmurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    # --- ‡∏™‡∏£‡πâ‡∏≤‡∏á filter ‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ (optional ‡πÅ‡∏ï‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥) ---
    mongo_filter: dict = {}
    if status:
        want = (status or "").strip()
        mongo_filter["$or"] = [
            {"status": {"$regex": f"^{re.escape(want)}$", "$options": "i"}},
            {"job.status": {"$regex": f"^{re.escape(want)}$", "$options": "i"}},
        ]

    # --- ‡∏Ç‡∏≠‡∏ü‡∏¥‡∏•‡∏î‡πå status ‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢ ---
    projection = {
        "_id": 1, "cm_date": 1, "reportDate": 1,
        "urls": 1, "createdAt": 1,
        "status": 1, "job": 1,   # üëà ‡πÄ‡∏û‡∏¥‡πà‡∏°
    }

    cursor = (
        coll.find(mongo_filter, projection)
            .sort([("createdAt", -1), ("_id", -1)])
            .skip(skip)
            .limit(pageSize)
    )

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents(mongo_filter)

    def _cm_date_from(doc: dict) -> str | None:
        s = doc.get("cm_date")
        if isinstance(s, str) and re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
            return s
        rd = doc.get("reportDate")
        if isinstance(rd, datetime):
            return rd.astimezone(th_tz).date().isoformat()
        if isinstance(rd, str):
            try:
                dt = datetime.fromisoformat(rd.replace("Z", "+00:00"))
            except Exception:
                try:
                    dt = datetime.fromisoformat(rd).replace(tzinfo=th_tz)
                except Exception:
                    return None
            return dt.astimezone(th_tz).date().isoformat()
        return None

    items = []
    cm_date_arr = []

    for it in items_raw:
        cm_date_str = _cm_date_from(it)
        if cm_date_str:
            cm_date_arr.append(cm_date_str)

        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""

        items.append({
            "id": str(it["_id"]),
            "cm_date": cm_date_str,
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "status": (it.get("status") or (it.get("job") or {}).get("status") or ""),  # üëà ‡∏î‡∏∂‡∏á‡∏ï‡∏£‡∏á‡πÜ
            "file_url": first_url,
            "urls": urls,
        })

    return {
        "items": items,
        "cm_date": [d for d in cm_date_arr if d],
        # ‡∏à‡∏∞ echo ‡∏Ñ‡πà‡∏≤ query ‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡πá‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô:
        # "status": (status or "").strip(),
        "page": page,
        "pageSize": pageSize,
        "total": total,
    }


class CMSubmitIn(BaseModel):
    station_id: str
    job: Dict[str, Any]          # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡∏ü‡∏≠‡∏£‡πå‡∏° (issue_id, found_date, ... )
    summary: str = ""            # ‡∏™‡∏£‡∏∏‡∏õ/‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡πÅ‡∏ö‡∏ö‡∏¢‡∏≤‡∏ß (‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ï‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ)
    cm_date: Optional[str] = None  # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO; ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏∞ fallback ‡πÄ‡∏õ‡πá‡∏ô job.found_date

async def _ensure_cm_indexes(coll):
    try:
        await coll.create_index([("createdAt", -1), ("_id", -1)])
        # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏•‡∏Ç‡πÉ‡∏ö‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ: ‡πÄ‡∏õ‡∏¥‡∏î unique issue_id ‡∏Å‡πá‡πÑ‡∏î‡πâ (‡∏ñ‡πâ‡∏≤‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ unique)
        # await coll.create_index("issue_id", unique=True, sparse=True)
    except Exception:
        pass

@app.post("/cmreport/submit")
async def cmreport_submit(body: CMSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    # Auth: admin ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏°‡∏î, ‡∏Ñ‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÉ‡∏ô station ‡∏ô‡∏µ‡πâ
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_cmreport_collection_for(station_id)
    await _ensure_cm_indexes(coll)

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î cm_date (string 'YYYY-MM-DD') ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á /cmreport/list
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‚Üí ‡πÉ‡∏ä‡πâ job.found_date ‚Üí ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏µ‡∏Å ‚Üí ‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ (‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢)
    cm_date_src = body.cm_date or body.job.get("found_date")
    if cm_date_src:
        cm_date = normalize_pm_date(cm_date_src)   # ‡∏Ñ‡∏∑‡∏ô "YYYY-MM-DD"
    else:
        cm_date = datetime.now(th_tz).date().isoformat()

    doc = {
        "station_id": station_id,
        "cm_date": cm_date,
        "job": body.job,              # ‡πÄ‡∏Å‡πá‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏Å‡πâ‡∏≠‡∏ô (issue_id, severity, etc.)
        "summary": body.summary,
        "issue_id": body.job.get("issue_id"),
        "status": body.job.get("status", "Open"),      # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏≤‡∏Å query
        "createdAt": datetime.now(timezone.utc),
        "updatedAt": datetime.now(timezone.utc),
        "photos": {},                 # ‡∏£‡∏π‡∏õ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏µ‡πà /cmreport/{report_id}/photos
    }

    res = await coll.insert_one(doc)
    return {"ok": True, "report_id": str(res.inserted_id)}


@app.get("/cmreport/{report_id}")
async def cmreport_detail_path(
    report_id: str,
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_cmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid, "station_id": station_id})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")

    return {
        "id": str(doc["_id"]),
        "station_id": doc.get("station_id"),
        "cm_date": doc.get("cm_date"),
        "issue_id": doc.get("issue_id"),
        "status": doc.get("status"),
        "summary": doc.get("summary", ""),
        "job": doc.get("job", {}),
        "photos": doc.get("photos", {}),
        "createdAt": _ensure_utc_iso(doc.get("createdAt")),
        "updatedAt": _ensure_utc_iso(doc.get("updatedAt")),
    }

@app.get("/cmreport/detail")
async def cmreport_detail_query(
    id: str = Query(..., alias="id"),
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    return await cmreport_detail_path(id, station_id, current)  # reuse logic

class CMStatusUpdateIn(BaseModel):
    station_id: str
    status: Literal["Open", "In Progress", "Closed"]
    job: Optional[Dict[str, Any]] = None
    summary: Optional[str] = None
    cm_date: Optional[str] = None  # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO

ALLOWED_STATUS: set[str] = {"Open", "In Progress", "Closed"}

@app.patch("/cmreport/{report_id}/status")
async def cmreport_update_status(
    report_id: str,
    body: CMStatusUpdateIn,
    current: UserClaims = Depends(get_current_user),
):
    station_id = body.station_id.strip()
    if body.status not in ALLOWED_STATUS:
        raise HTTPException(status_code=400, detail="Invalid status")

    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_cmreport_collection_for(station_id)
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    updates: Dict[str, Any] = {
        "status": body.status,          # top-level
        "job.status": body.status,      # sync ‡πÉ‡∏ô job
    }

    if body.summary is not None:
        updates["summary"] = body.summary

    if body.cm_date is not None:
        updates["cm_date"] = normalize_pm_date(body.cm_date)

    if body.job is not None:
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏à‡∏≤‡∏Å‡∏ü‡∏≠‡∏£‡πå‡∏°
        allowed_job_keys = {
            "issue_id","found_date","location","wo","sn",
            "equipment_list","problem_details","problem_type","severity",
            "reported_by","assignee","initial_cause","corrective_actions",
            "resolved_date","repair_result","preventive_action","remarks"
        }
        # ‡∏ñ‡πâ‡∏≤ job.status ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏°‡∏≤ ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡πÅ‡∏•‡∏∞ sync
        if "status" in body.job:
            js = body.job["status"]
            if js not in ALLOWED_STATUS:
                raise HTTPException(status_code=400, detail="Invalid job.status")
            updates["status"] = js
            updates["job.status"] = js

        for k, v in body.job.items():
            if k in allowed_job_keys:
                updates[f"job.{k}"] = v

        # optional: sync cm_date ‡∏à‡∏≤‡∏Å found_date
        if "found_date" in body.job and body.job.get("found_date"):
            try:
                updates.setdefault("cm_date", normalize_pm_date(body.job["found_date"]))
            except Exception:
                pass

    updates["updatedAt"] = datetime.now(timezone.utc)

    res = await coll.update_one({"_id": oid, "station_id": station_id}, {"$set": updates})
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")

    return {"ok": True, "status": updates["status"]}

#---------------------------------------------------------------------- Test Report (DC)
def get_dc_testreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    coll = DCTestReportDB.get_collection(str(station_id))
    return coll

def get_dcurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    coll = DCUrlDB.get_collection(str(station_id))
    return coll

@app.get("/dctestreport/list")
async def dctestreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_dc_testreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "inspection_date": 1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)
    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # --- ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å PMReportURL ‡πÇ‡∏î‡∏¢ map ‡∏î‡πâ‡∏ß‡∏¢ pm_date (string) ---
    dc_dates = [it.get("inspection_date") for it in items_raw if it.get("inspection_date")]
    urls_coll = get_dcurl_coll_upload(station_id)
    url_by_day: dict[str, str] = {}

    if dc_dates:
        ucur = urls_coll.find({"inspection_date": {"$in": dc_dates}}, {"inspection_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("inspection_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "inspection_date": it.get("inspection_date"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("inspection_date") or "", ""),
    } for it in items_raw]

    dc_date_arr = [it.get("inspection_date") for it in items_raw if it.get("inspection_date")]
    # status_arr = [it.get("status") for it in items_raw if it.get("status")]
    return {"items": items, "inspection_date": dc_date_arr, "page": page, "pageSize": pageSize, "total": total}

# ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå
UPLOADS_ROOT = os.getenv("UPLOADS_ROOT", "./uploads")
os.makedirs(UPLOADS_ROOT, exist_ok=True)

# ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏∑‡∏ô‡πÉ‡∏´‡πâ Frontend ‡∏ú‡πà‡∏≤‡∏ô /uploads/...
app.mount("/uploads", StaticFiles(directory=UPLOADS_ROOT, html=False), name="uploads")

# ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif"}
# MAX_FILE_MB = 10

def _safe_name(name: str) -> str:
    # ‡∏Å‡∏±‡∏ô path traversal ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏õ‡∏•‡∏Å ‡πÜ
    base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
    return base[:120] or secrets.token_hex(4)

def _ext(fname: str) -> str:
    return (fname.rsplit(".",1)[-1].lower() if "." in fname else "")

# ---- config ‡πÑ‡∏ü‡∏•‡πå/‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏ß‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô) ----
ALLOWED_IMAGE_EXTS = {"jpg", "jpeg", "png", "webp", "gif"}
MAX_IMAGE_FILE_MB = 10

PHOTO_GROUP_KEYS = [
    "nameplate",       # index 0
    "charger",         # index 1
    "circuit_breaker", # index 2
    "rcd",             # index 3
    "gun1",            # index 4
    "gun2",            # index 5
]

def _key_for_index(i: int) -> str:
    return PHOTO_GROUP_KEYS[i] if 0 <= i < len(PHOTO_GROUP_KEYS) else f"extra{i-5}"

@app.post("/dctestreport/{report_id}/photos")
async def dc_testreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    item_index: int = Form(...),               # <<-- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å group ‚Üí index
    files: list[UploadFile] = File(...),
    remark: str | None = Form(None),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ô‡∏µ‡πâ
    coll = get_dc_testreport_collection_for(station_id)
    from bson import ObjectId
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # ‡πÅ‡∏õ‡∏•‡∏á index ‚Üí ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏µ‡∏¢‡πå‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå/‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    key = _key_for_index(item_index)  # e.g. nameplate/charger/.../extra1

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "dctest" / station_id / report_id / key
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".", 1)[-1].lower() if "." in (f.filename or "") else "")
        if ext not in ALLOWED_IMAGE_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_IMAGE_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_IMAGE_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/dctest/{station_id}/{report_id}/{key}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc),
            "index": item_index,          # ‡πÄ‡∏Å‡πá‡∏ö index ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Å‡∏•‡∏±‡∏ö
        })

    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: push ‡∏•‡∏á photos.<key>
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos.{key}": {"$each": saved}}, "$set": {"updatedAt": datetime.now(timezone.utc)}}
    )

    return {"ok": True, "count": len(saved), "key": key, "files": saved}

@app.post("/dctestreport/{report_id}/finalize")
async def dc_testreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    current: UserClaims = Depends(get_current_user),
):
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_dc_testreport_collection_for(station_id)
    from bson import ObjectId
    oid = ObjectId(report_id)
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/dcurl/upload-files", status_code=201)
async def dcurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),                 # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO
    files: list[UploadFile] = File(...),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # ‡∏ï‡∏£‡∏ß‡∏à/‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô
    coll = get_dcurl_coll_upload(station_id)

    # parse ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô UTC datetime (‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
    dc_date = normalize_pm_date(reportDate)

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á: /uploads/pmurl/<station_id>/<YYYY-MM-DD>/
    # subdir = report_dt_utc.astimezone(th_tz).date().isoformat()
    subdir = dc_date
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "dcurl" / station_id / subdir
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls = []
    metas = []
    total_size = 0

    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if "." in f.filename else "")
        if ext not in ALLOWED_EXTS or ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        total_size += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        safe = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        dest = dest_dir / safe
        with open(dest, "wb") as out:
            out.write(data)

        url = f"/uploads/dcurl/{station_id}/{subdir}/{safe}"   # ‚Üê ‡∏à‡∏∞‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å StaticFiles ‡∏ó‡∏µ‡πà mount ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    now = datetime.now(timezone.utc)
    doc = {
        "station": station_id,
        "dc_date": dc_date,   
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    }
    res = await coll.insert_one(doc)

    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls}

@app.get("/dcurl/list")
async def dcurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå PM (PDF) ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡∏ï‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ ‡∏à‡∏≤‡∏Å PMUrlDB/<station_id>
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö pm_date (string 'YYYY-MM-DD') ‡πÅ‡∏•‡∏∞ reportDate (Date/ISO)
    - ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡πÄ‡∏Å‡πà‡∏≤ (createdAt desc, _id desc)
    - ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô /pmreport/list (‡∏°‡∏µ file_url ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
    """
    coll = get_dcurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    cursor = coll.find(
        {},
        {"_id": 1, "dc_date": 1, "reportDate": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    def _dc_date_from(doc: dict) -> str | None:
        """
        ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ string 'YYYY-MM-DD'
        - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ pm_date (string) ‚Üí ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ reportDate (datetime/string) ‚Üí ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß .date().isoformat()
        """
        # ‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô pm_date (string)
        s = doc.get("dc_date")
        if isinstance(s, str) and re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
            return s

        # ‡∏£‡∏∏‡πà‡∏ô‡πÄ‡∏Å‡πà‡∏≤: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô reportDate (Date/ISO)
        rd = doc.get("reportDate")
        if isinstance(rd, datetime):
            return rd.astimezone(th_tz).date().isoformat()
        if isinstance(rd, str):
            try:
                dt = datetime.fromisoformat(rd.replace("Z", "+00:00"))
            except Exception:
                # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢
                try:
                    dt = datetime.fromisoformat(rd).replace(tzinfo=th_tz)
                except Exception:
                    return None
            return dt.astimezone(th_tz).date().isoformat()

        return None

    items = []
    dc_date_arr = []

    for it in items_raw:
        dc_date_str = _dc_date_from(it)
        if dc_date_str:
            dc_date_arr.append(dc_date_str)

        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""

        items.append({
            "id": str(it["_id"]),
            "dc_date": dc_date_str,                         # 'YYYY-MM-DD' | None
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,                          # ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å (‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î)
            "urls": urls,                                   # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå‡∏≠‡∏¢‡∏≤‡∏Å‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        })

    return {
        "items": items,
        "dc_date": [d for d in dc_date_arr if d],          # ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô /pmreport/list
        "page": page,
        "pageSize": pageSize,
        "total": total,
    }

class EquipmentBlock(BaseModel):
    manufacturers: List[str] = []
    models: List[str] = []
    serialNumbers: List[str] = []

SymbolLiteral = Literal["", "pass", "notPass", "notTest"]
PhaseLiteral  = Literal["", "L1L2L3", "L3L2L1"]

class PersonSig(BaseModel):
    name: str = ""
    signature: str = ""   # ‡πÄ‡∏Å‡πá‡∏ö path/‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏≤‡∏¢‡πÄ‡∏ã‡πá‡∏ô (‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)
    date: str = ""        # "YYYY-MM-DD"
    company: str = ""

class ResponsibilityBlock(BaseModel):
    performed: PersonSig = PersonSig()
    approved:  PersonSig = PersonSig()
    witnessed: PersonSig = PersonSig()

class SignatureBlock(BaseModel):
    responsibility: ResponsibilityBlock = ResponsibilityBlock()

class DCSubmitIn(BaseModel):
    station_id: str
    issue_id: Optional[str] = None 
    job: Dict[str, Any]          # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡∏ü‡∏≠‡∏£‡πå‡∏° (issue_id, found_date, ... )
    head: Dict[str,Any]
    inspection_date: Optional[str] = None  # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO; ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏∞ fallback ‡πÄ‡∏õ‡πá‡∏ô job.found_date
    equipment: Optional[EquipmentBlock] = None
    electrical_safety: Dict[str, Any] = Field(default_factory=dict)
    charger_safety: Dict[str, Any] = Field(default_factory=dict)
    remarks: Dict[str, Any] = Field(default_factory=dict)
    symbol: Optional[SymbolLiteral] = None
    phaseSequence: Optional[PhaseLiteral] = None
    signature: Optional[SignatureBlock] = None 

async def _ensure_dc_indexes(coll):
    try:
        await coll.create_index([("createdAt", -1), ("_id", -1)])
        # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏•‡∏Ç‡πÉ‡∏ö‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ: ‡πÄ‡∏õ‡∏¥‡∏î unique issue_id ‡∏Å‡πá‡πÑ‡∏î‡πâ (‡∏ñ‡πâ‡∏≤‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ unique)
        # await coll.create_index("issue_id", unique=True, sparse=True)
    except Exception:
        pass

def _normalize_tick_to_pass(obj):
    if isinstance(obj, dict):
        return {k: _normalize_tick_to_pass(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_normalize_tick_to_pass(v) for v in obj]
    if isinstance(obj, str):
        return "pass" if obj == "‚úì" else obj
    return obj

@app.post("/dcreport/submit")
async def dcreport_submit(body: DCSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    # Auth: admin ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏°‡∏î, ‡∏Ñ‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÉ‡∏ô station ‡∏ô‡∏µ‡πâ
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_dc_testreport_collection_for(station_id)
    await _ensure_dc_indexes(coll)

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î cm_date (string 'YYYY-MM-DD') ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á /cmreport/list
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‚Üí ‡πÉ‡∏ä‡πâ job.found_date ‚Üí ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏µ‡∏Å ‚Üí ‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ (‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢)
    dc_date_src = body.inspection_date or body.head.get("inspection_date")
    if dc_date_src:
        dc_date = normalize_pm_date(dc_date_src)   # ‡∏Ñ‡∏∑‡∏ô "YYYY-MM-DD"
    else:
        dc_date = datetime.now(th_tz).date().isoformat()

    issue_id = (body.head or {}).get("issue_id")  or (body.job or {}).get("issue_id") 

    electrical_safety = _normalize_tick_to_pass(body.electrical_safety or {})

    charger_safety = _normalize_tick_to_pass(body.charger_safety or {})
    doc = {
        "station_id": station_id,
        "issue_id": issue_id,
        "inspection_date": dc_date,
        "head": body.head,
        "equipment": body.equipment.dict() if body.equipment else {"manufacturers": [], "models": [], "serialNumbers": []},
        "electrical_safety": electrical_safety,
        "charger_safety": charger_safety,
        "remarks": body.remarks or {},
        "symbol": body.symbol,
        "phaseSequence": body.phaseSequence,
        "signature": body.signature.dict() if body.signature else None,
        "createdAt": datetime.now(timezone.utc),
        "updatedAt": datetime.now(timezone.utc),
        "photos": {},                 # ‡∏£‡∏π‡∏õ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏µ‡πà /cmreport/{report_id}/photos
    }

    res = await coll.insert_one(doc)
    return {"ok": True, "report_id": str(res.inserted_id)}

#---------------------------------------------------------------------- Test Report (AC)
def get_ac_testreport_collection_for(station_id: str):
    _validate_station_id(station_id)
    coll = ACTestReportDB.get_collection(str(station_id))
    return coll

def get_acurl_coll_upload(station_id: str):
    _validate_station_id(station_id)
    coll = ACUrlDB.get_collection(str(station_id))
    return coll

@app.get("/actestreport/list")
async def actestreport_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    coll = get_ac_testreport_collection_for(station_id)
    skip = (page - 1) * pageSize

    cursor = coll.find({}, {"_id": 1, "inspection_date": 1, "createdAt": 1}).sort(
        [("createdAt", -1), ("_id", -1)]
    ).skip(skip).limit(pageSize)
    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    # --- ‡∏î‡∏∂‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏≤‡∏Å PMReportURL ‡πÇ‡∏î‡∏¢ map ‡∏î‡πâ‡∏ß‡∏¢ pm_date (string) ---
    ac_dates = [it.get("inspection_date") for it in items_raw if it.get("inspection_date")]
    urls_coll = get_acurl_coll_upload(station_id)
    url_by_day: dict[str, str] = {}

    if ac_dates:
        ucur = urls_coll.find({"inspection_date": {"$in": ac_dates}}, {"inspection_date": 1, "urls": 1})
        url_docs = await ucur.to_list(length=10_000)
        for u in url_docs:
            day = u.get("inspection_date")
            first_url = (u.get("urls") or [None])[0]
            if day and first_url and day not in url_by_day:
                url_by_day[day] = first_url

    items = [{
        "id": str(it["_id"]),
        "inspection_date": it.get("inspection_date"),
        "createdAt": _ensure_utc_iso(it.get("createdAt")),
        "file_url": url_by_day.get(it.get("inspection_date") or "", ""),
    } for it in items_raw]

    ac_date_arr = [it.get("inspection_date") for it in items_raw if it.get("inspection_date")]
    # status_arr = [it.get("status") for it in items_raw if it.get("status")]
    return {"items": items, "inspection_date": ac_date_arr, "page": page, "pageSize": pageSize, "total": total}

# ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ö‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå
UPLOADS_ROOT = os.getenv("UPLOADS_ROOT", "./uploads")
os.makedirs(UPLOADS_ROOT, exist_ok=True)

# ‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏∑‡∏ô‡πÉ‡∏´‡πâ Frontend ‡∏ú‡πà‡∏≤‡∏ô /uploads/...
app.mount("/uploads", StaticFiles(directory=UPLOADS_ROOT, html=False), name="uploads")

# ALLOWED_EXTS = {"jpg","jpeg","png","webp","gif"}
# MAX_FILE_MB = 10

def _safe_name(name: str) -> str:
    # ‡∏Å‡∏±‡∏ô path traversal ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡πÅ‡∏õ‡∏•‡∏Å ‡πÜ
    base = re.sub(r"[^A-Za-z0-9._-]+", "_", name)
    return base[:120] or secrets.token_hex(4)

def _ext(fname: str) -> str:
    return (fname.rsplit(".",1)[-1].lower() if "." in fname else "")

# ---- config ‡πÑ‡∏ü‡∏•‡πå/‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡∏ß‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô) ----
ALLOWED_IMAGE_EXTS = {"jpg", "jpeg", "png", "webp", "gif"}
MAX_IMAGE_FILE_MB = 10

PHOTO_GROUP_KEYS = [
    "nameplate",       # index 0
    "charger",         # index 1
    "circuit_breaker", # index 2
    "rcd",             # index 3
    "gun1",            # index 4
    "gun2",            # index 5
]

def _key_for_index(i: int) -> str:
    return PHOTO_GROUP_KEYS[i] if 0 <= i < len(PHOTO_GROUP_KEYS) else f"extra{i-5}"

@app.post("/actestreport/{report_id}/photos")
async def ac_testreport_upload_photos(
    report_id: str,
    station_id: str = Form(...),
    item_index: int = Form(...),               # <<-- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å group ‚Üí index
    files: list[UploadFile] = File(...),
    remark: str | None = Form(None),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ô‡∏µ‡πâ
    coll = get_ac_testreport_collection_for(station_id)
    from bson import ObjectId
    try:
        oid = ObjectId(report_id)
    except Exception:
        raise HTTPException(status_code=400, detail="Bad report_id")

    doc = await coll.find_one({"_id": oid}, {"_id": 1, "station_id": 1})
    if not doc:
        raise HTTPException(status_code=404, detail="Report not found")
    if doc.get("station_id") != station_id:
        raise HTTPException(status_code=400, detail="station_id mismatch")

    # ‡πÅ‡∏õ‡∏•‡∏á index ‚Üí ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏µ‡∏¢‡πå‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå/‡∏Ñ‡∏µ‡∏¢‡πå‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
    key = _key_for_index(item_index)  # e.g. nameplate/charger/.../extra1

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "actest" / station_id / report_id / key
    dest_dir.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        ext = (f.filename.rsplit(".", 1)[-1].lower() if "." in (f.filename or "") else "")
        if ext not in ALLOWED_IMAGE_EXTS:
            raise HTTPException(status_code=400, detail=f"File type not allowed: {ext}")

        data = await f.read()
        if len(data) > MAX_IMAGE_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_IMAGE_FILE_MB} MB)")

        fname = _safe_name(f.filename or f"image_{secrets.token_hex(3)}.{ext}")
        path = dest_dir / fname
        with open(path, "wb") as out:
            out.write(data)

        url_path = f"/uploads/actest/{station_id}/{report_id}/{key}/{fname}"
        saved.append({
            "filename": fname,
            "size": len(data),
            "url": url_path,
            "remark": remark or "",
            "uploadedAt": datetime.now(timezone.utc),
            "index": item_index,          # ‡πÄ‡∏Å‡πá‡∏ö index ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏Å‡∏•‡∏±‡∏ö
        })

    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: push ‡∏•‡∏á photos.<key>
    await coll.update_one(
        {"_id": oid},
        {"$push": {f"photos.{key}": {"$each": saved}}, "$set": {"updatedAt": datetime.now(timezone.utc)}}
    )

    return {"ok": True, "count": len(saved), "key": key, "files": saved}


@app.post("/actestreport/{report_id}/finalize")
async def ac_testreport_finalize(
    report_id: str,
    station_id: str = Form(...),
    current: UserClaims = Depends(get_current_user),
):
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_ac_testreport_collection_for(station_id)
    from bson import ObjectId
    oid = ObjectId(report_id)
    res = await coll.update_one(
        {"_id": oid},
        {"$set": {"status": "submitted", "submittedAt": datetime.now(timezone.utc)}}
    )
    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Report not found")
    return {"ok": True}

@app.post("/acurl/upload-files", status_code=201)
async def acurl_upload_files(
    station_id: str = Form(...),
    reportDate: str = Form(...),                 # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO
    files: list[UploadFile] = File(...),
    current: UserClaims = Depends(get_current_user),
):
    # auth
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    # ‡∏ï‡∏£‡∏ß‡∏à/‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô
    coll = get_acurl_coll_upload(station_id)

    # parse ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô UTC datetime (‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
    ac_date = normalize_pm_date(reportDate)

    # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á: /uploads/pmurl/<station_id>/<YYYY-MM-DD>/
    # subdir = report_dt_utc.astimezone(th_tz).date().isoformat()
    subdir = ac_date
    dest_dir = pathlib.Path(UPLOADS_ROOT) / "acurl" / station_id / subdir
    dest_dir.mkdir(parents=True, exist_ok=True)

    urls = []
    metas = []
    total_size = 0

    for f in files:
        ext = (f.filename.rsplit(".",1)[-1].lower() if "." in f.filename else "")
        if ext not in ALLOWED_EXTS or ext != "pdf":
            raise HTTPException(status_code=400, detail=f"Only PDF allowed, got: {ext}")

        data = await f.read()
        total_size += len(data)
        if len(data) > MAX_FILE_MB * 1024 * 1024:
            raise HTTPException(status_code=413, detail=f"File too large (> {MAX_FILE_MB} MB)")

        safe = _safe_name(f.filename or f"file_{secrets.token_hex(3)}.pdf")
        dest = dest_dir / safe
        with open(dest, "wb") as out:
            out.write(data)

        url = f"/uploads/acurl/{station_id}/{subdir}/{safe}"   # ‚Üê ‡∏à‡∏∞‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å StaticFiles ‡∏ó‡∏µ‡πà mount ‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß
        urls.append(url)
        metas.append({"name": f.filename, "size": len(data)})

    now = datetime.now(timezone.utc)
    doc = {
        "station": station_id,
        "ac_date": ac_date,   
        "urls": urls,
        "meta": {"files": metas},
        "source": "upload-files",
        "createdAt": now,
        "updatedAt": now,
    }
    res = await coll.insert_one(doc)

    return {"ok": True, "inserted_id": str(res.inserted_id), "count": len(urls), "urls": urls}

@app.get("/acurl/list")
async def acurl_list(
    station_id: str = Query(...),
    page: int = Query(1, ge=1),
    pageSize: int = Query(20, ge=1, le=100),
):
    """
    ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå PM (PDF) ‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡∏ï‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ ‡∏à‡∏≤‡∏Å PMUrlDB/<station_id>
    - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö pm_date (string 'YYYY-MM-DD') ‡πÅ‡∏•‡∏∞ reportDate (Date/ISO)
    - ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏õ‡πÄ‡∏Å‡πà‡∏≤ (createdAt desc, _id desc)
    - ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô /pmreport/list (‡∏°‡∏µ file_url ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
    """
    coll = get_acurl_coll_upload(station_id)
    skip = (page - 1) * pageSize

    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    cursor = coll.find(
        {},
        {"_id": 1, "ac_date": 1, "reportDate": 1, "urls": 1, "createdAt": 1}
    ).sort([("createdAt", -1), ("_id", -1)]).skip(skip).limit(pageSize)

    items_raw = await cursor.to_list(length=pageSize)
    total = await coll.count_documents({})

    def _ac_date_from(doc: dict) -> str | None:
        """
        ‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ string 'YYYY-MM-DD'
        - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ pm_date (string) ‚Üí ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
        - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ reportDate (datetime/string) ‚Üí ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡πâ‡∏ß .date().isoformat()
        """
        # ‡∏£‡∏∏‡πà‡∏ô‡πÉ‡∏´‡∏°‡πà: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô pm_date (string)
        s = doc.get("ac_date")
        if isinstance(s, str) and re.fullmatch(r"\d{4}-\d{2}-\d{2}$", s):
            return s

        # ‡∏£‡∏∏‡πà‡∏ô‡πÄ‡∏Å‡πà‡∏≤: ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô reportDate (Date/ISO)
        rd = doc.get("reportDate")
        if isinstance(rd, datetime):
            return rd.astimezone(th_tz).date().isoformat()
        if isinstance(rd, str):
            try:
                dt = datetime.fromisoformat(rd.replace("Z", "+00:00"))
            except Exception:
                # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ã‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ ‚Üí ‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢
                try:
                    dt = datetime.fromisoformat(rd).replace(tzinfo=th_tz)
                except Exception:
                    return None
            return dt.astimezone(th_tz).date().isoformat()

        return None

    items = []
    ac_date_arr = []

    for it in items_raw:
        ac_date_str = _ac_date_from(it)
        if ac_date_str:
            ac_date_arr.append(ac_date_str)

        urls = it.get("urls") or []
        first_url = urls[0] if urls else ""

        items.append({
            "id": str(it["_id"]),
            "ac_date": ac_date_str,                         # 'YYYY-MM-DD' | None
            "createdAt": _ensure_utc_iso(it.get("createdAt")),
            "file_url": first_url,                          # ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å (‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ‡∏õ‡∏∏‡πà‡∏°‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î)
            "urls": urls,                                   # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ü‡∏£‡∏≠‡∏ô‡∏ï‡πå‡∏≠‡∏¢‡∏≤‡∏Å‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        })

    return {
        "items": items,
        "ac_date": [d for d in ac_date_arr if d],          # ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô /pmreport/list
        "page": page,
        "pageSize": pageSize,
        "total": total,
    }

class ACSubmitIn(BaseModel):
    station_id: str
    issue_id: Optional[str] = None 
    # job: Dict[str, Any]          # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡∏ü‡∏≠‡∏£‡πå‡∏° (issue_id, found_date, ... )
    head: Dict[str,Any]
    inspection_date: Optional[str] = None  # "YYYY-MM-DD" ‡∏´‡∏£‡∏∑‡∏≠ ISO; ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏∞ fallback ‡πÄ‡∏õ‡πá‡∏ô job.found_date
    equipment: Optional[EquipmentBlock] = None
    electrical_safety: Dict[str, Any] = Field(default_factory=dict)
    charger_safety: Dict[str, Any] = Field(default_factory=dict)
    remarks: Dict[str, Any] = Field(default_factory=dict)
    symbol: Optional[SymbolLiteral] = None
    phaseSequence: Optional[PhaseLiteral] = None
    signature: Optional[SignatureBlock] = None 

async def _ensure_dc_indexes(coll):
    try:
        await coll.create_index([("createdAt", -1), ("_id", -1)])
        # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡πÄ‡∏•‡∏Ç‡πÉ‡∏ö‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ: ‡πÄ‡∏õ‡∏¥‡∏î unique issue_id ‡∏Å‡πá‡πÑ‡∏î‡πâ (‡∏ñ‡πâ‡∏≤‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ unique)
        # await coll.create_index("issue_id", unique=True, sparse=True)
    except Exception:
        pass

def _normalize_tick_to_pass(obj):
    if isinstance(obj, dict):
        return {k: _normalize_tick_to_pass(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_normalize_tick_to_pass(v) for v in obj]
    if isinstance(obj, str):
        return "pass" if obj == "‚úì" else obj
    return obj

@app.post("/acreport/submit")
async def acreport_submit(body: ACSubmitIn, current: UserClaims = Depends(get_current_user)):
    station_id = body.station_id.strip()
    # Auth: admin ‡∏ú‡πà‡∏≤‡∏ô‡∏´‡∏°‡∏î, ‡∏Ñ‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÉ‡∏ô station ‡∏ô‡∏µ‡πâ
    if current.role != "admin" and station_id not in set(current.station_ids):
        raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_ac_testreport_collection_for(station_id)
    await _ensure_dc_indexes(coll)

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î cm_date (string 'YYYY-MM-DD') ‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á /cmreport/list
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ ‚Üí ‡πÉ‡∏ä‡πâ job.found_date ‚Üí ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏µ‡∏Å ‚Üí ‡πÉ‡∏ä‡πâ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ (‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏ó‡∏¢)
    ac_date_src = body.inspection_date or body.head.get("inspection_date")
    if ac_date_src:
        ac_date = normalize_pm_date(ac_date_src)   # ‡∏Ñ‡∏∑‡∏ô "YYYY-MM-DD"
    else:
        ac_date = datetime.now(th_tz).date().isoformat()

    issue_id = (body.head or {}).get("issue_id")  or (body.head or {}).get("issue_id") 

    electrical_safety = _normalize_tick_to_pass(body.electrical_safety or {})

    charger_safety = _normalize_tick_to_pass(body.charger_safety or {})
    doc = {
        "station_id": station_id,
        "issue_id": issue_id,
        "inspection_date": ac_date,
        "head": body.head,
        "equipment": body.equipment.dict() if body.equipment else {"manufacturers": [], "models": [], "serialNumbers": []},
        "electrical_safety": electrical_safety,
        "charger_safety": charger_safety,
        "remarks": body.remarks or {},
        "symbol": body.symbol,
        "phaseSequence": body.phaseSequence,
        "signature": body.signature.dict() if body.signature else None,
        "createdAt": datetime.now(timezone.utc),
        "updatedAt": datetime.now(timezone.utc),
        "photos": {},                 # ‡∏£‡∏π‡∏õ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏¥‡∏°‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏µ‡πà /cmreport/{report_id}/photos
    }

    res = await coll.insert_one(doc)
    return {"ok": True, "report_id": str(res.inserted_id)}

# ----------------------------------------------------------------------- device page
def get_device_collection_for(station_id: str):
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return deviceDB.get_collection(str(station_id))

# (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡πÅ‡∏ö‡∏ö lazy ‡∏ï‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
async def _ensure_util_index(coll):
    try:
        await coll.create_index([("timestamp", -1), ("_id", -1)])
    except Exception:
        pass

@app.get("/utilization/stream")
async def utilization_stream(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_device_collection_for(station_id)
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    async def event_generator():
        # ‡∏™‡πà‡∏á snapshot ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
        latest = await coll.find_one({}, sort=[("timestamp", -1), ("_id", -1)])
        if latest:
            latest["_id"] = str(latest["_id"])
            latest["timestamp_utc"] = _ensure_utc_iso(latest.get("timestamp_utc"))
            yield f"event: init\ndata: {json.dumps(latest)}\n\n"

        # ‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ change stream (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô replica set / Atlas tier ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)
        try:
            async with coll.watch(full_document='updateLookup') as stream:
                async for change in stream:
                    if await request.is_disconnected():
                        break
                    doc = change.get("fullDocument")
                    if not doc:
                        continue
                    doc["_id"] = str(doc["_id"])
                    doc["timestamp_utc"] = _ensure_utc_iso(doc.get("timestamp_utc"))
                    yield f"data: {json.dumps(doc)}\n\n"
        except Exception:
            # fallback: ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô standalone) ‡πÉ‡∏´‡πâ polling
            last_id = latest.get("_id") if latest else None
            while not await request.is_disconnected():
                doc = await coll.find_one({}, sort=[("timestamp_utc", -1), ("_id", -1)])
                if doc and str(doc["_id"]) != str(last_id):
                    last_id = str(doc["_id"])
                    doc["_id"] = last_id
                    doc["timestamp_utc"] = _ensure_utc_iso(doc.get("timestamp_utc"))
                    yield f"data: {json.dumps(doc)}\n\n"
                else:
                    yield ": keep-alive\n\n"
                await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

#-------------------------------------------------------------------- setting page
def get_setting_collection_for(station_id: str):
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return settingDB.get_collection(str(station_id))

# (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡πÅ‡∏ö‡∏ö lazy ‡∏ï‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ
async def _ensure_util_index(coll):
    try:
        await coll.create_index([("timestamp", -1), ("_id", -1)])
    except Exception:
        pass

@app.get("/setting/stream")
async def setting_stream(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    # if current.role != "admin" and station_id not in set(current.station_ids):
    #     raise HTTPException(status_code=403, detail="Forbidden station_id")

    coll = get_setting_collection_for(station_id)
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }

    async def event_generator():
        # ‡∏™‡πà‡∏á snapshot ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô
        latest = await coll.find_one({}, sort=[("timestamp", -1), ("_id", -1)])
        if latest:
            latest["_id"] = str(latest["_id"])
            latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
            yield f"event: init\ndata: {json.dumps(latest)}\n\n"

        # ‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ change stream (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô replica set / Atlas tier ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö)
        try:
            async with coll.watch(full_document='updateLookup') as stream:
                async for change in stream:
                    if await request.is_disconnected():
                        break
                    doc = change.get("fullDocument")
                    if not doc:
                        continue
                    doc["_id"] = str(doc["_id"])
                    doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                    yield f"data: {json.dumps(doc)}\n\n"
        except Exception:
            # fallback: ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (‡πÄ‡∏ä‡πà‡∏ô standalone) ‡πÉ‡∏´‡πâ polling
            last_id = latest.get("_id") if latest else None
            while not await request.is_disconnected():
                doc = await coll.find_one({}, sort=[("timestamp", -1), ("_id", -1)])
                if doc and str(doc["_id"]) != str(last_id):
                    last_id = str(doc["_id"])
                    doc["_id"] = last_id
                    doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                    yield f"data: {json.dumps(doc)}\n\n"
                else:
                    yield ": keep-alive\n\n"
                await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

@app.get("/setting")
async def setting_query(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    """
    SSE ‡πÅ‡∏ö‡∏ö query param:
    - ‡∏™‡πà‡∏á snapshot ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (event: init)
    - ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô polling ‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á ‡πÜ
    """
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }
    coll = get_setting_collection_for(station_id)

    async def event_generator():
        last_id = None
        latest = await coll.find_one({}, sort=[("_id", -1)])  # ‚¨ÖÔ∏è ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á filter station_id ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÅ‡∏•‡πâ‡∏ß
        if latest:
            latest["timestamp"] = latest.get("timestamp")
            last_id = latest.get("_id")
            yield "retry: 3000\n"
            yield "event: init\n"
            yield f"data: {to_json(latest)}\n\n"
        else:
            yield "retry: 3000\n\n"

        while True:
            if await request.is_disconnected():
                break

            doc = await coll.find_one({}, sort=[("_id", -1)])
            if doc and doc.get("_id") != last_id:
                # doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                doc["timestamp"] = doc.get("timestamp")
                last_id = doc.get("_id")
                yield f"data: {to_json(doc)}\n\n"
            else:
                yield ": keep-alive\n\n"

            await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

from pdf import pdf_routes1
app.include_router(pdf_routes1.router)

class PLCMaxSetting(BaseModel):
    station_id: str = Field(..., min_length=1)
    dynamic_max_current1: Optional[float] = None   # A
    dynamic_max_power1: Optional[float] = None  

@app.post("/setting/PLC/MAX")
async def setting_plc(payload: PLCMaxSetting):
    now_iso = datetime.now().isoformat()

    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà client ‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏£‡∏¥‡∏á ‡πÜ (‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á default None)
    try:
        incoming = payload.model_dump(exclude_unset=True)   # Pydantic v2
    except Exception:
        incoming = payload.dict(exclude_unset=True)         # Pydantic v1

    station_id = incoming.get("station_id", payload.station_id)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á changes ‡∏à‡∏≤‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô request
    keys = ("dynamic_max_current1", "dynamic_max_power1")
    changes = {k: float(incoming[k]) for k in keys if k in incoming}

    # logging ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    print(f"[{now_iso}] ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Front:")
    print(f"  station_id = {station_id}")
    print("  dynamic_max_current1 =", changes.get("dynamic_max_current1", "(no change)"), "A")
    print("  dynamic_max_power1  =", changes.get("dynamic_max_power1",  "(no change)"), "kW")

    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏±‡∏Å‡∏ü‡∏¥‡∏•‡∏î‡πå ‚Üí ‡πÑ‡∏°‡πà publish (‡∏à‡∏∞‡∏ï‡∏≠‡∏ö 200 ‡∏´‡∏£‡∏∑‡∏≠ 400 ‡∏Å‡πá‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏ï‡πà‡∏î‡∏µ‡πÑ‡∏ã‡∏ô‡πå)
    if not changes:
        return {
            "ok": True,
            "message": "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á (‡πÑ‡∏°‡πà‡∏™‡πà‡∏á MQTT)",
            "timestamp": now_iso,
            "mqtt": {
                "broker": f"{BROKER_HOST}:{BROKER_PORT}",
                "topic": MQTT_TOPIC,
                "published": False,
            },
            "data": {"station_id": station_id, "timestamp": now_iso},
        }

    # ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö payload MQTT ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô
    msg = {
        "station_id": station_id,
        **changes,
        "timestamp": now_iso,
        # "source": "fastapi/setting_plc"
    }
    payload_str = json.dumps(msg, ensure_ascii=False)

    # ‡∏™‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô MQTT
    published = False
    try:
        pub_result = mqtt_client.publish(MQTT_TOPIC, payload_str, qos=1, retain=False)
        pub_result.wait_for_publish(timeout=2.0)
        published = pub_result.is_published()
        rc = pub_result.rc
        print(f"[MQTT] publish rc={rc}, published={published}, topic={MQTT_TOPIC}")
    except Exception as e:
        print(f"[MQTT] publish error: {e}")
        published = False

    # ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö frontend
    return {
        "ok": True,
        "message": "‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å frontend ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡πà‡∏á MQTT ‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)",
        "timestamp": now_iso,
        "mqtt": {
            "broker": f"{BROKER_HOST}:{BROKER_PORT}",
            "topic": MQTT_TOPIC,
            "published": bool(published),
        },
        "data": msg,
    }
class PLCCPCommand(BaseModel):
    station_id: str
    cp_status1: Literal["start", "stop"]

@app.post("/setting/PLC/CP")
async def setting_plc(payload: PLCCPCommand):
    now_iso = datetime.now().isoformat()

    # log ‡∏ù‡∏±‡πà‡∏á‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå
    print(f"[{now_iso}] ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Front:")
    print(f"  station_id = {payload.station_id}")
    print(f"  cp_status1 = {payload.cp_status1}")
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° message ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô MQTT (‡πÉ‡∏™‡πà timestamp ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡πâ)
    msg = {
        "station_id": payload.station_id,
        "cp_status1": payload.cp_status1,
        "timestamp": now_iso,
        # "source": "fastapi/setting_plc"
    }
    payload_str = json.dumps(msg, ensure_ascii=False)

    # ‡∏™‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô MQTT (QoS 1, ‡πÑ‡∏°‡πà retain)
    try:
        pub_result = mqtt_client.publish(MQTT_TOPIC, payload_str, qos=1, retain=False)
        # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡πÜ (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå)
        pub_result.wait_for_publish(timeout=2.0)
        published = pub_result.is_published()
        rc = pub_result.rc
        print(f"[MQTT] publish rc={rc}, published={published}, topic={MQTT_TOPIC}")
    except Exception as e:
        print(f"[MQTT] publish error: {e}")
        published = False

    # ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö frontend
    return {
        "ok": True,
        "message": "‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å frontend ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡πà‡∏á MQTT ‡πÅ‡∏•‡πâ‡∏ß",
        "timestamp": now_iso,
        "mqtt": {
            "broker": f"{BROKER_HOST}:{BROKER_PORT}",
            "topic": MQTT_TOPIC,
            "published": bool(published),
        },
        "data": msg,
    }

class PLCH2MaxSetting(BaseModel):
    station_id: str = Field(..., min_length=1)
    dynamic_max_current2: Optional[float] = None   # A  ‚Üê optional
    dynamic_max_power2: Optional[float] = None     # kW (‡∏à‡∏≤‡∏Å front)

@app.post("/setting/PLC/MAXH2")
async def setting_plc(payload: PLCH2MaxSetting):
    now_iso = datetime.now().isoformat()

    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ fields ‡∏ó‡∏µ‡πà client ‡∏™‡πà‡∏á‡∏°‡∏≤ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ default)
    try:
        incoming = payload.model_dump(exclude_unset=True)  # Pydantic v2
    except Exception:
        incoming = payload.dict(exclude_unset=True)        # Pydantic v1

    station_id = incoming.get("station_id", payload.station_id)

    # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏à‡∏≤‡∏Å‡∏™‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
    keys = ("dynamic_max_current2", "dynamic_max_power2")
    changes = {k: float(incoming[k]) for k in keys if k in incoming}

    if not changes:
        # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô error ‡∏Å‡πá return ok=False ‡πÑ‡∏î‡πâ‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô
        raise HTTPException(
            status_code=400,
            detail="At least one of dynamic_max_current2 or dynamic_max_power2 is required"
        )

    print(f"[{now_iso}] ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Front: station_id={station_id}")
    print("  dynamic_max_current2 =", changes.get("dynamic_max_current2", "(no change)"), "A")
    print("  dynamic_max_power2  =", changes.get("dynamic_max_power2", "(no change)"), "kW")

    msg = {"station_id": station_id, **changes, "timestamp": now_iso}
    payload_str = json.dumps(msg, ensure_ascii=False)

    published = False
    try:
        pub_result = mqtt_client.publish(MQTT_TOPIC, payload_str, qos=1, retain=False)
        pub_result.wait_for_publish(timeout=2.0)
        published = pub_result.is_published()
        rc = pub_result.rc
        print(f"[MQTT] publish rc={rc}, published={published}, topic={MQTT_TOPIC}")
    except Exception as e:
        print(f"[MQTT] publish error: {e}")
        published = False

    return {
        "ok": True,
        "message": "‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô",
        "timestamp": now_iso,
        "mqtt": {
            "broker": f"{BROKER_HOST}:{BROKER_PORT}",
            "topic": MQTT_TOPIC,
            "published": bool(published),
        },
        "data": msg,
    }

class PLCH2CPCommand(BaseModel):
    station_id: str
    cp_status2: Literal["start", "stop"]

@app.post("/setting/PLC/CPH2")
async def setting_plc(payload: PLCH2CPCommand):
    now_iso = datetime.now().isoformat()

    # log ‡∏ù‡∏±‡πà‡∏á‡πÄ‡∏ã‡∏¥‡∏£‡πå‡∏ü‡πÄ‡∏ß‡∏≠‡∏£‡πå
    print(f"[{now_iso}] ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å Front:")
    print(f"  station_id = {payload.station_id}")
    print(f"  cp_status2 = {payload.cp_status2}")
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° message ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô MQTT (‡πÉ‡∏™‡πà timestamp ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡πâ)
    msg = {
        "station_id": payload.station_id,
        "cp_status2": payload.cp_status2,
        "timestamp": now_iso,
        # "source": "fastapi/setting_plc"
    }
    payload_str = json.dumps(msg, ensure_ascii=False)

    # ‡∏™‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô MQTT (QoS 1, ‡πÑ‡∏°‡πà retain)
    try:
        pub_result = mqtt_client.publish(MQTT_TOPIC, payload_str, qos=1, retain=False)
        # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡πÜ (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏ß‡∏£‡πå)
        pub_result.wait_for_publish(timeout=2.0)
        published = pub_result.is_published()
        rc = pub_result.rc
        print(f"[MQTT] publish rc={rc}, published={published}, topic={MQTT_TOPIC}")
    except Exception as e:
        print(f"[MQTT] publish error: {e}")
        published = False

    # ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö frontend
    return {
        "ok": True,
        "message": "‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å frontend ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏™‡πà‡∏á MQTT ‡πÅ‡∏•‡πâ‡∏ß",
        "timestamp": now_iso,
        "mqtt": {
            "broker": f"{BROKER_HOST}:{BROKER_PORT}",
            "topic": MQTT_TOPIC,
            "published": bool(published),
        },
        "data": msg,
    }

# --------------------------------------------------------------------- CBM Page
def get_cbm_collection_for(station_id: str):
    # ‡∏Å‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏Å ‡πÜ / injection: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï a-z A-Z 0-9 _ -
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return CBM_DB.get_collection(str(station_id))

@app.get("/CBM")
async def cbm_query(request: Request, station_id: str = Query(...), current: UserClaims = Depends(get_current_user)):
    """
    SSE ‡πÅ‡∏ö‡∏ö query param:
    - ‡∏™‡πà‡∏á snapshot ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (event: init)
    - ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô polling ‡∏Ç‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á ‡πÜ
    """
    headers = {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }
    coll = get_cbm_collection_for(station_id)

    async def event_generator():
        last_id = None
        latest = await coll.find_one({}, sort=[("_id", -1)])  # ‚¨ÖÔ∏è ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á filter station_id ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÅ‡∏•‡πâ‡∏ß
        if latest:
            # latest["timestamp"] = _ensure_utc_iso(latest.get("timestamp"))
            latest["timestamp"] = latest.get("timestamp")
            last_id = latest.get("_id")
            yield "retry: 3000\n"
            yield "event: init\n"
            yield f"data: {to_json(latest)}\n\n"
        else:
            yield "retry: 3000\n\n"

        while True:
            if await request.is_disconnected():
                break

            doc = await coll.find_one({}, sort=[("_id", -1)])
            if doc and doc.get("_id") != last_id:
                # doc["timestamp"] = _ensure_utc_iso(doc.get("timestamp"))
                doc["timestamp"] = doc.get("timestamp")
                last_id = doc.get("_id")
                yield f"data: {to_json(doc)}\n\n"
            else:
                yield ": keep-alive\n\n"

            await asyncio.sleep(5)

    return StreamingResponse(event_generator(), headers=headers)

# -------------------------------------------------------------- AI
def get_outputModule6_collection_for(station_id: str):
    # ‡∏Å‡∏±‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡πÄ‡∏•‡∏Å‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏Å ‡πÜ / injection: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï a-z A-Z 0-9 _ -
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    return outputModule6.get_collection(str(station_id))

@app.get("/outputModule6")
async def get_latest(
    request: Request,
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    coll = get_outputModule6_collection_for(station_id)
    doc = await coll.find_one({}, sort=[("_id", -1)])
    if not doc:
        raise HTTPException(status_code=404, detail="No data")

    # ‚úÖ ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏±‡∏ç‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏£‡∏≠‡∏ö ETag ‡∏ï‡∏≤‡∏°‡∏™‡πÄ‡∏õ‡∏Å
    etag = f"\"{str(doc['_id'])}\""

    inm = request.headers.get("if-none-match")
    # ‚úÖ ‡πÉ‡∏ä‡πâ Response ‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ body/JSONResponse
    if inm and etag in inm:
        return Response(status_code=304)

    # ‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡∏™‡πà‡∏á body ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á _id ‡∏Å‡πà‡∏≠‡∏ô
    doc["_id"] = str(doc["_id"])
    payload = jsonable_encoder(doc)

    resp = JSONResponse(content=payload)
    resp.headers["ETag"] = etag
    # ‡∏ñ‡πâ‡∏≤ endpoint ‡∏°‡∏µ auth ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ private
    resp.headers["Cache-Control"] = "private, max-age=86400, stale-while-revalidate=604800"
    return resp

@app.get("/outputModule6/progress")
async def get_module6_progress(
    request: Request,
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ progress/health index ‡∏à‡∏≤‡∏Å MongoDB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö module 6"""
    coll = get_outputModule6_collection_for(station_id)
    doc = await coll.find_one({}, sort=[("_id", -1)])
    
    if not doc:
        return JSONResponse(content={"progress": 0})
    
    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤ health index/progress ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ü‡∏¥‡∏•‡∏î‡πå 'health_index' ‡∏´‡∏£‡∏∑‡∏≠ 'rul_percentage'
    # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
    progress = doc.get("health_index", 0)  # ‡∏´‡∏£‡∏∑‡∏≠ doc.get("rul_percentage", 0)
    
    etag = f"\"{str(doc['_id'])}\""
    inm = request.headers.get("if-none-match")
    
    if inm and etag in inm:
        return Response(status_code=304)
    
    resp = JSONResponse(content={"progress": progress})
    resp.headers["ETag"] = etag
    resp.headers["Cache-Control"] = "private, max-age=60, stale-while-revalidate=300"
    return resp

@app.get("/modules/detail")
async def get_module_detail(
    request: Request,
    module_id: str = Query(..., description="‡πÄ‡∏ä‡πà‡∏ô module1..module7"),
    station_id: str = Query(..., description="‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ ‡πÄ‡∏ä‡πà‡∏ô Klongluang3"),
    current: UserClaims = Depends(get_current_user),
):
    if module_id not in MODULES:
        raise HTTPException(status_code=400, detail="Unknown module_id")

    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")

    input_doc = None
    output_doc = None

    # --- OUTPUT ---
    try:
        out_db = OUTPUT_DBS.get(module_id)
        if out_db is not None:
            out_coll = out_db.get_collection(str(station_id))
            output_doc = await out_coll.find_one({}, sort=[("_id", -1)])
    except Exception as e:
        print("get_module_detail output error:", e)

    # --- INPUT ---
    try:
        in_db = INPUT_DBS.get(module_id)
        if in_db is not None:
            in_coll = in_db.get_collection(str(station_id))
            input_doc = await in_coll.find_one({}, sort=[("_id", -1)])
    except Exception as e:
        print("get_module_detail input error:", e)

    # üîß ‡πÅ‡∏õ‡∏•‡∏á ObjectId ‚Üí str ‡∏î‡πâ‡∏ß‡∏¢ custom_encoder
    input_enc = jsonable_encoder(
        input_doc,
        custom_encoder={ObjectId: str},
    ) if input_doc else None

    output_enc = jsonable_encoder(
        output_doc,
        custom_encoder={ObjectId: str},
    ) if output_doc else None

    payload = {
        "module_id": module_id,
        "station_id": station_id,
        "input": input_enc,
        "output": output_enc,
    }

    resp = JSONResponse(content=payload)
    resp.headers["Cache-Control"] = "no-store"
    return resp



# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö modules ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
@app.get("/modules/progress")
async def get_all_modules_progress(
    request: Request,
    station_id: str = Query(...),
    current: UserClaims = Depends(get_current_user),
):
    """‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ progress ‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å modules ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô"""
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", str(station_id)):
        raise HTTPException(status_code=400, detail="Bad station_id")
    
    result = {}
    
    # Module 1 - MDB Filters
    try:
        coll1 = outputModule1.get_collection(str(station_id))
        doc1 = await coll1.find_one({}, sort=[("_id", -1)])
        result["module1"] = doc1.get("health").get("health_index", 0) if doc1 else 0
    except:
        result["module1"] = 0
    try:
        coll2 = outputModule2.get_collection(str(station_id))
        doc2 = await coll2.find_one({}, sort=[("_id", -1)])
        result["module2"] = doc2.get("health_index_percent", 0) if doc2 else 0
    except:
        result["module2"] = 0
    
    # Module 3 - Online/Offline
    try:
        coll3 = outputModule3.get_collection(str(station_id))
        doc3 = await coll3.find_one({}, sort=[("_id", -1)])
        result["module3"] = doc3.get("health_index", 0) if doc3 else 0
    except:
        result["module3"] = 0
    
    # Module 4 - Power Supply
    try:
        coll4 = outputModule4.get_collection(str(station_id))
        doc4 = await coll4.find_one({}, sort=[("_id", -1)])
        result["module4"] = doc4.get("health_index", 0) if doc4 else 0
    except:
        result["module4"] = 0
    
    # Module 5 - Network
    try:
        coll5 = outputModule5.get_collection(str(station_id))
        doc5 = await coll5.find_one({}, sort=[("_id", -1)])
        result["module5"] = doc5.get("health").get("health_index",0) if doc5 else 0
    except:
        result["module5"] = 0
    
    # Module 6 - RUL
    try:
        coll6 = outputModule6.get_collection(str(station_id))
        doc6 = await coll6.find_one({}, sort=[("_id", -1)])
        result["module6"] = doc6.get("overall_health").get("average_health_index",0) if doc6 else 0
    except:
        result["module6"] = 0
    
    # Module 7 - Root Cause Analysis
    try:
        coll7 = outputModule7.get_collection(str(station_id))
        doc7 = await coll7.find_one({}, sort=[("_id", -1)])
        result["module7"] = doc7.get("health_index",0) if doc7 else 0
    except:
        result["module7"] = 0
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì overall health index (‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å modules)
    module_values = list(result.values())
    overall_health = round(sum(module_values) / len(module_values), 2) if module_values else 0
    result["overall"] = overall_health
    
    resp = JSONResponse(content=result)
    resp.headers["Cache-Control"] = "private, max-age=60"
    return resp


def get_station_collection():
    return station_collection

class ModuleToggleIn(BaseModel):
    enabled: bool

@app.patch("/station/{station_id}/{module_id}", status_code=204)
async def patch_module_toggle(
    station_id: str,
    module_id: str,
    body: ModuleToggleIn,
    current: UserClaims = Depends(get_current_user),
):
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ station_id ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    if not re.fullmatch(r"[A-Za-z0-9_\-]+", station_id):
        raise HTTPException(status_code=400, detail="Bad station_id")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ module_id ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    if module_id not in MODULES:
        raise HTTPException(status_code=400, detail="Invalid module_id")

    coll = get_station_collection()
    now = datetime.now(timezone.utc)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ç‡∏≠‡∏á‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ï‡∏≤‡∏° module_id ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏°‡∏≤
    module_field = f"{module_id}_isActive"

    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÇ‡∏°‡∏î‡∏π‡∏•
    res = coll.update_one(
        {"station_id": station_id},
        {"$set": {
            module_field: body.enabled,
            "updatedAt": now,
            # "updated_by": getattr(current, "sub", None),
        }},
        upsert=False,   # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ True
    )

    if res.matched_count == 0:
        raise HTTPException(status_code=404, detail="Station not found")
